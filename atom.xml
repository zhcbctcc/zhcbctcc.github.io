<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>临街小站</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://peihao.space/"/>
  <updated>2018-04-26T08:22:11.576Z</updated>
  <id>http://peihao.space/</id>
  
  <author>
    <name>clinjie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Golang入门</title>
    <link href="http://peihao.space/2018/04/26/golang-tutorial/"/>
    <id>http://peihao.space/2018/04/26/golang-tutorial/</id>
    <published>2018-04-26T08:04:44.000Z</published>
    <updated>2018-04-26T08:22:11.576Z</updated>
    
    <content type="html"><![CDATA[<h1 id="永远的Hello_World">永远的Hello World</h1><p>Hello world</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"><span class="keyword">func</span> main()</span><br><span class="line"><span class="comment">/*Golang语言注释*/</span></span><br><span class="line">fmt.Println(<span class="string">"Hello,World!"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li><p>第一行代码 package main 定义了包名。你必须在源文件中非注释的第一行指明这个文件属于哪个包，如：package main。package main表示一个可独立执行的程序，每个 Go 应用程序都包含一个名为 main 的包。</p></li><li><p>main 函数是每一个可执行程序所必须包含的，一般来说都是在启动后第一个执行的函数（如果有 init() 函数则会先执行该函数）。</p></li><li><p>当标识符（包括常量、变量、类型、函数名、结构字段等等）以一个大写字母开头，如：Group1，那么使用这种形式的标识符的对象就可以被外部包的代码所使用（客户端程序需要先导入这个包），这被称为导出（像面向对象语言中的 public）；标识符如果以小写字母开头，则对包外是不可见的，但是他们在整个包的内部是可见并且可用的</p></li></ol><a id="more"></a><h2 id="结构">结构</h2><p>Go使用package来组织，只要package名称为main的包可以包含main函数，一个可执行程序有且仅有一个main包。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> "fmt"</span><br><span class="line"><span class="keyword">import</span> "io"</span><br><span class="line"><span class="comment">/*与下面等同*/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> &#123;</span><br><span class="line"><span class="string">"fmt"</span>,</span><br><span class="line"><span class="string">"io"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以将导入的包起别名</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fmt2 <span class="string">"fmt"</span></span><br><span class="line"><span class="comment">/*上面的代码使用fmt2，位fmt起了别名*/</span></span><br></pre></td></tr></table></figure><p>省略调用，在调用其他包的方法的时候，不需要再先写其他的包名</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import . <span class="string">"fmt"</span></span><br><span class="line"><span class="function"><span class="title">Println</span><span class="params">(<span class="string">"Hello,Wolrld!"</span>)</span></span></span><br></pre></td></tr></table></figure><p>其他一些用法</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="comment">// 导入其他包</span></span><br><span class="line"><span class="keyword">import</span> . <span class="string">"fmt"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 常量定义</span></span><br><span class="line"><span class="keyword">const</span> PI = <span class="number">3.14</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局变量的声明和赋值</span></span><br><span class="line"><span class="keyword">var</span> name = <span class="string">"gopher"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 一般类型声明</span></span><br><span class="line"><span class="keyword">type</span> newType <span class="typename">int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 结构的声明</span></span><br><span class="line"><span class="keyword">type</span> gopher <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 接口的声明</span></span><br><span class="line"><span class="keyword">type</span> golang <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 由main函数作为程序入口点启动</span></span><br><span class="line"><span class="keyword">func</span> main() &#123;</span><br><span class="line">    Println(<span class="string">"Hello World!"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Go语言中，使用大小写来决定该常量、变量、类型、接口、结构或函数是否可以被外部包所调用。函数名首字母小写即为 private ，函数名首字母大写即为 public 。</p><h2 id="数据类型">数据类型</h2><ol><li>布尔类型</li></ol><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> flag <span class="keyword">bool</span>=<span class="keyword">true</span>;</span><br></pre></td></tr></table></figure><ol><li>数字类型</li></ol><p>整形：<code>uint/int/byte</code></p><p>浮点型： <code>float32/float64</code></p><p>复数型：<code>complex</code></p><ol><li>字符串</li></ol><p>字符串中的字节使用UTF8编码标识Unicode文本。</p><p>Unicoder有2个字节形式、4个字节形式</p><p>UTF8是变长编码，英文1个字节，汉字3个字节，生僻的为4-6字节</p><ol><li>派生类型</li></ol><p>例如指针、数组、结构化、Channel、函数、切片、接口、Map</p><p>下面是一些变量赋值代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*全局变量声明*/</span></span><br><span class="line"><span class="keyword">var</span> isActivate <span class="typename">bool</span></span><br><span class="line"><span class="comment">/*忽略类型的声明*/</span></span><br><span class="line"><span class="keyword">var</span> enabled,disabled = <span class="constant">true</span>, flase</span><br><span class="line"></span><br><span class="line"><span class="keyword">func</span> test()&#123;</span><br><span class="line"><span class="comment">/*普通声明*/</span></span><br><span class="line"><span class="keyword">var</span> available <span class="typename">bool</span></span><br><span class="line">*简短声明*/</span><br><span class="line">valid := flase</span><br><span class="line"><span class="comment">/*赋值操作*/</span></span><br><span class="line">vailable = <span class="constant">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>值类型：</p><p>所有像int、float、bool和string这些基本类型都属于值类型，使用这些类型的变量直接指向存在内存中的值，使用等号赋值的时候，实际上是在内存中将数值进行了拷贝。可以使用<code>&amp;i</code>获取变量i的内存地址</p><p>引用类型：</p><p>一个引用类型的变量r1存储的是r1值值所有的内存地址，或者内存地址中第一个字所在的位置，这个地址被称之为指针。</p><p>对于局部变量，仅仅声明不赋值不使用，以及仅仅声明赋值不进行使用都是不行的，会报错。局部变量必须使用。</p><p>声明的几种方法：<code>var + type</code>，<code>var</code>，<code>:=</code></p><p>空白标识符_也用与抛弃值，，因为他是一个只读类型的变量。</p><h3 id="数组">数组</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">var number = [<span class="number">6</span>] <span class="keyword">int</span>&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>&#125;</span><br><span class="line">number := [<span class="number">6</span>]<span class="keyword">int</span>&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*numbers[0]=1...numbers[4]=0,numbers[5]=0*/</span></span><br></pre></td></tr></table></figure><h3 id="常量">常量</h3><p>常量是一个简单值的标识符，在程序运行时，不会被修改的量。使用const进行标识</p><p>常量还可以用作枚举：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">    Unknown = <span class="number">0</span></span><br><span class="line">    Female = <span class="number">1</span></span><br><span class="line">    Male = <span class="number">2</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>常量可以用len(), cap(), unsafe.Sizeof()函数计算表达式的值。其中<code>unsafe.Sizeof</code>表示占用的内存字节数；例如<code>a=12;fmt.Println(unsafe.Sizrof(a))</code>输出的就是8，因为a的类型为int8；对于一个字符串，<code>unsafe.Sizeof</code>一直都是16字节，因为他是一个结构，其中8个字节指向了存储的空间，8个字节保存了这个字符串的长度。</p><p><strong>iota</strong></p><p>iota，特殊常量，可以认为是一个可以被编译器修改的常量。</p><p>在每一个const关键字出现时，被重置为0，然后再<strong>下一个const出现之前</strong>，每出现一次iota，其所代表的数字会自动增加1。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">    a = <span class="constant">iota</span><span class="comment">//0</span></span><br><span class="line">    b = <span class="constant">iota</span><span class="comment">//1</span></span><br><span class="line">    c = <span class="constant">iota</span><span class="comment">//2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">            a = <span class="constant">iota</span>   <span class="comment">//0</span></span><br><span class="line">            b          <span class="comment">//1</span></span><br><span class="line">            c          <span class="comment">//2</span></span><br><span class="line">            d = <span class="string">"ha"</span>   <span class="comment">//d="ha"   iota = 3</span></span><br><span class="line">            e          <span class="comment">//e="ha"   iota = 4</span></span><br><span class="line">            f = <span class="number">100</span>    <span class="comment">//f=100    iota = 5</span></span><br><span class="line">            g          <span class="comment">//g=100    iota = 6</span></span><br><span class="line">            h = <span class="constant">iota</span>   <span class="comment">//7</span></span><br><span class="line">            i          <span class="comment">//8</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="一些运算符">一些运算符</h3><p>位运算运算符：</p><p>&amp; 按位与，!按位或，^按位异或，&lt;&lt;左移运算（高位丢弃，低位补0），&gt;&gt;右移运算</p><h3 id="channel">channel</h3><p>Channel是Go中的一个核心类型，你可以把它看成一个管道，通过它并发核心单元就可以发送或者接收数据进行通讯</p><p>channel包含三种形式定义，分别是双向管道，以及两个单向（单发送和单接受）</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">chan</span> T          <span class="comment">// 可以接收和发送类型为 T 的数据  </span></span><br><span class="line"><span class="keyword">chan</span>&lt;- <span class="typename">float64</span>  <span class="comment">// 只可以用来发送 float64 类型的数据  </span></span><br><span class="line">&lt;-<span class="keyword">chan</span> <span class="typename">int</span>      <span class="comment">// 只可以用来接收 int 类型的数据  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ch &lt;- v    <span class="comment">// 发送值v到Channel ch中  </span></span><br><span class="line">v := &lt;-ch  <span class="comment">// 从Channel ch中接收数据，并将数据赋值给v</span></span><br></pre></td></tr></table></figure><h3 id="条件语句">条件语句</h3><p>if语句，if else，wiitch，select</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> a&lt;<span class="number">20</span>&#123;</span><br><span class="line">fmt.Println(<span class="string">"xixi"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> a &lt; <span class="number">20</span> &#123;</span><br><span class="line">       fmt.Printf(<span class="string">"a 小于 20\n"</span> );</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       fmt.Printf(<span class="string">"a 不小于 20\n"</span> );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> grade <span class="typename">string</span> = <span class="string">"B"</span></span><br><span class="line"><span class="keyword">var</span> marks <span class="typename">int</span> = <span class="number">90</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> marks &#123;</span><br><span class="line">   <span class="keyword">case</span> <span class="number">90</span>: grade = <span class="string">"A"</span></span><br><span class="line">   <span class="keyword">case</span> <span class="number">80</span>: grade = <span class="string">"B"</span></span><br><span class="line">   <span class="keyword">case</span> <span class="number">50</span>,<span class="number">60</span>,<span class="number">70</span> : grade = <span class="string">"C"</span></span><br><span class="line">   <span class="keyword">default</span>: grade = <span class="string">"D"</span>  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*select随机执行一个可运行的case。如果没有case可运行，它将阻塞，直到有case可运行*/</span></span><br><span class="line"><span class="keyword">var</span> c1, c2, c3 <span class="keyword">chan</span> <span class="typename">int</span></span><br><span class="line"><span class="keyword">var</span> i1, i2 <span class="typename">int</span></span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line">   <span class="keyword">case</span> i1 = &lt;-c1:</span><br><span class="line">      fmt.Printf(<span class="string">"received "</span>, i1, <span class="string">" from c1\n"</span>)</span><br><span class="line">   <span class="keyword">case</span> c2 &lt;- i2:</span><br><span class="line">      fmt.Printf(<span class="string">"sent "</span>, i2, <span class="string">" to c2\n"</span>)</span><br><span class="line">   <span class="keyword">case</span> i3, ok := (&lt;-c3):</span><br><span class="line">      <span class="keyword">if</span> ok &#123;</span><br><span class="line">         fmt.Printf(<span class="string">"received "</span>, i3, <span class="string">" from c3\n"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         fmt.Printf(<span class="string">"c3 is closed\n"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">   <span class="keyword">default</span>:</span><br><span class="line">      fmt.Printf(<span class="string">"no communication\n"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="循环语句">循环语句</h3><p>for循环的几种形式：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*和C语言的for循环一样*/</span></span><br><span class="line"><span class="keyword">for</span> init;condition;post</span><br><span class="line"></span><br><span class="line"><span class="comment">/*和while循环一样*/</span></span><br><span class="line"><span class="keyword">for</span> condition &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key,value := <span class="keyword">range</span> oldMap&#123;&#125;</span><br></pre></td></tr></table></figure><p>goto 语句可以无条件地转移到过程中指定的行。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">LOOP: <span class="keyword">for</span> a &lt; <span class="number">20</span>&#123;</span><br><span class="line"><span class="keyword">if</span> a == <span class="number">15</span>&#123;</span><br><span class="line">a += <span class="number">1</span></span><br><span class="line"><span class="keyword">goto</span> LOOP</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>示例：打印小于100的素数</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">func</span> main() &#123;</span><br><span class="line"><span class="keyword">for</span> a := <span class="number">2</span>;a&lt;=<span class="number">100</span>;a+=<span class="number">1</span>&#123;</span><br><span class="line">flag := <span class="constant">true</span></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">2</span>; i &lt;= a/<span class="number">2</span>; i+=<span class="number">1</span>&#123;</span><br><span class="line"><span class="keyword">if</span> a%i == <span class="number">0</span>&#123;</span><br><span class="line">flag = <span class="constant">false</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> flag&#123;</span><br><span class="line">fmt.Println(a)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="函数">函数</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">func</span> <span class="tag">function_name</span>( <span class="attr_selector">[parameter list]</span> ) <span class="attr_selector">[return_types]</span> <span class="rules">&#123;&#125;</span></span><br></pre></td></tr></table></figure><p>实例：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">func</span> max(num1,num2 <span class="typename">int</span>) <span class="typename">int</span>&#123;</span><br><span class="line"><span class="keyword">if</span> num1&gt;num2&#123;</span><br><span class="line"><span class="keyword">return</span> num1</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="keyword">return</span> num2</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重写打印素数</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*检查数值是否为素数*/</span></span><br><span class="line"><span class="keyword">func</span> check(num <span class="typename">int</span>) <span class="typename">bool</span>&#123;</span><br><span class="line"><span class="keyword">for</span> a:=<span class="number">2</span>;a&lt;=num/<span class="number">2</span>;a+=<span class="number">1</span>&#123;</span><br><span class="line"><span class="keyword">if</span> num%a==<span class="number">0</span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="constant">false</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="constant">true</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">func</span> main() &#123;</span><br><span class="line"><span class="keyword">for</span> a := <span class="number">2</span>;a&lt;=<span class="number">100</span>;a+=<span class="number">1</span>&#123;</span><br><span class="line"><span class="keyword">if</span> check(a)&#123;</span><br><span class="line">fmt.Println(a)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="指针">指针</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a <span class="typename">int</span>= <span class="number">20</span>   <span class="comment">/* 声明实际变量 */</span></span><br><span class="line">  <span class="keyword">var</span> ip *<span class="typename">int</span>        <span class="comment">/* 声明指针变量 */</span></span><br><span class="line"></span><br><span class="line">  ip = &amp;a  <span class="comment">/* 指针变量的存储地址 */</span></span><br><span class="line"></span><br><span class="line">  fmt.Printf(<span class="string">"a 变量的地址是: %x\n"</span>, &amp;a  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 指针变量的存储地址 */</span></span><br><span class="line">  fmt.Printf(<span class="string">"ip 变量储存的指针地址: %x\n"</span>, ip )</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 使用指针访问值 */</span></span><br><span class="line">  fmt.Printf(<span class="string">"*ip 变量的值: %d\n"</span>, *ip )</span><br></pre></td></tr></table></figure><p>指针数组：</p><p>数组中的每个元素都是指针：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a := []<span class="keyword">int</span>&#123;<span class="number">10</span>,<span class="number">100</span>,<span class="number">200</span>&#125;</span><br><span class="line">var ptr [<span class="number">3</span>]*<span class="keyword">int</span></span><br><span class="line"><span class="keyword">for</span>  i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++ &#123;</span><br><span class="line">     ptr[i] = &amp;a[i]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>  i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++ &#123;</span><br><span class="line">     fmt.Printf(<span class="string">"a[%d] = %d\n"</span>, i,*ptr[i] )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>指针作为参数</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">func</span> swap(x *<span class="typename">int</span>, y *<span class="typename">int</span>) &#123;</span><br><span class="line">   <span class="keyword">var</span> temp <span class="typename">int</span></span><br><span class="line">   temp = *x   </span><br><span class="line">   *x = *y      </span><br><span class="line">   *y = temp   </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> a <span class="typename">int</span> = <span class="number">100</span></span><br><span class="line"><span class="keyword">var</span> b <span class="typename">int</span> = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">swap(&amp;a, &amp;b)</span><br></pre></td></tr></table></figure><h3 id="结构体">结构体</h3><p>type对结构体进行命名</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="label">type</span> <span class="keyword">Books </span><span class="keyword">struct </span>&#123;</span><br><span class="line">   title <span class="keyword">string</span><br><span class="line"></span>   author <span class="keyword">string</span><br><span class="line"></span>   <span class="keyword">subject </span><span class="keyword">string</span><br><span class="line"></span>   <span class="keyword">book_id </span>int</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="label">var</span> <span class="keyword">Book1 </span><span class="keyword">Books</span><br><span class="line"></span><span class="keyword">Book1.title </span>= <span class="string">"Go 语言"</span></span><br><span class="line"><span class="keyword">Book1.author </span>= <span class="string">"www.runoob.com"</span></span><br><span class="line"><span class="keyword">Book1.subject </span>= <span class="string">"Go 语言教程"</span></span><br><span class="line"><span class="keyword">Book1.book_id </span>= <span class="number">6495407</span></span><br></pre></td></tr></table></figure><p>使用指向结构体的指针，访问结构体内部元素</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="label">var</span> <span class="keyword">struct_pointer </span>*<span class="keyword">Books</span><br><span class="line"></span><span class="keyword">struct_pointer </span>= &amp;<span class="keyword">Book1</span><br><span class="line"></span><span class="keyword">struce_pointer.title</span></span><br></pre></td></tr></table></figure><h3 id="切片">切片</h3><p>Go 数组的长度不可改变，在特定场景中这样的集合就不太适用，Go中提供了一种灵活，功能强悍的内置类型切片(“动态数组”)，与数组相比切片的长度是不固定的，可以追加元素，在追加时可能使切片的容量增大。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s :=[] <span class="keyword">int</span> &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> &#125; <span class="comment">/*不设置数组长度，此时分片的初始len=cap=3*/</span></span><br><span class="line">s :=make([]<span class="keyword">int</span>,len,cap) <span class="comment">/*其中len是当前使用的长度，cap是当前切片最大可以容纳多少元素*/</span></span><br></pre></td></tr></table></figure><p>copy函数与append函数</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">var</span> numbers []int</span><br><span class="line"><span class="function"><span class="title">printSlice</span><span class="params">(numbers)</span></span></span><br><span class="line"></span><br><span class="line">numbers = <span class="function"><span class="title">append</span><span class="params">(numbers, <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">printSlice</span><span class="params">(numbers)</span></span><span class="comment">/*[0]*/</span></span><br><span class="line"></span><br><span class="line">numbers = <span class="function"><span class="title">append</span><span class="params">(numbers, <span class="number">1</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">printSlice</span><span class="params">(numbers)</span></span><span class="comment">/*[0,1]*/</span></span><br><span class="line"></span><br><span class="line">numbers = <span class="function"><span class="title">append</span><span class="params">(numbers, <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">printSlice</span><span class="params">(numbers)</span></span><span class="comment">/*[0,1,2,3,4]*/</span></span><br><span class="line"></span><br><span class="line">numbers1 := <span class="function"><span class="title">make</span><span class="params">([]int, len(numbers)</span></span>, (<span class="function"><span class="title">cap</span><span class="params">(numbers)</span></span>)*<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">copy</span><span class="params">(numbers1,numbers)</span></span><span class="comment">/*将numbers中的元素拷贝到切片numbers1中*/</span></span><br><span class="line"><span class="function"><span class="title">printSlice</span><span class="params">(numbers1)</span></span></span><br></pre></td></tr></table></figure><h3 id="Range方法">Range方法</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">func <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    nums := []<span class="keyword">int</span>&#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line">    sum := <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _, num := range nums &#123;</span><br><span class="line">        sum += num</span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Println(<span class="string">"sum:"</span>, sum)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, num := range nums &#123;</span><br><span class="line">        <span class="keyword">if</span> num == <span class="number">3</span> &#123;</span><br><span class="line">            fmt.Println(<span class="string">"index:"</span>, i)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    kvs := <span class="built_in">map</span>[<span class="built_in">string</span>]<span class="built_in">string</span>&#123;<span class="string">"a"</span>: <span class="string">"apple"</span>, <span class="string">"b"</span>: <span class="string">"banana"</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> k, v := range kvs &#123;</span><br><span class="line">        fmt.Printf(<span class="string">"%s -&gt; %s\n"</span>, k, v)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, c := range <span class="string">"go"</span> &#123;</span><br><span class="line">        fmt.Println(i, c)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="map">map</h3><p>Map 是一种无序的键值对的集合。Map 最重要的一点是通过 key 来快速检索数据，key 类似于索引，指向数据的值。</p><p>Map 是一种集合，所以我们可以像迭代数组和切片那样迭代它。不过，Map 是无序的，我们无法决定它的返回顺序，这是因为 Map 是使用 hash 表来实现的。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> countryCapitalMap <span class="keyword">map</span>[<span class="typename">string</span>]<span class="typename">string</span></span><br><span class="line">countryCapitalMap = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="typename">string</span>]<span class="typename">string</span>)</span><br><span class="line"></span><br><span class="line">countryCapitalMap[<span class="string">"France"</span>] = <span class="string">"Paris"</span></span><br><span class="line">countryCapitalMap[<span class="string">"Italy"</span>] = <span class="string">"Rome"</span></span><br><span class="line">countryCapitalMap[<span class="string">"Japan"</span>] = <span class="string">"Tokyo"</span></span><br><span class="line">countryCapitalMap[<span class="string">"India"</span>] = <span class="string">"New Delhi"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> country := <span class="keyword">range</span> countryCapitalMap &#123;</span><br><span class="line">   fmt.Println(<span class="string">"Capital of"</span>,country,<span class="string">"is"</span>,countryCapitalMap[country])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 查看元素在集合中是否存在 */</span></span><br><span class="line">captial, ok := countryCapitalMap[<span class="string">"United States"</span>]</span><br><span class="line"><span class="comment">/* 如果 ok 是 true, 则存在，否则不存在 */</span></span><br><span class="line"><span class="keyword">if</span>(ok)&#123;</span><br><span class="line">   fmt.Println(<span class="string">"Capital of United States is"</span>, captial)  </span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">   fmt.Println(<span class="string">"Capital of United States is not present"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*删除map中的某个元素*/</span></span><br><span class="line"><span class="built_in">delete</span>(countryCapitalMap,<span class="string">"France"</span>)</span><br></pre></td></tr></table></figure><h3 id="语言类型转换">语言类型转换</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">func</span> main() &#123;</span><br><span class="line">   <span class="keyword">var</span> sum <span class="typename">int</span> = <span class="number">17</span></span><br><span class="line">   <span class="keyword">var</span> count <span class="typename">int</span> = <span class="number">5</span></span><br><span class="line">   <span class="keyword">var</span> mean <span class="typename">float32</span></span><br><span class="line"></span><br><span class="line">   mean = <span class="typename">float32</span>(sum)/<span class="typename">float32</span>(count)</span><br><span class="line">   fmt.Printf(<span class="string">"mean 的值为: %f\n"</span>,mean)<span class="comment">/*3.400000*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="接口">接口</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Phone <span class="keyword">interface</span> &#123;</span><br><span class="line">    call()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> NokiaPhone <span class="keyword">struct</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*可以看出来与普通函数不同，普通函数是func func_name() type&#123;&#125;*/</span></span><br><span class="line"><span class="comment">/*而这里是func () interface_func_name() type&#123;&#125;*/</span></span><br><span class="line"><span class="keyword">func</span> (nokiaPhone NokiaPhone) call() &#123;</span><br><span class="line">    fmt.Println(<span class="string">"I am Nokia, I can call you!"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> IPhone <span class="keyword">struct</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">func</span> (iPhone IPhone) call() &#123;</span><br><span class="line">    fmt.Println(<span class="string">"I am iPhone, I can call you!"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">func</span> main() &#123;</span><br><span class="line">    <span class="keyword">var</span> phone Phone</span><br><span class="line"></span><br><span class="line">    phone = <span class="built_in">new</span>(NokiaPhone)</span><br><span class="line">    phone.call()</span><br><span class="line"></span><br><span class="line">    phone = <span class="built_in">new</span>(IPhone)</span><br><span class="line">    phone.call()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上面的例子中，我们定义了一个接口Phone，接口里面有一个方法call()。然后我们在main函数里面定义了一个Phone类型变量，并分别为之赋值为NokiaPhone和IPhone。然后调用call()方法</p><h3 id="错误处理">错误处理</h3><p>Go 语言通过内置的错误接口提供了非常简单的错误处理机制。</p><p>error类型是一个接口类型，这是它的定义：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> error <span class="keyword">interface</span> &#123;</span><br><span class="line">    Error() <span class="typename">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们需要在相应的地方实现这个接口方法。</p><p>函数通常在最后的返回值中返回错误信息。使用errors.New 可返回一个错误信息</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f float64)</span> <span class="params">(float64, error)</span> &#123;</span></span><br><span class="line">    <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"math: square root of negative number"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    // 实现</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;永远的Hello_World&quot;&gt;永远的Hello World&lt;/h1&gt;&lt;p&gt;Hello world&lt;/p&gt;
&lt;figure class=&quot;highlight go&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;package&lt;/span&gt; main&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;fmt&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;func&lt;/span&gt; main()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;comment&quot;&gt;/*Golang语言注释*/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	fmt.Println(&lt;span class=&quot;string&quot;&gt;&quot;Hello,World!&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;第一行代码 package main 定义了包名。你必须在源文件中非注释的第一行指明这个文件属于哪个包，如：package main。package main表示一个可独立执行的程序，每个 Go 应用程序都包含一个名为 main 的包。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;main 函数是每一个可执行程序所必须包含的，一般来说都是在启动后第一个执行的函数（如果有 init() 函数则会先执行该函数）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;当标识符（包括常量、变量、类型、函数名、结构字段等等）以一个大写字母开头，如：Group1，那么使用这种形式的标识符的对象就可以被外部包的代码所使用（客户端程序需要先导入这个包），这被称为导出（像面向对象语言中的 public）；标识符如果以小写字母开头，则对包外是不可见的，但是他们在整个包的内部是可见并且可用的&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Go" scheme="http://peihao.space/categories/Go/"/>
    
    
      <category term="Go" scheme="http://peihao.space/tags/Go/"/>
    
  </entry>
  
  <entry>
    <title>一些NLP知识</title>
    <link href="http://peihao.space/2018/04/20/nlp-sth/"/>
    <id>http://peihao.space/2018/04/20/nlp-sth/</id>
    <published>2018-04-20T15:35:03.000Z</published>
    <updated>2018-04-20T16:13:23.932Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一些NLP">一些NLP</h1><p>简单的例子，判断一个词的词性，是动词还是名词。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 <code>f(x)-&gt;y</code> 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，一般的表现形式并不是数值型，所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里。这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种。</p><p>一般的数学模型只接受数值型输入，这里的x经常使用one-hot encoder，这种方式忽略了句子当中词语的次序，但是将每个词装换成为一个列表，词语之间的距离都为1。例如’我喜欢你’，转化成3个词：’’我’，’喜欢，’你’，三个word分别可以表示为可以表示为<code>[1,0,0],[0,1,0],[0,0,1]</code></p><p>现在已经可以将句子中的词转化成为数值型数据，输入到模型中，为什么还要word2vec呢？</p><a id="more"></a><p>word2vec从大量文本语料中以无监督的方式学习语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。比如“机器”和“机械”意思很相近，而“机器”和“猴子”的意思相差就很远了，那么由word2vec构建的这个数值空间中，“机器”和“机械”的距离较“机器”和“猴子”的距离而言是要近很多的。</p><p><strong>word2vec是将onehot encoder后的数据，通过训练映射到新的特征空间中。原始的onehot-encoder是将所有word的间隔（距离）都设置为1，</strong>这在实际中是不复合认知的。word2vec中，相似的word的向量在距离上也是相近的，没什么关系的距离较远；同时，对于一个大文本来讲，词形成的词典大小很大，相应的表示为每个词需要的长度也很大，这些词中有用的信息很少，只有那个为1的index是有用的，表示形式很sparse，稀疏，<strong>word2vec的size一般情况下远远小于词语总数，相当于将onehot-encoder降维。</strong></p><p>我们来看个例子，如何用 Word2vec 寻找相似词：</p><ol><li>对于一句话：『她们 夸 吴彦祖 帅 到 没朋友』，如果输入 x 是『吴彦祖』，那么 y 可以是『她们』、『夸』、『帅』、『没朋友』这些词</li><li>现有另一句话：『她们 夸 我 帅 到 没朋友』，如果输入 x 是『我』，那么不难发现，这里的上下文 y 跟上面一句话一样</li><li>从而 f(吴彦祖) = f(我) = y，所以大数据告诉我们：我 = 吴彦祖（完美的结论）</li></ol><h2 id="Skip-gram/CBOW">Skip-gram/CBOW</h2><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/67314351.jpg" alt=""></p><p>一个word作为输入，预测周围的上下文，模型叫做Skip-gram模型；<br>一个word的上下文（窗口）作为输入，预测这个word本身，模型叫做CBOW模型</p><p>先以Skip-gram为例，一个word x作为输入，预测周围的一个word y为输出</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/46209492.jpg" alt=""></p><p>x是上面提到的经过onehot-encoder形式的输入，y是在V（词典的size）个词上输出的概率，期望形式是y真实的onehot-encoder</p><p>在这里，跟常规的NN不同的是，Wordvec的隐层激活函数是线性的，而非sigmoid、relu这些非线性形式。</p><p>当模型训练完成之后，需要的的是神经网络的权重。</p><p>比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。这就是Word2vec的核心思想。</p><p>如图，这里有两个部分的参数W，分别是输入到隐层以及隐层到输出，这两部分都可以作为词向量形式，默认使用输入到隐层的参数作为 词向量。</p><p>假设共有词10000个，则one-hot encoder有10000维，对于一对一的预测，输入是10000维的onehot encoder，输出是10000维的每个词出现的概率值（一般通过softmax层进行归一化），这样维度实在是太大了。我们设置隐层，有神经元500个，那么参数w就是10000<em>500，现在一个词可以表示为500维度。方法是训练后的参数10000</em>500，对于输入x，找到值为1的index，提取出来对应的w[index]即可。</p><p>上面的形式是极端情况，通过一个word来预测周围的一个word或者通过上下文的一个word来预测word。上面提到过，CBOW是上下文预测word，Skip-gram是word预测上下文，常规形式如下：</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/37728731.jpg" alt="CBOW"></p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/64583235.jpg" alt="Skip-gram"></p><h3 id="区别">区别</h3><p>CBOW用于普通的数据量计算；Skip-gram用于数据量较大的情况。</p><h3 id="测试">测试</h3><ol><li>获取语料库</li><li>对语料进行分词（jieba分词）、去停用词</li><li>将分词、去停用词后的语料填充到word2vec模型中训练（使用<code>gensim.models.word2vec</code>）</li><li>使用model[word]获得词向量，使用model.similarity(word1,word2)计算word间的相似度</li></ol><h2 id="一些进阶">一些进阶</h2><p>Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。</p><h3 id="hierarchical_softmax">hierarchical softmax</h3><p>本质是把 N 分类问题变成 log(N)次二分类</p><h3 id="negative_sampling">negative sampling</h3><p>本质是预测总体类别的一个子集</p><h2 id="BOW">BOW</h2><p>Bag of words词袋模型。忽略了文本的语法和语序等要素（举个例子，Bow模型认为“我爱你”和“你爱我”是相同的），将其仅仅看作是若干词汇的集合。BoW 使用一组无序的单词（word）来表达一段文字或一个文档，并且文档中每个单词的出现都是独立的。</p><p>举例：</p><blockquote><p>John likes to watch movies. Mary likes too.<br>John also likes to watch football games.</p></blockquote><p>基于上述两个文档中出现的单词，构建如下一个词典 (dictionary)：</p><p><code>{&#39;John&#39;: 1, &#39;likes&#39;: 2,&#39;to&#39;: 3, &#39;watch&#39;: 4, &#39;movies&#39;: 5,&#39;also&#39;: 6, &#39;football&#39;: 7, &#39;games&#39;: 8,&#39;Mary&#39;: 9, &#39;too&#39;: 10}</code></p><p>上面的词典中包含10个单词, 每个单词有唯一的索引, 那么每个文本我们可以使用一个10维的向量来表示，向量中的元素是词典中对应的词语出现的频数。如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">1</span>,<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>该向量与原来文本中单词出现的顺序没有关系，而是词典中每个单词在文本中出现的频数</p><h2 id="哈夫曼树">哈夫曼树</h2><p>满二叉树：除了叶节点外每一个结点都有左右子女且叶节点都处在最底层的二叉树。<br>完全二叉树：只有最下面的两层结点度小于2，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树。<br>平衡二叉树：又称为AVL树，它是一颗空树或它的左右两个子树的高度差的绝对值不超过1</p><p>哈夫曼树：带权路径长度达到最小的二叉树，也叫做最优二叉树。</p><p>注意到这里，哈夫曼树只是一棵最优二叉树，不一定是完全二叉树，也不一定是平衡二叉树。</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/27252002.jpg" alt=""></p><p>fastText的霍夫曼树叶子结点对应为最终的label，可以看到，权重最大的，也就是权重最大的label，其深度最小，fastText 充分利用了这个性质，使得其速度得以提升。</p><h2 id="fastText">fastText</h2><p>fastText通过上下文预测标签（这个标签就是文本的类别，是训练模型之前通过人工标注等方法事先确定下来的），沿用了CBOW的单层神经网络的模式。</p><p>fastText模型的输入是一个词的序列（一段文本或者一句话)，输出是这个词序列属于不同类别的概率。在序列中的词和词组构成特征向量，特征向量通过线性变换映射到中间层，再由中间层映射到标签。<strong>fastText在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数</strong>。</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/31789162.jpg" alt=""></p><p>第一个权重矩阵$w_1$可以被视作某个句子的词向量，然后被平均成一个文本表征，然后其会被馈送入一个线性分类器。这个构架和CBOW模型相似，只是中间词（middle word）被替换成了标签（label）。</p><p>模型将一系列单词作为输入并产生一个预定义类的概率分布。使用softmax方程来计算这些概率。当数据量巨大时，线性分类器的计算代价十分大，所以fastText使用了一个基于霍夫曼编码树的<strong>分层softmax</strong>方法。</p><p>常用的文本特征表示方法是词袋模型，然而词袋（BoW）是忽略词序的。作为替代，fastText使用n-gram获取额外特征来得到关于局部词顺序的部分信息。</p><h3 id="分层softmax">分层softmax</h3><p>分层softmax的目的是降低softmax层的计算复杂度。</p><p><img src="http://img.mp.itc.cn/upload/20161024/f4f60271bdd84160a4826ec8924f53d4_th.jpeg" alt=""></p><p>把原来的softmax看做深度为1的树，词表V中的每一个词语表示一个叶子节点。如果把softmax改为二叉树结构，每个word表示叶子节点，那么只需要沿着通向该词语的叶子节点的路径搜索，而不需要考虑其它的节点。这就是为什<strong>么fastText可以解决不平衡分类问题</strong>，因为在对某个节点进行计算时，完全<strong>不依赖于它的上一层的叶子节点</strong>（即权重大于它的叶结点），也就是数目较大的label不能影响数目较小的label</p><p>具体说来，当遍历树的时候，我们需要能够计算左侧分枝或是右侧分枝的概率值。</p><p><img src="http://img.mp.itc.cn/upload/20161024/590c2f3fb8924c7c92201024ff42bff7.png" alt=""></p><p><strong>哈夫曼树与分层softmax的关系：fasttext输出是一个哈夫曼树，模型构造了一颗哈夫曼树，计算softmax的时候，需要计算向左还是向右，这里就需要基于词频作为权重构造的哈夫曼树结构，在这里加快了训练速度。</strong></p><p>fasttext采用了基于哈夫曼树的分层softmax方法。label样本多的，离顶点近，样本少的离顶点远，综合起来时间复杂度为O(hlog2k)</p><p>值得注意的是，此方法只是加速了训练过程，因为我们可以提前知道将要预测的词语（以及其搜索路径）。在测试过程中，被预测词语是未知的，仍然无法避免计算所有词语的概率值。</p><p>fasttext输入是简单的BOW模型加上局部的ngram（考虑词序），每个哈夫曼的节点向量都是训练的结果，也是通过梯度向下方法不断拟合。最后与相应的h(input)相乘，得出偏向左还是偏向右。</p><p>fasttext的hierarhical loss，粗略地说，是将所有的label，也就是所有的单词，放到一个哈夫曼树里面，<strong>出现频率越少的单词越接近叶子端</strong>。假设我们有一万个单词在词典里面，简单的softmax是一个10000类的分类问题；<strong>假设我们的一个目标单词在哈夫曼树上面是01001位置（比如我们用0表示左子树，1表示右子树）那么在这个样本上的分类问题是5个二分类问题</strong>：我们期望预测的单词在这条路径上每一个位置都更倾向于正确的方向。</p><h3 id="fasttext中的ngram">fasttext中的ngram</h3><p>常用的特征是词袋模型，在第一部分小编已经介绍过词袋模型了。词袋模型不考虑词之间的顺序，因此 fastText 还加入了 N-gram 特征。</p><p>“我爱你”：如果使用2-gram，这句话的特征还有 “我-爱”和“爱-你”，这两句话“我爱你”和“你爱我”就能区别开来了，因为“你爱我”的2-gram的特征还包括“你-爱”和“爱-我”，这样就可以区分“你爱我”和“我爱你”了。为了提高效率，实务中会过滤掉低频的 N-gram。否则将会严重影响速度。</p><p>在fastText 中一个低维度向量与每个单词都相关。隐藏表征在不同类别所有分类器中进行共享，使得文本信息在不同类别中能够共同使用。这类表征被称为词袋（bag of words）（此处忽视词序）。在 fastText中也使用向量表征单词 n-gram来将局部词序考虑在内，这对很多文本分类问题来说十分重要。</p><h3 id="用途">用途</h3><p>文本分类</p><p>同近义词的挖掘</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一些NLP&quot;&gt;一些NLP&lt;/h1&gt;&lt;p&gt;简单的例子，判断一个词的词性，是动词还是名词。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 &lt;code&gt;f(x)-&amp;gt;y&lt;/code&gt; 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，一般的表现形式并不是数值型，所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里。这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种。&lt;/p&gt;
&lt;p&gt;一般的数学模型只接受数值型输入，这里的x经常使用one-hot encoder，这种方式忽略了句子当中词语的次序，但是将每个词装换成为一个列表，词语之间的距离都为1。例如’我喜欢你’，转化成3个词：’’我’，’喜欢，’你’，三个word分别可以表示为可以表示为&lt;code&gt;[1,0,0],[0,1,0],[0,0,1]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;现在已经可以将句子中的词转化成为数值型数据，输入到模型中，为什么还要word2vec呢？&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://peihao.space/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://peihao.space/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>sql复习</title>
    <link href="http://peihao.space/2018/04/01/sql-review/"/>
    <id>http://peihao.space/2018/04/01/sql-review/</id>
    <published>2018-04-01T10:18:15.000Z</published>
    <updated>2018-04-20T10:31:24.097Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SQL">SQL</h1><p>数据查询语言：select、where、order by、group by、having</p><p>数据操作语言：insert、update、delete</p><p>事务语言：begin transaction、commit、rollback</p><p>数据定义语言：create table、drop table；为表加入索引</p><p>数据类型：字符型、文本型、数值型、逻辑型、日期型</p><p>字符型：varchar占用少内存、硬盘；不会多出来多余的后面的空格；存储小于255字符</p><p>文本型：可存放超过20亿字符的串；尽量避免使用；即时是空值，也会分配2K空间</p><p>数值型：int、numeric（范围最大）、money钱数</p><a id="more"></a><p>逻辑型：bit</p><p>日期型：datetime范围大，且精确到毫秒；smalldatatime范围小，精确到秒</p><h2 id="语法">语法</h2><p>SELECT - 从数据库中提取数据<br>UPDATE - 更新数据库中的数据<br>DELETE - 从数据库中删除数据<br>INSERT INTO - 向数据库中插入新数据<br>CREATE DATABASE - 创建新数据库<br>ALTER DATABASE - 修改数据库<br>CREATE TABLE - 创建新表<br>ALTER TABLE - 变更（改变）数据库表<br>DROP TABLE - 删除表<br>CREATE INDEX - 创建索引（搜索键）<br>DROP INDEX - 删除索引</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#结合where条件差查询</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> [*] <span class="keyword">FROM</span> [TableName] <span class="keyword">WHERE</span> [condition1]</span><br><span class="line">#子条件查询</span><br><span class="line"><span class="keyword">SELECT</span> [*] <span class="keyword">FROM</span> [TableName] <span class="keyword">WHERE</span> [condition1] [<span class="keyword">AND</span> [<span class="keyword">OR</span>]] [condition2]...</span><br><span class="line">#排序后查询</span><br><span class="line"><span class="keyword">SELECT</span> column_name() <span class="keyword">FROM</span> table_name <span class="keyword">ORDER</span> <span class="keyword">BY</span> column_name() <span class="keyword">ASC</span> <span class="keyword">or</span> <span class="keyword">DESC</span></span><br><span class="line">#对某表插入数据</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> table_name (<span class="keyword">column</span>, column1, column2, column3, ...) <span class="keyword">VALUES</span> (<span class="keyword">value</span>, value1, value2, value3 ...)</span><br><span class="line">#更新数据</span><br><span class="line"><span class="keyword">UPDATE</span> table_name <span class="keyword">SET</span> <span class="keyword">column</span>=<span class="keyword">value</span>, column1=value1,... <span class="keyword">WHERE</span> someColumn=someValue</span><br><span class="line">#删除数据</span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> tableName <span class="keyword">WHERE</span> someColumn = someValue</span></span><br></pre></td></tr></table></figure><p>选择属性不同的值：select distinct</p><p><code>select distinct column1,column2,... from table;</code></p><p>SQL在文本周围使用单引号；大部分也接受双引号；<br>数值字段不使用引号</p><h3 id="where运算符">where运算符</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">=等于</span><br><span class="line"><span class="tag">&lt;&gt;</span>不等于。 注意：在某些版本的SQL中，这个操作符可能写成！=</span><br><span class="line">&gt;大于</span><br><span class="line"><span class="tag">&lt;小于</span><br><span class="line">&gt;</span>=大于等于</span><br><span class="line"><span class="tag">&lt;<span class="title">=</span>小于等于</span><br><span class="line"><span class="attribute">BETWEEN</span>在某个范围内</span><br><span class="line"><span class="attribute">LIKE</span>搜索某种模式</span><br><span class="line"><span class="attribute">IN</span>为列指定多个可能的值</span></span><br></pre></td></tr></table></figure><p>###条件过滤</p><p>and、or、not</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">WHERE</span> <span class="keyword">NOT</span> Country=<span class="string">'Germany'</span> <span class="keyword">AND</span> <span class="keyword">NOT</span> Country=<span class="string">'USA'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">WHERE</span> Country=<span class="string">'Germany'</span> <span class="keyword">AND</span> (City=<span class="string">'Berlin'</span> <span class="keyword">OR</span> City=<span class="string">'München'</span>);</span></span><br></pre></td></tr></table></figure><h3 id="排序">排序</h3><p>order by| asc/desc</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">ORDER</span> <span class="keyword">BY</span> Country <span class="keyword">DESC</span>;</span></span><br></pre></td></tr></table></figure><p>多列：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">ORDER</span> <span class="keyword">BY</span> Country, CustomerName;</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">ORDER</span> <span class="keyword">BY</span> Country <span class="keyword">ASC</span>, CustomerName <span class="keyword">DESC</span>;</span></span><br></pre></td></tr></table></figure><h3 id="插入">插入</h3><p>自动递增字段不需要插入，会自动生成</p><p>如果要为表中的所有列添加值，则不需要在SQL查询中指定列名称。但是，请确保值的顺序与表中的列顺序相同</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> table_name (column1, column2, column3, ...) <span class="keyword">VALUES</span> (value1, value2, value3,</span><br><span class="line">...);</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> table_name <span class="keyword">VALUES</span> (value1, value2, value3, ...);</span></span><br></pre></td></tr></table></figure><h3 id="NULL">NULL</h3><p>NULL 用于表示缺失的值。数据表中的 NULL 值表示该值所处的字段为空。</p><p>如果表中的字段是可选的，则可以插入新记录或更新记录而不向该字段添加值。然后，该字段将被保存为NULL值。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> column_names <span class="keyword">FROM</span> table_name <span class="keyword">WHERE</span> column_name <span class="keyword">IS</span> <span class="literal">NULL</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> column_names <span class="keyword">FROM</span> table_name <span class="keyword">WHERE</span> column_name <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>;</span></span><br></pre></td></tr></table></figure><h3 id="UPDATE">UPDATE</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">UPDATE</span> table_name <span class="keyword">SET</span> column1 = value1, column2 = value2, ... <span class="keyword">WHERE</span> condition;</span></span><br></pre></td></tr></table></figure><p>省略where会更新表中所有行的*数据</p><h2 id="DELETE">DELETE</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> Customers <span class="keyword">WHERE</span> CustomerName=<span class="string">'Alfreds Futterkiste'</span>;</span></span><br></pre></td></tr></table></figure><p>删除所有数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">table</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">delete</span> * <span class="keyword">from</span> <span class="keyword">table</span>;</span></span><br></pre></td></tr></table></figure><h3 id="TOP">TOP</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#mysql</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">LIMIT</span> <span class="number">5</span>;</span></span><br><span class="line">#sql</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> TOP <span class="number">2</span> * <span class="keyword">FROM</span> Customers;</span></span><br></pre></td></tr></table></figure><p>选择前5个返回</p><h3 id="Like">Like</h3><p>在WHERE子句中使用LIKE运算符来搜索列中的指定模式。</p><p>％ - 百分号表示零个，一个或多个字符<br>_ - 下划线表示单个字符</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/28491796.jpg" alt=""></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#选择客户名称在第二位具有“r”的所有客户：</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">WHERE</span> CustomerName <span class="keyword">LIKE</span> <span class="string">'_r%'</span>;</span></span><br></pre></td></tr></table></figure><ul><li>[charlist] 通配符</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#所有客户City以"b"、"s"或"p"开头：</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers  <span class="keyword">WHERE</span> City <span class="keyword">LIKE</span> <span class="string">'[bsp]%'</span>;</span></span><br><span class="line">#以a/b/c开头</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers  <span class="keyword">WHERE</span> City <span class="keyword">LIKE</span> <span class="string">'[a-c]%'</span>;</span></span><br><span class="line">#所有客户City不以"b"、"s"或"p"开头：</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers  <span class="keyword">WHERE</span> City <span class="keyword">LIKE</span> <span class="string">'[!bsp]%'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">WHERE</span> City <span class="keyword">NOT</span> <span class="keyword">LIKE</span> <span class="string">'[bsp]%'</span>;</span></span><br></pre></td></tr></table></figure><h3 id="in">in</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">WHERE</span> Country <span class="keyword">IN</span> (<span class="string">'Germany'</span>, <span class="string">'France'</span>, <span class="string">'UK'</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">WHERE</span> Country <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="string">'Germany'</span>, <span class="string">'France'</span>, <span class="string">'UK'</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Customers <span class="keyword">WHERE</span> Country <span class="keyword">IN</span> (<span class="keyword">SELECT</span> Country <span class="keyword">FROM</span> Suppliers);</span></span><br></pre></td></tr></table></figure><h3 id="Between">Between</h3><p>BETWEEN 操作符用于选取介于两个值之间的数据范围内的值。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Products <span class="keyword">WHERE</span> Price <span class="keyword">BETWEEN</span> <span class="number">10</span> <span class="keyword">AND</span> <span class="number">20</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Products <span class="keyword">WHERE</span> Price <span class="keyword">NOT</span> <span class="keyword">BETWEEN</span> <span class="number">10</span> <span class="keyword">AND</span> <span class="number">20</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Products <span class="keyword">WHERE</span> (Price <span class="keyword">BETWEEN</span> <span class="number">10</span> <span class="keyword">AND</span> <span class="number">20</span>) <span class="keyword">AND</span> <span class="keyword">NOT</span> CategoryID <span class="keyword">IN</span> (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Products <span class="keyword">WHERE</span> ProductName <span class="keyword">BETWEEN</span> <span class="string">'Carnarvon Tigers'</span> <span class="keyword">AND</span> <span class="string">'Mozzarella di Giovanni'</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> ProductName;</span></span><br></pre></td></tr></table></figure><h3 id="一些操作">一些操作</h3><p>alter</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name  <span class="keyword">ADD</span> column_name datatype</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name  <span class="keyword">DROP</span> <span class="keyword">COLUMN</span> column_name</span></span><br></pre></td></tr></table></figure><p>as别名</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name <span class="keyword">AS</span> column_alias <span class="keyword">FROM</span> table_name</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> column_name <span class="keyword">FROM</span> table_name <span class="keyword">AS</span> table_alias</span><br></pre></td></tr></table></figure><p>create</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> database_name;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_name</span><br><span class="line">(column_name1 data_type,</span><br><span class="line">column_name2 data_type,</span><br><span class="line">column_name2 data_type,</span><br><span class="line">...)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> index_name</span><br><span class="line"><span class="keyword">ON</span> table_name (column_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">UNIQUE</span> <span class="keyword">INDEX</span> index_name</span><br><span class="line"><span class="keyword">ON</span> table_name (column_name)</span></span><br></pre></td></tr></table></figure><p>view</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_name <span class="keyword">AS</span> <span class="keyword">SELECT</span> column_name(s) <span class="keyword">FROM</span> table_name <span class="keyword">WHERE</span> condition</span></span><br></pre></td></tr></table></figure><p>drop</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">DROP</span> <span class="keyword">DATABASE</span> database_name</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">INDEX</span> index_name</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> table_name</span></span><br></pre></td></tr></table></figure><p>group by</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> column_name, aggregate_function(column_name) <span class="keyword">FROM</span> table_name <span class="keyword">WHERE</span> column_name <span class="keyword">operator</span> <span class="keyword">value</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> column_name</span><br><span class="line"></span><br><span class="line">#统计男女的数量</span><br><span class="line"><span class="keyword">SELECT</span> sex, <span class="keyword">count</span>(sex) <span class="keyword">FROM</span> table_name <span class="keyword">WHERE</span> sex <span class="keyword">in</span> (<span class="string">'man'</span>,<span class="string">'woman'</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> sex</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(CustomerID), Country</span><br><span class="line"><span class="keyword">FROM</span> Customers</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> Country</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">COUNT</span>(CustomerID) <span class="keyword">DESC</span>;</span></span><br></pre></td></tr></table></figure><p><code>aggregate_function</code>是sum、min之类的</p><p>having</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> column_name, aggregate_function(column_name) <span class="keyword">FROM</span> table_name <span class="keyword">WHERE</span> column_name <span class="keyword">operator</span> <span class="keyword">value</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> column_name <span class="keyword">HAVING</span> aggregate_function(column_name) <span class="keyword">operator</span> <span class="keyword">value</span></span></span><br></pre></td></tr></table></figure><p>join</p><p>INNER JOIN：如果表中有至少一个匹配，则返回行。从多个表中返回满足 JOIN 条件的所有行。如果表中有一个匹配项，INNER JOIN 关键字将返回一行。如果on的条件不匹配不返回</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> orders.orderid,customers.customerName,orders.orderDate <span class="keyword">from</span> orders <span class="keyword">inner</span> <span class="keyword">join</span> customers <span class="keyword">on</span> customers.customerid=orders.customerid</span><br><span class="line"></span><br><span class="line">#三张表<span class="keyword">inner</span> <span class="keyword">join</span></span><br><span class="line"><span class="keyword">select</span> orders.orderid,customers.customerName,shippers.shippername <span class="keyword">from</span> (orders <span class="keyword">inner</span> <span class="keyword">join</span> customers <span class="keyword">on</span> orders.customerid=customers,customerid) <span class="keyword">inner</span> <span class="keyword">join</span> shippers <span class="keyword">on</span> orders.shipperid=shippers.shipperid</span></span><br></pre></td></tr></table></figure><p>LEFT JOIN：即使右表中没有匹配，也从左表返回所有的行。如果不匹配，对应的结果是NULL</p><p>结果集尺寸在左表尺寸</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> column_name(s) <span class="keyword">FROM</span> table_name1 <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> table_name2  <span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span></span><br></pre></td></tr></table></figure><p>RIGHT JOIN：即使左表中没有匹配，也从右表返回所有的行。结果集尺寸在右表尺寸</p><p>FULL JOIN：只要其中一个表中存在匹配，则返回左右所有的记录，是非常大的结果集。没有匹配一行的话，结果集不返回；只有出现一行匹配，就返回会所有。假设匹配了k行，结果集尺寸为左表尺寸+右表尺寸-k</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/31840677.jpg" alt=""></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> column_name(s) <span class="keyword">FROM</span> table_name1 <span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> table_name2  <span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> column_name(s) <span class="keyword">FROM</span> table_name1 <span class="keyword">FULL</span> <span class="keyword">JOIN</span> table_name2  <span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span></span><br></pre></td></tr></table></figure><p>select into</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#根据查询创建新表</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">INTO</span> new_table_name [<span class="keyword">IN</span> externaldatabase] <span class="keyword">FROM</span> old_table_name</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> column_name(s) <span class="keyword">INTO</span> new_table_name [<span class="keyword">IN</span> externaldatabase] <span class="keyword">FROM</span> old_table_name</span></span><br></pre></td></tr></table></figure><p>union</p><p>UNION中的每个SELECT语句必须具有相同的列数<br>这些列也必须具有相似的数据类型<br>每个SELECT语句中的列也必须以相同的顺序排列</p><p>union选择的是不同的值；union all选择的可以是重复值；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> column_name(s) <span class="keyword">FROM</span> table_name1 <span class="keyword">UNION</span> <span class="keyword">SELECT</span> column_name(s) <span class="keyword">FROM</span> table_name2</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> column_name(s) <span class="keyword">FROM</span> table_name1 <span class="keyword">UNION</span> ALL <span class="keyword">SELECT</span> column_name(s) <span class="keyword">FROM</span> table_name2</span></span><br></pre></td></tr></table></figure><p> 自连接</p><p>匹配来自同一城市的客户<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> A.CustomerName <span class="keyword">AS</span> CustomerName1, B.CustomerName <span class="keyword">AS</span> CustomerName2, A.City</span><br><span class="line"><span class="keyword">FROM</span> Customers A, Customers B</span><br><span class="line"><span class="keyword">WHERE</span> A.CustomerID &lt;&gt; B.CustomerID</span><br><span class="line"><span class="keyword">AND</span> A.City = B.City</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> A.City;</span></span><br></pre></td></tr></table></figure></p><p>insert into</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> Customers (CustomerName, Country)</span><br><span class="line"><span class="keyword">SELECT</span> SupplierName, Country <span class="keyword">FROM</span> Suppliers;</span></span><br></pre></td></tr></table></figure><h2 id="约束">约束</h2><p>约束是作用于数据表中列上的规则，用于限制表中数据的类型。约束的存在保证了数据库中数据的精确性和可靠性。</p><p>约束有列级和表级之分，列级约束作用于单一的列，而表级约束作用于整张数据表。</p><p>NOT NULL 约束：保证列中数据不能有 NULL 值<br>DEFAULT 约束：提供该列数据未指定时所采用的默认值<br>UNIQUE 约束：保证列中的所有数据各不相同<br>主键约束：唯一标识数据表中的行/记录<br>外键约束：唯一标识其他表中的一条行/记录<br>CHECK 约束：此约束保证列中的所有值满足某一条件<br>索引：用于在数据库中快速创建或检索数据</p><p>约束可以在创建表时规定（通过 CREATE TABLE 语句），或者在表创建之后规定（通过 ALTER TABLE 语句）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Persons</span><br><span class="line"></span><br><span class="line">(</span><br><span class="line"></span><br><span class="line">P_Id int NOT NULL,</span><br><span class="line"></span><br><span class="line">LastName varchar(<span class="number">255</span>) NOT NULL,</span><br><span class="line"></span><br><span class="line">FirstName varchar(<span class="number">255</span>),</span><br><span class="line"></span><br><span class="line">Address varchar(<span class="number">255</span>),</span><br><span class="line"></span><br><span class="line">City varchar(<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>UNIQUE 和 PRIMARY KEY 约束均为列或列集合提供了唯一性的保证。</p><p>PRIMARY KEY 约束拥有自动定义的 UNIQUE 约束。</p><p>请注意，每个表可以有多个 UNIQUE 约束，但是每个表只能有一个 PRIMARY KEY 约束。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Persons</span><br><span class="line"></span><br><span class="line">(</span><br><span class="line"></span><br><span class="line">P_Id <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"></span><br><span class="line">LastName <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">UNIQUE</span> (P_Id)</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> Persons <span class="keyword">ADD</span> <span class="keyword">UNIQUE</span> (P_Id)</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> Persons <span class="keyword">DROP</span> <span class="keyword">INDEX</span> uc_PersonID</span></span><br></pre></td></tr></table></figure><p>主键：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PRIMARY KEY (P_Id)</span><br><span class="line"></span><br><span class="line">CONSTRAINT pk_PersonID PRIMARY KEY (P_Id,LastName)</span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> Persons <span class="keyword">ADD</span> <span class="keyword">CONSTRAINT</span> pk_PersonID PRIMARY <span class="keyword">KEY</span> (P_Id,LastName)</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> Persons <span class="keyword">ADD</span> PRIMARY <span class="keyword">KEY</span> (P_Id)</span></span><br></pre></td></tr></table></figure><p>一个表中的 FOREIGN KEY 指向另一个表中的 PRIMARY KEY。</p><p><code>FOREIGN KEY (P_Id) REFERENCES Persons(P_Id)</code></p><p><code>CONSTRAINT fk_PerOrders FOREIGN KEY (P_Id) REFERENCES Persons(P_Id)</code></p><p><code>CONSTRAINT chk_Person CHECK (P_Id&gt;0 AND City=&#39;Sandnes&#39;)</code></p><h2 id="克隆表">克隆表</h2><ol><li><code>SHOW CREATE TABLE tb_name</code></li><li>返回的结果，表名修改为新表，执行</li><li><code>INSERT INTO new_tb... SELECT</code></li></ol><h2 id="索引">索引</h2><p>索引能够提高 SELECT 查询和 WHERE 子句的速度，但是却降低了包含 UPDATE 语句或 INSERT 语句的数据输入过程的速度。索引的创建与删除不会对表中的数据产生影响。</p><p><code>CREATE INDEX index_name ON table_name;</code></p><p>单列：</p><p><code>CREATE INDEX index_name ON table_name (column_name);</code></p><p>唯一索引：</p><p>唯一索引不止用于提升查询性能，还用于保证数据完整性。唯一索引不允许向表中插入任何重复值。其基本语法如下所示：</p><p><code>CREATE UNIQUE INDEX index_name on table_name (column_name);</code></p><p>聚簇索引：</p><p>聚簇索引在表中两个或更多的列的基础上建立。其基本语法如下所示：</p><p><code>CREATE INDEX index_name on table_name (column1, column2);</code></p><p>创建单列索引还是聚簇索引，要看每次查询中，哪些列在作为过滤条件的 WHERE 子句中最常出现。</p><p>如果只需要一列，那么就应当创建单列索引。如果作为过滤条件的 WHERE 子句用到了两个或者更多的列，那么聚簇索引就是最好的选择。</p><p><strong>隐式索引由数据库服务器在创建某些对象的时候自动生成。例如，对于主键约束和唯一约束，数据库服务器就会自动创建索引。</strong></p><p>小的数据表不应当使用索引；<br>需要频繁进行大批量的更新或者插入操作的表；<br>如果列中包含大数或者 NULL 值，不宜创建索引；<br>频繁操作的列不宜创建索引。</p><h2 id="子查询">子查询</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> *</span><br><span class="line">     <span class="keyword">FROM</span> CUSTOMERS</span><br><span class="line">     <span class="keyword">WHERE</span> <span class="keyword">ID</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> <span class="keyword">ID</span></span><br><span class="line">                  <span class="keyword">FROM</span> CUSTOMERS</span><br><span class="line">                  <span class="keyword">WHERE</span> SALARY &gt; <span class="number">4500</span>) ;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> CUSTOMERS_BKP</span><br><span class="line">     <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> CUSTOMERS</span><br><span class="line">     <span class="keyword">WHERE</span> <span class="keyword">ID</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> <span class="keyword">ID</span></span><br><span class="line">                  <span class="keyword">FROM</span> CUSTOMERS) ;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">UPDATE</span> CUSTOMERS</span><br><span class="line">     <span class="keyword">SET</span> SALARY = SALARY * <span class="number">0.25</span></span><br><span class="line">     <span class="keyword">WHERE</span> AGE <span class="keyword">IN</span> (<span class="keyword">SELECT</span> AGE <span class="keyword">FROM</span> CUSTOMERS_BKP</span><br><span class="line">                   <span class="keyword">WHERE</span> AGE &gt;= <span class="number">27</span> );</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> CUSTOMERS</span><br><span class="line">     <span class="keyword">WHERE</span> AGE <span class="keyword">IN</span> (<span class="keyword">SELECT</span> AGE <span class="keyword">FROM</span> CUSTOMERS_BKP</span><br><span class="line">                   <span class="keyword">WHERE</span> AGE &gt; <span class="number">27</span> );</span></span><br></pre></td></tr></table></figure><h2 id="ALTER">ALTER</h2><p> ALTER TABLE 命令用于添加、删除或者更改现有数据表中的列。</p><p>你还可以用 ALTER TABLE 命令来添加或者删除现有数据表上的约束。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> CUSTOMERS <span class="keyword">ADD</span> SEX <span class="built_in">char</span>(<span class="number">1</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> CUSTOMERS <span class="keyword">DROP</span> SEX;</span></span><br></pre></td></tr></table></figure><h2 id="TRUNCATE">TRUNCATE</h2><p>可以使用 DROP TABLE 命令来删除整个数据表，不过 DROP TABLE 命令不但会删除表中所有数据，还会将整个表结构从数据库中移除。如果想要重新向表中存储数据的话，必须重建该数据表。</p><p>TRUNCATE TABLE 命令用于删除现有数据表中的所有数据。</p><p><code>TRUNCATE TABLE  table_name;</code></p><h2 id="View">View</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_name <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> column1, column2.....</span><br><span class="line"><span class="keyword">FROM</span> table_name</span><br><span class="line"><span class="keyword">WHERE</span> [condition];</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> CUSTOMERS_VIEW <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">name</span>, age</span><br><span class="line"><span class="keyword">FROM</span>  CUSTOMERS;</span></span><br></pre></td></tr></table></figure><p>WITH CHECK OPTION 是 CREATE VIEW 语句的一个可选项。WITH CHECK OPTION 用于保证所有的 UPDATE 和 INSERT 语句都满足视图定义中的条件。</p><p>如果不能满足这些条件，UPDATE 或 INSERT 就会返回错误。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> CUSTOMERS_VIEW <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">name</span>, age</span><br><span class="line"><span class="keyword">FROM</span>  CUSTOMERS</span><br><span class="line"><span class="keyword">WHERE</span> age <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="literal">NULL</span></span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CHECK</span> <span class="keyword">OPTION</span>;</span></span><br></pre></td></tr></table></figure><p>这里 WITH CHECK OPTION 使得视图拒绝任何 AGE 字段为 NULL 的条目，因为视图的定义中，AGE 字段不能为空。</p><p>视图更新：</p><p>select字句不包含distinct、汇总函数、集合函数、集合运算、order by、子查询、group by、having；所有列都是not null</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">UPDATE</span> CUSTOMERS_VIEW</span><br><span class="line">      <span class="keyword">SET</span> AGE = <span class="number">35</span></span><br><span class="line">      <span class="keyword">WHERE</span> <span class="keyword">name</span>=<span class="string">'Ramesh'</span>;</span></span><br></pre></td></tr></table></figure><h2 id="having字句">having字句</h2><p>HAVING 子句使你能够指定过滤条件，从而<strong>控制查询结果中哪些组</strong>可以出现在最终结果里面。</p><p>WHERE 子句对被选择的列施加条件，而 HAVING 子句则对 <strong>GROUP BY</strong> 子句所产生的组施加条件。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line"><span class="keyword">HAVING</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span></span></span><br></pre></td></tr></table></figure><p>选择某个年龄段，这个年龄的人数》=2</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">ID</span>, <span class="keyword">NAME</span>, AGE, ADDRESS, SALARY</span><br><span class="line"><span class="keyword">FROM</span> CUSTOMERS</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> age</span><br><span class="line"><span class="keyword">HAVING</span> <span class="keyword">COUNT</span>(age) &gt;= <span class="number">2</span>;</span></span><br></pre></td></tr></table></figure><h2 id="事务">事务</h2><p>事务具有以下四个标准属性，通常用缩略词 ACID 来表示：</p><p>原子性：保证任务中的所有操作都执行完毕；否则，事务会在出现错误时终止，并回滚之前所有操作到原始状态。<br>一致性：如果事务成功执行，则数据库的状态得到了进行了正确的转变。<br>隔离性：保证不同的事务相互独立、透明地执行。<br>持久性：即使出现系统故障，之前成功执行的事务的结果也会持久存在。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">COMMIT</span>：提交更改；</span><br><span class="line"><span class="keyword">ROLLBACK</span>：回滚更改；</span><br><span class="line"><span class="keyword">SAVEPOINT</span>：在事务内部创建一系列可以 <span class="keyword">ROLLBACK</span> 的还原点；</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">TRANSACTION</span>：命名事务；</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SAVEPOINT</span> SP1;</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="operator"><span class="keyword">ROLLBACK</span> <span class="keyword">TO</span> SP1;</span></span><br><span class="line"><span class="operator"><span class="keyword">RELEASE</span> <span class="keyword">SAVEPOINT</span> SP1</span></span><br></pre></td></tr></table></figure><p>SET TRANSACTION 命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SET</span> <span class="keyword">TRANSACTION</span> [ <span class="keyword">READ</span> WRITE | <span class="keyword">READ</span> <span class="keyword">ONLY</span> ];</span></span><br></pre></td></tr></table></figure><p><strong>临时表有时候对于保存临时数据非常有用。有关临时表你需要知道的最重要的一点是，它们会在当前的终端会话结束后被删除。</strong></p><h2 id="Mysql数据类型">Mysql数据类型</h2><p>TEXT    存放最大长度为 65,535 个字符的字符串。2^16<br>BLOB    用于 BLOBs（Binary Large OBjects）。存放最多 65,535 字节的数据。二进制</p><p>MEDIUMTEXT 2^24<br>MEDIUMBLOB</p><p>LONGTEXT 2^32<br>LONGBLOB</p><h2 id="函数">函数</h2><p>MAX() 函数返回所选列的最大值。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">MAX</span>(column_name)</span><br><span class="line"><span class="keyword">FROM</span> table_name</span><br><span class="line"><span class="keyword">WHERE</span> condition;</span></span><br></pre></td></tr></table></figure><p>MIN()函数</p><p>COUNT() 函数返回符合指定条件的行数。</p><p>AVG() 函数返回<strong>数字列</strong>的平均值。</p><p>SUM() 函数返回数字列的总和。</p><p>FIRST() - 函数返回指定的列中最后一个记录的值。<br>LAST() - 函数返回指定的列中第一个记录的值。</p><p>FIELD()函数：</p><p>类似find函数，返回index</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">FIELD</span>(<span class="string">'ej'</span>, <span class="string">'Hej'</span>, <span class="string">'ej'</span>, <span class="string">'Heja'</span>, <span class="string">'hej'</span>, <span class="string">'foo'</span>);</span></span><br></pre></td></tr></table></figure><p>在<code>[&#39;Hej&#39;, &#39;ej&#39;, &#39;Heja&#39;, &#39;hej&#39;, &#39;foo&#39;]</code>中查找<code>ej</code>，得到结果2（初始index=1），没有找到就返回0</p><p>字符串函数：</p><p>LOWER(s)或者LCASE(s)函数可以将字符串s中的字母字符全部转换成小写字母。<br>UPPER(s)或UCASE(s)函数可以将字符串s中的字母字符全部转换成大写字母。</p><p>MID函数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">MID</span>(City,<span class="number">1</span>,<span class="number">4</span>) <span class="keyword">AS</span> ShortCity</span><br><span class="line"><span class="keyword">FROM</span> Customers;</span></span><br></pre></td></tr></table></figure><p>返回的city，只取前4个字符</p><p>ROUND函数</p><p>函数用于把数值字段舍入为指定的小数位数。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">ROUND</span>(column_name,decimals) <span class="keyword">FROM</span> table_name;</span></span><br><span class="line">decimals必需。规定要返回的小数位数。可以为0</span><br></pre></td></tr></table></figure><p>FORMAT函数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> ProductName, Price, <span class="keyword">FORMAT</span>(<span class="keyword">Now</span>(),<span class="string">'YYYY-MM-DD'</span>) <span class="keyword">AS</span> PerDate</span><br><span class="line"><span class="keyword">FROM</span> Products;</span></span><br></pre></td></tr></table></figure><p>sqrt平方根<br>rand()返回0-1随机数<br>concat字符串连接</p><p>Replace字符串替换</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">update</span> <span class="string">`article`</span> <span class="keyword">set</span> title=<span class="keyword">replace</span>(title,<span class="string">'w3cschool'</span>,<span class="string">'hello'</span>);</span></span><br></pre></td></tr></table></figure><h2 id="Mysql面试题">Mysql面试题</h2><h3 id="索引-1">索引</h3><p>组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。创建复合索引时应该将最常用（频率）作限制条件的列放在最左边，依次递减。</p><p>唯一索引允许空值</p><p>全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时好空间。</p><p>为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。创建<strong>复合索引</strong>时应该将最常用（频率）作限制条件的列放在最左边，依次递减。</p><p>hsah索引把数据的索引以hash形式组织起来，因此当查找某一条记录的时候,速度非常快。当时因为是hash结构，每个键只对应一个值，而且是散列的方式分布。所以他并不支持范围查找和排序等功能。</p><p>B+tree是mysql使用最频繁的一个索引数据结构，是Inodb和Myisam存储引擎模式的索引类型。相对Hash索引，B+树在查找单条记录的速度比不上Hash索引，但是因为更适合排序等操作，所以他更受用户的欢迎。毕竟不可能只对数据库进行单条记录的操作。</p><h3 id="InnoDB/MYISAM">InnoDB/MYISAM</h3><p>1&gt;.InnoDB支持事物，而MyISAM不支持事物</p><p>2&gt;.InnoDB支持行级锁，而MyISAM支持表级锁</p><p>3&gt;.InnoDB支持MVCC, 而MyISAM不支持</p><p>4&gt;.InnoDB支持外键，而MyISAM不支持</p><p>5&gt;.InnoDB不支持全文索引，而MyISAM支持。</p><p> InnoDB支持事务，MyISAM不支持，对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务；</p><p>InnoDB支持外键，对一个包含外键的InnoDB表转为MYISAM会失败</p><p>Innodb不支持全文索引，而MyISAM支持全文索引，查询效率上MyISAM要高；</p><p>InnoDB是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而MyISAM是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。</p><p>myisam引擎,记录数count是结构的一部分,已经cache在内存中了,很快就可以得到结构,而innodb仍然需要计算,id如果是主键索引的话,无疑会加快速度;</p><p>是否要支持事务，如果要请选择innodb，如果不需要可以考虑MyISAM；<br>如果表中绝大多数都只是读查询，可以考虑MyISAM，如果既有读写也挺频繁，请使用InnoDB。</p><h3 id="其他">其他</h3><p>drop、delete、truncate区别</p><p> TRUNCATE 和DELETE只删除数据，而DROP则删除整个表（结构和数据）。</p><p> delete语句为DML（data maintain Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。</p><p> truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚</p><p>视图不能被索引，也不能有关联的触发器或默认值</p><p>1）应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。<br>2）应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：<br>select id from t where num is null<br>可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：<br>select id from t where num=0<br>3）很多时候用 exists 代替 in 是一个好的选择<br>4）用Where子句替换HAVING 子句 因为HAVING 只会在检索出所有记录之后才对结果集进行过滤</p><p>会员备注信息 ， 如果需要建索引的话，可以选择 FULLTEXT，全文搜索。</p><p>不过 FULLTEXT 用于搜索很长一篇文章的时候，效果最好。<br>用在比较短的文本，如果就一两行字的，普通的 INDEX 也可以。</p><p>全文检索的索引被称为倒排索引，之所以成为倒排索引，是因为将每一个单词作为索引项，根据该索引项查找包含该单词的文本。因此，索引都是单词和唯一记录文本的标示是一对多的关系。将索引单词排序，根据排序后的单词定位包含该单词的文本。</p><h3 id="explain">explain</h3><p>在查询语句前加expalin；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">explain</span> <span class="keyword">SELECT</span> <span class="keyword">MID</span>(City,<span class="number">1</span>,<span class="number">4</span>) <span class="keyword">AS</span> ShortCity</span><br><span class="line"><span class="keyword">FROM</span> Customers <span class="keyword">where</span> <span class="keyword">length</span>(City)&gt;<span class="number">3</span> <span class="keyword">UNION</span> <span class="keyword">SELECT</span> <span class="keyword">MID</span>(City,<span class="number">0</span>,<span class="number">4</span>) <span class="keyword">AS</span> ShortCity</span><br><span class="line"><span class="keyword">FROM</span> Customers <span class="keyword">where</span> <span class="keyword">length</span>(City)&lt;<span class="number">4</span>;</span></span><br></pre></td></tr></table></figure><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/32620040.jpg" alt=""></p><p>d是一组数字，表示查询中执行select子句或操作表的顺序。</p><p>selecttype有simple，primary，subquery，derived(衍生)，union，unionresult。</p><p>type叫访问类型，表示在表中找到所需行的方式，常见类型有all，index，range，ref，eq_ref，const，system，NULL 性能从做至右由差至好。</p><p>possiblekey表示能使用哪个索引在表中找到行，查询涉及到的字段上若存在索引，则该索引被列出，但不一定被查询使用。</p><p>key表示查询时使用的索引。若查询中使用了覆盖索引，则该索引仅出现在key</p><p>keylen表示索引所使用的字节数，可以通过该列结算查询中使用的索引长度</p><h3 id="其他-1">其他</h3><p>MySQL数据库cpu飙升到500%的话他怎么处理</p><p>列出所有进程  show processlist  观察所有进程  多秒没有状态变化的(干掉)<br>查看超时日志或者错误日志 (做了几年开发,一般会是查询以及大批量的插入会导致cpu与i/o上涨,,,,当然不排除网络状态突然断了,,导致一个请求服务器只接受到一半，比如where子句或分页子句没有发送,,当然的一次被坑经历)</p><p>备份恢复时间：</p><p>20G的2分钟（mysqldump）<br>80G的30分钟(mysqldump)<br>111G的30分钟（mysqldump)<br>288G的3小时（xtra)<br>3T的4小时（xtra)</p><p>主从一致性校验有多种工具 例如checksum、mysqldiff、pt-table-checksum等</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;SQL&quot;&gt;SQL&lt;/h1&gt;&lt;p&gt;数据查询语言：select、where、order by、group by、having&lt;/p&gt;
&lt;p&gt;数据操作语言：insert、update、delete&lt;/p&gt;
&lt;p&gt;事务语言：begin transaction、commit、rollback&lt;/p&gt;
&lt;p&gt;数据定义语言：create table、drop table；为表加入索引&lt;/p&gt;
&lt;p&gt;数据类型：字符型、文本型、数值型、逻辑型、日期型&lt;/p&gt;
&lt;p&gt;字符型：varchar占用少内存、硬盘；不会多出来多余的后面的空格；存储小于255字符&lt;/p&gt;
&lt;p&gt;文本型：可存放超过20亿字符的串；尽量避免使用；即时是空值，也会分配2K空间&lt;/p&gt;
&lt;p&gt;数值型：int、numeric（范围最大）、money钱数&lt;/p&gt;
    
    </summary>
    
      <category term="面试" scheme="http://peihao.space/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="SQL" scheme="http://peihao.space/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>红黑/B+/234</title>
    <link href="http://peihao.space/2018/03/20/red-black-tree/"/>
    <id>http://peihao.space/2018/03/20/red-black-tree/</id>
    <published>2018-03-20T10:04:52.000Z</published>
    <updated>2018-04-20T10:31:39.381Z</updated>
    
    <content type="html"><![CDATA[<h1 id="红黑、B、B+">红黑、B、B+</h1><h2 id="二叉搜索树">二叉搜索树</h2><p>二叉查找树（Binary Search Tree），（又：二叉搜索树，二叉排序树）它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树。</p><a id="more"></a><h2 id="平衡二叉树">平衡二叉树</h2><p>平衡二叉搜索树（Self-balancing binary search tree）又被称为AVL树（有别于AVL算法），且具有以下性质：</p><p>是一 棵空树或左右子树的高度差绝对值不超过1，且左右子树都是一棵平衡二叉树，</p><p>平衡二叉树必定是二叉搜索树，反之则不一定。</p><p>一般的二叉搜索树，期望高度（即为一棵平衡树时）为log2n，各操作的时间复杂度（O(log2n)）也由此而决定。但在某些极端的情况下（如在插入的序列是有序的时），二叉搜索树将退化成近似链或链，此时，其操作的时间复杂度将退化成线性的，即O(n)。我们可以通过随机化建立二叉搜索树来尽量的避免这种情况，但在进行多次操作后，由于在删除时，我们总是选择将待删除节点的后继代替它本身，这样就会造成总是右边的节点数目减少，以至于树向左偏沉。这同时也会造成树的平衡性受到破坏，提高它的操作的时间复杂度。</p><p>在平衡二叉搜索树中，我们可以看到，其高度一般都良好地维持在O（log（n）），大大降低了操作的时间复杂度。</p><h2 id="2-3-4树">2-3-4树</h2><p>在二叉树中，每个节点有一个数据项，最多有两个子节点。如果允许每个节点可以有更多的数据项和更多的子节点，就是多叉树。2-3-4树就是多叉树，它的每个节点最多有四个子节点和三个数据项。</p><p>2-3-4树和红黑树一样是平衡树。它的效率比红黑树稍差，但编程容易。通过2-3-4树可以更容易地理解B-树。</p><p>B-树是另一种多叉树，专门用在外部存储中来组织数据。B-树中的节点可以有几十或几百个子节点。</p><p>2-3-4树名字中的2,3,4的含义是指一个节点可能含有的子节点的个数。对非叶子节点有三种可能的情况：</p><ul><li><p>有一个数据项的节点总是有两个子节点：sub1&lt;data1&lt;sub2</p></li><li><p>有两个数据项的节点总是有三个子节点：sub1&lt;data1&lt;sub2&lt;data2&lt;sub3</p></li><li><p>有三个数据项的节点总是有四个子节点：sub1&lt;data1&lt;sub2&lt;data2&lt;sub3&lt;data3&lt;sub4</p></li></ul><p>2-3-4树中所有<strong>叶结点总是在同一层</strong>，即叶子节点到根节点长度相同</p><p>查询：比较数值，递归查找</p><p>插入：比较查找范围，然后2节点变3节点，3节点表4节点。4节点情况需要将当前节点的中间节点上移一层到父节点。为了避免父节点也是4节点，需要控制4节点只会出现在最后一层。</p><h2 id="红黑树">红黑树</h2><p>典型的应用是实现<strong>关联数组</strong>、java中的<strong>TreeMap</strong>、<strong>TreeSet</strong>以及<strong>HashMap</strong>。关联数组不是顺序存储，不仅可以通过整数来索引，还可以使用字符串或其他类型的值来索引。</p><p>红黑树（Red Black Tree） 是一种<strong>自平衡二叉查找树</strong>，在进行插入和删除操作时通过特定操作保持二叉查找树的平衡，从而获得较高的查找性能。虽然复杂，但最坏情况运行时间良好： 它可以在O(log n)时间内做查找，插入和删除，这里的n 是树中元素的数目。</p><p>有5个性质：</p><ol><li>节点是红色或黑色。</li><li>根节点是黑色。</li><li>每个叶节点（NIL节点，空节点）是黑色的。</li><li>每个红色节点的两个子节点都是黑色。(即每个叶子到根的路径不能有两个连续的红色节点)</li><li>从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。</li></ol><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/87824488.jpg" alt=""></p><p>这些约束强制了红黑树的关键性质: <strong>从根到叶子的最长的可能路径不多于最短的可能路径的两倍长</strong>。结果是这个树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的</p><p>注意到性质4导致了路径不能有两个毗连的红色节点就足够了。最短的可能路径都是黑色节点，最长的可能路径有交替的红色和黑色节点。因为根据性质5所有最长的路径都有相同数目的黑色节点，这就表明了没有路径能多于任何其他路径的两倍长。</p><p>在红黑树上只读操作不需要对用于二叉查找树的操作做出修改，因为它也是二叉查找树。但是，在插入和删除之后，红黑属性可能变得违规。恢复红黑属性需要少量(O(log n))的颜色变更，并且不超过三次树旋转(对于插入是两次)。这允许插入和删除保持为 O(log n) 次，但是它导致了非常复杂的操作。</p><p>插入时可能会破坏红黑树的特性，需要进行调整，有两种策略：变色和旋转。</p><p>插入一次可能需要同时进行多次的变色和旋转。（旋转还分为左旋转右旋转）</p><h2 id="B树">B树</h2><ul><li><p>每个结点至多拥有m棵子树；</p></li><li><p>根结点至少拥有两颗子树（存在子树的情况下）；</p></li><li><p>除了根结点以外，其余每个分支结点至少拥有 m//2 棵子树；</p></li><li><p>所有的叶结点都在同一层上；</p></li><li><p>有 k 棵子树的分支结点则存在 k-1 个关键码，关键码按照递增次序进行排列；</p></li></ul><h3 id="插入">插入</h3><p>新结点一般插在第h层，通过搜索找到对应的结点进行插入，那么根据即将插入的结点的数量又分为下面几种情况。</p><ol><li>如果该结点的关键字个数没有到达m-1个，那么直接插入即可；</li><li>如果该结点的关键字个数已经到达了m-1个，那么根据B树的性质显然无法满足，需要将其进行分裂。分裂的规则是该结点分成两半，将中间的关键字进行提升，加入到父亲结点中，但是这又可能存在父亲结点也满员的情况，则不得不向上进行回溯，甚至是要对根结点进行分裂，那么整棵树都加了一层。</li></ol><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/97842376.jpg" alt=""></p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/64505982.jpg" alt=""></p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/82345964.jpg" alt=""></p><h3 id="删除">删除</h3><p>同样的，我们需要先通过搜索找到相应的值，存在则进行删除，需要考虑删除以后的情况，</p><ol><li>如果该结点拥有关键字数量仍然满足B树性质，则不做任何处理；</li><li>如果该结点在删除关键字以后不满足B树的性质（关键字没有到达ceil(m/2)-1的数量），则需要向兄弟结点借关键字，这有分为兄弟结点的关键字数量是否足够的情况：<ul><li>如果兄弟结点的关键字足够借给该结点，则过程为将父亲结点的关键字下移，兄弟结点的关键字上移；</li><li>如果兄弟结点的关键字在借出去以后也无法满足情况，即之前兄弟结点的关键字的数量为ceil(m/2)-1，借的一方的关键字数量为ceil(m/2)-2的情况，那么我们可以将该结点合并到兄弟结点中，合并之后的子结点数量少了一个，则需要将父亲结点的关键字下放，如果父亲结点不满足性质，则向上回溯；<br>其余情况参照BST中的删除。</li></ul></li></ol><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/70638689.jpg" alt=""></p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/62589104.jpg" alt=""></p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/92905830.jpg" alt=""></p><h2 id="B+树">B+树</h2><p>B+ 树通常用于数据库和操作系统的文件系统中。NTFS, ReiserFS, NSS, XFS, JFS, ReFS 和BFS等文件系统都在使用B+树作为元数据索引。B+ 树的特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。B+ 树元素自底向上插入。</p><p>B+树是应文件系统所需而出的一种B树的变型树。一棵m阶的B+树和m阶的B-树的差异在于：</p><ol><li>有n棵子树的结点含有n个关键字，每个关键字不保存数据，只用来索引，所有数据都保存在叶子节点。</li></ol><ol><li>所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。</li></ol><ol><li>所有的非终端结点可以看成是索引部分，结点中仅含其子树（根结点）中的最大（或最小）关键字。</li></ol><p>通常在B+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。</p><h3 id="对比">对比</h3><p>为什么说B+树比B 树更适合实际应用中操作系统的文件索引和数据库索引？</p><ul><li>B+树的磁盘读写代价更低</li></ul><p>B+树的内部结点并没有指向关键字具体信息的指针。因此其<strong>内部结点相对B 树更小</strong>。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。</p><p>举个例子，假设磁盘中的一个盘块容纳16bytes，而一个关键字2bytes，一个关键字具体信息指针2bytes。一棵9阶B-tree(一个结点最多8个关键字)的内部结点需要2个盘块。而B+树内部结点只需要1个盘块。当需要把内部结点读入内存中的时候，B 树就比B+树多一次盘块查找时间(在磁盘中就是盘片旋转的时间)。</p><ul><li>B+树的查询效率更加稳定</li></ul><p>由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致<strong>每一个数据的查询效率相当</strong>。</p><h2 id="具体应用">具体应用</h2><ul><li><p>AVL树: 最早的平衡二叉树之一。windows对进程地址空间的管理用到了AVL树。</p></li><li><p>红黑树: 平衡二叉树，广泛用在C++的STL中。如map/set/multimap。还有Java中的HashMap、TreeSet、TreeMap以及关联数组。</p></li></ul><hr><p>简单来说都是用来搜索的。</p><p>AVL树：平衡二叉树，一般是用平衡因子差值决定并通过旋转来实现，左右子树树高差不超过1，那么和红黑树比较它是严格的平衡二叉树，平衡条件非常严格（树高差只有1），只要插入或删除不满足上面的条件就要通过旋转来保持平衡。由于旋转是非常耗费时间的。</p><p>我们可以推出<strong>AVL</strong>树适合用于<strong>插入删除次数比较少，但查找多</strong>的情况。这种情况优于红黑树。</p><p>红黑树，确保没有一条路径会比其他路径长2倍，因而是近似平衡的。所以相对于严格要求平衡的AVL树来说，它的旋转保持平衡次数较少。用于<strong>搜索时，插入删除次数多</strong>的情况下我们就用<strong>红黑树</strong>来取代AVL。</p><hr><ul><li>B/B+树: 用在磁盘文件组织 数据索引和数据库索引。新的值可以插在已有的节点里，而不需要改变树的高度，从而大量减少重新平衡和数据迁移的次数，适合做数据库索引这种需要持久化在磁盘，同时需要大量查询和插入操作的应用。</li></ul><p>多路查找树，因为分支多层数少，适用于数据库系统。磁盘IO是非常耗时的，而像大量数据存储在磁盘中所以我们要有效的减少磁盘IO次数避免磁盘频繁的查找。能保持log(n)的插入与查询</p><p>B+树是B树的变种树，有n棵子树的节点中含有n个关键字，每个关键字不保存数据，只用来索引，数据都保存在叶子节点。是为文件系统而生的。</p><p>B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可。B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序扫，所以B+树更加适合在区间查询的情况。</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/53517463.jpg" alt=""><br><img src="http://opu8lkq3n.bkt.clouddn.com/18-4-20/87519861.jpg" alt=""></p><hr><ul><li>Trie树(字典树): 用在统计和排序大量字符串，如自动机。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;红黑、B、B+&quot;&gt;红黑、B、B+&lt;/h1&gt;&lt;h2 id=&quot;二叉搜索树&quot;&gt;二叉搜索树&lt;/h2&gt;&lt;p&gt;二叉查找树（Binary Search Tree），（又：二叉搜索树，二叉排序树）它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树。&lt;/p&gt;
    
    </summary>
    
      <category term="面试" scheme="http://peihao.space/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="数据结构" scheme="http://peihao.space/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>卡特兰数</title>
    <link href="http://peihao.space/2018/03/10/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/"/>
    <id>http://peihao.space/2018/03/10/卡特兰数/</id>
    <published>2018-03-10T13:56:52.000Z</published>
    <updated>2018-03-27T15:34:03.219Z</updated>
    
    <content type="html"><![CDATA[<p>卡特兰数又称卡塔兰数，是组合数学中一个常出现在各种计数问题中出现的数列。以比利时的数学家欧仁·查理·卡塔兰 (1814–1894)命名。</p><p>前20项为（OEIS中的数列A000108）：1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440, 9694845, 35357670, 129644790, 477638700, 1767263190</p><p>用几个在OJ中经常遇到的问题来说下具体应用。</p><ol><li><p>一个栈(无穷大)的进栈序列为1,2,3,..n,有多少个不同的出栈序列</p></li><li><p>2n人进剧院。入场费1元。n人有1元钞票，n人有2元钞票，剧院无钞票，有多少方式进场</p></li></ol><a id="more"></a><p>以上面第二个为例，该题求的是左端点为首元素的任意区间内，1的个数大于等于2的个数。</p><p>可以认为问题是，任意两种操作，持1元者买票是操作一，持2元买票者是操作二。要求每种操作的总次数一样，且进行第k次操作2前必须先进行至少k次操作1。我们假设一个人在原点，操作1是此人沿右上角45°走一个单位（一个单位设为根号2，这样他第一次进行操作1就刚好走到（1,1）点），操作2是此人沿右下角45°走一个单位。第k次操作2前必须先进行至少k次操作1，就是说明所走出来的折线不能跨越x轴走到y=-1这条线上！在进行n次操作1和n此操作2后，此人必将到到达（2n，0）！若无跨越x轴的限制，折线的种数将为$C\tbinom{n}{2n}$，即在2n次操作中选出n次作为操作1的方法数。</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-3-10/69269494.jpg" alt=""></p><p>只要减去跨越了x轴的情况数。对于任意跨越x轴的情况，必有将与y=-1相交。找出第一个与y=-1相交的点k，将k点以右的折线根据y=-1对称（即操作1与操作2互换了）。可以发现终点最终都会从（2n，0）对称到（2n，-2）。由于对称总是能进行的，且是可逆的。我们可以得出所有跨越了x轴的折线总数是与从（0,0）到（2n,-2）的折线总数。而后者的操作2比操作1要多0-（-2）=2次。即操作1为n-1,操作2为n+1。总数为$C\tbinom{n-1}{2n}$</p><p><img src="http://opu8lkq3n.bkt.clouddn.com/18-3-10/69269494.jpg" alt=""></p><p>这个证明的关键就是最终一定会到达（2n,0)这个点。</p><p>对于不满足情况的方案，它一定会越过y=-1，捉住这个特点，我们可将求不合法的方案数这个问题换个说法来：从（0，0）到（2n,-2)一共有多少种走法？这个走法数就是C(2n,n-1)因为走右下角的要多走2步，同时一共只走2n步，那就右下角走n+1步，为$C\tbinom{n-1}{2n}$</p><p>最终结果为$C\tbinom{n}{2n}-C\tbinom{n-1}{2n}$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卡特兰数又称卡塔兰数，是组合数学中一个常出现在各种计数问题中出现的数列。以比利时的数学家欧仁·查理·卡塔兰 (1814–1894)命名。&lt;/p&gt;
&lt;p&gt;前20项为（OEIS中的数列A000108）：1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440, 9694845, 35357670, 129644790, 477638700, 1767263190&lt;/p&gt;
&lt;p&gt;用几个在OJ中经常遇到的问题来说下具体应用。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;一个栈(无穷大)的进栈序列为1,2,3,..n,有多少个不同的出栈序列&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2n人进剧院。入场费1元。n人有1元钞票，n人有2元钞票，剧院无钞票，有多少方式进场&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="algorithm" scheme="http://peihao.space/categories/algorithm/"/>
    
    
      <category term="algorithm" scheme="http://peihao.space/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>冬</title>
    <link href="http://peihao.space/2017/12/09/winter/"/>
    <id>http://peihao.space/2017/12/09/winter/</id>
    <published>2017-12-09T10:56:13.000Z</published>
    <updated>2017-12-31T10:59:50.042Z</updated>
    
    <content type="html"><![CDATA[<center><br><br>绯冬荥韵 芳崋灼灼<br><br></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;&lt;br&gt;&lt;br&gt;绯冬荥韵 芳崋灼灼&lt;br&gt;&lt;br&gt;&lt;/center&gt;
      
    
    </summary>
    
      <category term="随笔" scheme="http://peihao.space/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://peihao.space/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>秋</title>
    <link href="http://peihao.space/2017/10/09/autumn/"/>
    <id>http://peihao.space/2017/10/09/autumn/</id>
    <published>2017-10-09T10:56:05.000Z</published>
    <updated>2017-12-31T11:07:56.703Z</updated>
    
    <content type="html"><![CDATA[<center><br><br>清秋氲旒，夜阑潇潇<br><br></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;&lt;br&gt;&lt;br&gt;清秋氲旒，夜阑潇潇&lt;br&gt;&lt;br&gt;&lt;/center&gt;
      
    
    </summary>
    
      <category term="随笔" scheme="http://peihao.space/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://peihao.space/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>夏</title>
    <link href="http://peihao.space/2017/06/09/summer/"/>
    <id>http://peihao.space/2017/06/09/summer/</id>
    <published>2017-06-09T10:55:56.000Z</published>
    <updated>2017-12-31T10:57:28.172Z</updated>
    
    <content type="html"><![CDATA[<center><br><br>盛夏流苏，光影斑斑<br><br></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;&lt;br&gt;&lt;br&gt;盛夏流苏，光影斑斑&lt;br&gt;&lt;br&gt;&lt;/center&gt;
      
    
    </summary>
    
      <category term="随笔" scheme="http://peihao.space/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://peihao.space/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>word2vec</title>
    <link href="http://peihao.space/2017/03/28/word2vec/"/>
    <id>http://peihao.space/2017/03/28/word2vec/</id>
    <published>2017-03-28T11:18:42.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>仅适用基于Hierarchical Softmax。</p><p>edit. 适用Negative Smapling</p><p><a href="http://download.csdn.net/detail/peihaozhu/9796105" target="_blank" rel="external">资源</a></p><p>word2vec通过训练将每个词映射成K维实数向量。通过词之间的距离，比如cosine相似度，欧式距离等来判断他们之间的语义相似度。根据词频使用哈弗慢编码，使得所有词频相似的词隐藏层的激活内容基本一致。出现频率越高的词语，他们激活的隐藏层数目越少，这样有效的降低了计算的复杂度。<br><a id="more"></a></p><blockquote><p>用one-hot来通俗简单的理解，有三个词“麦克风”、“话筒”、“杯子”，用one-hot编码，三个词对应[1, 0, 0]， [0, 1, 0]，[0, 0, 1]。当成千上万的词语，会造成每个词向量含有很多0，也就是稀疏编码，所以我们需要降维。比如说，上面的三个词语可以表示为[0, 1]， [0.4, 0.9]，[1, 0]。这里要注意的是“麦克风”和“话筒”的意思很接近，所以它们对应的向量也很接近，即空间距离短，向量夹角小。</p></blockquote><p><img src="http://peihao.space/img/article/ml/word1.png" alt=""></p><p><strong>分布式假设，其核心思想为出现于上下文情景中的词汇都有相类似的语义。</strong></p><p>word2vec中用到的两个重要模型-CBOW（Continuous Bag-of-Words Model）模型和Skip-gram（Contunuous Skip-gram）模型。</p><p>两个模型都包含三层：输入层、投影层和输出层。前者是在已知当前词$w_t$的上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$的前提下预测当前词$w_t$，即$P(w_t \mid w_{t-2},w_{t-1},w_{t+1},w_{t+2})$；后者恰恰相反，在已知当前词$w_t$的前提下，预测上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$,也就是$P(w_{t-2},w_{t-1},w_{t+1},w_{t+2} \mid w_t)$</p><p><img src="http://peihao.space/img/article/ml/word2.png" alt=""></p><h1 id="CBOW模型">CBOW模型</h1><p>CBOW模型包括三层：输入层投影层和输出层。</p><p><img src="http://peihao.space/img/article/ml/word3.png" alt=""><br>上面讲过CBOW模型网络结构是通过上下文context对当前词target做预测，求$P(w_t \mid w_{t-1},w_{t+1})$。</p><p>这里是假设Context(w)由w前后各c个词构成</p><ol><li><p>输入层：包含Context(w)中2c个词的词向量$v(Context(w)_1),v(Context(w)_2),…,v(Context(w)_{2c})$，$Context(w) \in R^m$，这里，m的含义同上表示词向量的长度。</p></li><li><p>投影层：将输出层的2c个向量做求和累加，即$x_w=\sum_{i=1}^{2c}v(Countext(w)_i) \in R^m$</p></li><li><p>输出层：输出层对应一个二叉树，以语聊中出现的词当叶子节点，以各次在语聊中出现的次数当权值构造出来哈夫曼树。在哈夫曼树中，叶子节点一共$\mid D \mid$个，分别对应词典D中词，非叶子结点N-1个</p></li></ol><p>取一个适当大小的窗口当做context window，输入层读入窗口内的词，将它们的向量（K维，初始随机）加和在一起，形成隐藏层K个节点。输出层是一个巨大的二叉树，叶节点代表语料里所有的词（语料含有V个独立的词，则二叉树有|V|个叶节点）。而这整颗二叉树构建的算法就是Huffman树。这样，对于叶节点的每一个词，就会有一个全局唯一的编码，形如”010011”，不妨记左子树为1，右子树为0。接下来，隐层的每一个节点都会跟二叉树的内节点有连边，于是对于二叉树的每一个内节点都会有K条连边，每条边上也会有权值。</p><p>对于语料库中的某个词$w_$t，对应着二叉树的某个叶子节点，因此它必然有一个二进制编码，如”010011”。在训练阶段，当给定上下文，要预测后面的词$w_t$的时候，我们就从二叉树的根节点开始遍历，这里的目标就是预测这个词的二进制编号的每一位。即对于给定的上下文，我们的目标是使得预测词的二进制编码概率最大。形象地说，我们希望在根节点，词向量和与根节点相连经过logistic计算得到bit=1的概率尽量接近0，在第二层，希望其bit=1的概率尽量接近1，这么一直下去，我们把一路上计算得到的概率相乘，即得到目标词$w_t$在当前网络下的概率$P(w_t\mid w_{t-1},w_{t+1})$。显而易见，按照目标词的二进制编码计算到最后的概率值就是归一化的.</p><p>模型的目标式子$p(w \mid Context(w))=\prod\limits_{j=2}^N p(d_j^w \mid x_w,\theta_{j-1}^w)$,用<strong>SGD</strong>训练优化模型，过程中更新相关的参数值（输入之后的加和向量值$x$，哈夫曼树中的内部节点向量$\theta$,以及原始的输入context中的所有向量参数$w$），每次取出一个样本$(context(w),w)$做训练，就要对整体的参数进行更新一次。对于$\theta$和$x$的值，通过SGD求偏导更新，而没有直接参与训练预测的原始context输入词向量$w$，我们通过$x$这个所有context词的加和向量来更新词向量$w$。把梯度贡献的更新项分配到所有输入中。</p><h1 id="Skip-gram">Skip-gram</h1><blockquote><p>Skip-Gram模型采取CBOW的逆过程的动机在于：CBOW算法对于很多分布式信息进行了平滑处理（例如将一整段上下文信息视为一个单一观察量）。很多情况下，对于小型的数据集，这一处理是有帮助的。相形之下，Skip-Gram模型将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。</p></blockquote><p>Skip-gram模型包括两层：输入层和输出层，相比CBOW，少了投影层，因为它不需要对所有的context词进行特征向量相加。</p><p><img src="http://peihao.space/img/article/ml/word4.png" alt=""></p><ol><li><p>输入层：目标词的词向量$v(w) \in R^m$</p></li><li><p>输出层：与CBOW类似，输出层十一颗哈夫曼树</p></li></ol><p>大体上来讲，Skip-Gramm与CBOW的流程相反，通过目标词对context进行预测，Skip-Gramm将其定义为$p(Context(w)\mid w)=\prod\limits_{w \in Context(w)} p(u \mid w)$，而上市中的$p(u \mid w)$可以按照上面的思想，通过哈夫曼树的路径节点乘积确定：$p(u \mid w)=\prod\limits_{j=2}^{l^u} p(d_j^u \mid v(w),\theta_{j-1}^u)$</p><p>之后就是对公式进行MLE的SGD优化。要注意的是，因为这里是要对context词进行预测，所以要从哈夫曼树的根部到叶子结点，进行$\mid context \mid$次。每次预测之后，都要处理完一个Context中的一个词之后，就要更新输入词的特征向量。</p><hr><p>下面开始介绍基于Negative Smapling的模型，翻译过来就是负采样模型，是由NCE（Noise Contrastive Estimation）的一个简化版本，目的是用来提高训练速度并改善词向量的质量。与上面介绍的基于Hierarchical Softmax的CBOW和Skip-gram相比，这种不使用复杂的哈夫曼树，而使用相对简单的随机负采样。因为在计算损失函数的时候，只是有我们挑选出来的k个noise word，而非整个的语料库V，这使得训练非常快。大幅度提高性能。</p><p>假设要求的未知的概率密度函数为X，已知的概率密度是Y，如果知道了X与Y的关系，那么X也就可以求出来。本质就是利用已知的概率密度估计未知的概率密度函数。</p><h1 id="负采样">负采样</h1><p>算法主要讲对于一个给定的词w，如何生成$NEG(w)$。</p><p>词典$D$中的词在语料$C$中出现的次数有高有低，高频词应该被选为负样本的概率应该高点，低的反之，本质上是一个带权采样，这就是大体的要求。</p><p>记$l_0=0,l_k=\sum\limits_{j=1}^k len(w_j),k=1,2,3…,N$，这里$w_j$表示词典D中的第j个词，则以${l_j}^N_{j=0}$为部分节点可得到区间$[0,1]$上的一个等距划分，$I_i=(l_{i-1},l_i],i=1,2,…,N$为N个剖分区间。</p><p>进一步的引入区间$[0,1]$上的等距离剖分，剖分节点为${m_j}_{j=0}^M$，其中$M &gt; &gt; N$</p><p><img src="http://peihao.space/img/article/ml/word5.png" alt=""></p><p>将内部剖分节点${m_j}^{M-1}_{j=1}$投影到非等距剖分上，则可建立${m_j}^{M-1}_{j=1}$与区间${I_j}^N_{j=1}$的映射关系。之后每次生成一个$[1,M-1]$间的堆积整数r，通过映射关系就能确定选择那个词作负样本。注意万一选中的就是w自己，则跳过。</p><h1 id="基于负采样的CBOW">基于负采样的CBOW</h1><p>在CBOW模型中，一直词w的上下文Context(w)，需要预测w，因此对于给定的Countext(w)，词w就是一个正样本，其他词就是负样本（分类问题）。</p><p>假定现在选好了关于Countext(w)的负采样集$NEG(w) \neq \emptyset$，并且<br>$$L^w(\hat{w})=<br>\begin{cases}<br>1,\hat{w}=w;\0,\hat{w}=w<br>\end{cases}<br>$$</p><p>表示词$\hat{w}$的标签，即正样本的标签为1，负样本的标签为0.</p><p>对于一个给定的正样本（Context（w）,w），<strong>我们希望最大化</strong>$$g(w)=\prod_{u \in {w} \bigcup NEG(w)} p(u \mid Context(w))$$</p><p>其中<br>$$p(u\mid Context(w))=<br>\begin{cases}<br>\sigma(x^T_w\theta^u),\ \ L^w(u)=1;\ \  1-\sigma(x^T_w\theta^u), \ \ L^w(u)=0<br>\end{cases}<br>$$</p><p>这里$x_w$表示Context(w)中各词的词向量之和，而$\theta^u \in R^m$表示词u对应的一个辅助向量，其实就是word-embeding中的嵌入值。</p><p>$\sigma(x_w^T \theta^w)$表示当上下文为Context(w)时，预测中心词为w的概率，而$\sigma(x^T_w\theta^u)\, u \in NEG(w)$则表示上下文为Context(w)时，预测中心词为u的概率。式子$g(w)$表示，所有在NEG集合加上实际的中心词w概率相乘，最大化这个式子，每一项，如果是实际的中心词w，最大化$p$，如果属于NEG集合，最大化$(1-p)$。增大正样本的概率同时降低负样本的概率。</p><p>之后的内容就是使用SGD对求解最大化这个公式进行训练，参数的更新，包括$\theta$对应词的嵌入，$x$对应Context(w)中各词的词向量之和，以及通过$x$更新最初的输入词$w$的向量，对于整个数据集，当梯度下降的过程中不断地更新参数，对应产生的效果就是不断地移动每个单词的嵌套向量，直到可以把真实单词和噪声单词很好得区分开。</p><h1 id="基于负采样的Skip-gram">基于负采样的Skip-gram</h1><p>对于一个给定的样本$(w,Context(w))$，我们希望最大化$$g(w)=\prod\limits_{\hat{w} \in Context(w)}\prod\limits_{u \in {w} \bigcup NEG^{\hat{w}}(w)} p(u\mid \hat{w})$$</p><p>其中$$p(u\mid \hat{w})=\begin{cases}\sigma(v(\hat{w})^T\theta^u, \ \ L^w(u)=1; \  \1-\sigma(v(\hat{w})^T\theta^u, \ \ L^w(u)=0 \end{cases}$$</p><p>这里$NEG^{\hat{w}}(w)$表示处理词$\hat{w}$时候生成的负样本子集，于是对于一个给定的语料库C，函数$$G=\prod\limits_{w\in C}g(w)$$<br>作为整体优化的目标，然后为了变成和项，我们取对数等等。</p><p>之后的步骤就跟原来一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow.python.platform</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Download the data.</span></span><br><span class="line">url = <span class="string">'http://mattmahoney.net/dc/'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download</span><span class="params">(filename, expected_bytes)</span>:</span></span><br><span class="line">  <span class="string">"""Download a file if not present, and make sure it's the right size."""</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filename):</span><br><span class="line">    filename, _ = urllib.request.urlretrieve(url + filename, filename)</span><br><span class="line">  <span class="comment"># 文件信息获取</span></span><br><span class="line">  statinfo = os.stat(filename)</span><br><span class="line">  <span class="keyword">if</span> statinfo.st_size == expected_bytes:</span><br><span class="line">    print(<span class="string">'Found and verified'</span>, filename)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(statinfo.st_size)</span><br><span class="line">    <span class="keyword">raise</span> Exception(</span><br><span class="line">        <span class="string">'Failed to verify '</span> + filename + <span class="string">'. Can you get to it with a browser?'</span>)</span><br><span class="line">  <span class="keyword">return</span> filename</span><br><span class="line"></span><br><span class="line">filename = maybe_download(<span class="string">'text8.zip'</span>, <span class="number">31344016</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data into a string.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">  f = zipfile.ZipFile(filename)</span><br><span class="line">  <span class="comment"># 获取压缩文件中的文件列表，返回第一个文件内容，根据空格进行分割成列表。</span></span><br><span class="line">  <span class="keyword">for</span> name <span class="keyword">in</span> f.namelist():</span><br><span class="line">  <span class="keyword">return</span> f.read(name).split()</span><br><span class="line">  f.close()</span><br><span class="line"></span><br><span class="line">words = read_data(filename)</span><br><span class="line">print(<span class="string">'Data size'</span>, len(words))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Build the dictionary and replace rare words with UNK token.#将稀少的词使用UNK替换</span></span><br><span class="line">vocabulary_size = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></span><br><span class="line">  count = [[<span class="string">'UNK'</span>, -<span class="number">1</span>]]<span class="comment"># 词'UNK'代表UnKnow</span></span><br><span class="line">  <span class="comment"># 将count扩展，使用collections模块的计数器，根据出现次数的多少进行排序然后填充进count，排序之后UNK为第一位。s.most_common(n)方法返回对象s的Top n数据，没有指定的话，返回全部</span></span><br><span class="line">  count.extend(collections.Counter(words).most_common(vocabulary_size - <span class="number">1</span>))</span><br><span class="line">  dictionary = dict()</span><br><span class="line">  <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:<span class="comment">#count中按item有两个内容：str以及对应的times频率。定义一个字典对象，键为str，对应的值为上面count中按str频率高低的排名例如 the:1,of:2。。。</span></span><br><span class="line">    dictionary[word] = len(dictionary)</span><br><span class="line">  data = list()</span><br><span class="line">  unk_count = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">      index = dictionary[word]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      index = <span class="number">0</span>  <span class="comment"># dictionary['UNK'] 注意之前在dictionary中根据排序，UNK还是在第一位，对应的值为0</span></span><br><span class="line">      unk_count = unk_count + <span class="number">1</span></span><br><span class="line">    <span class="comment"># index代表了词对应的排名，出现一次，填充进data中一次。data中包含的是原来文件中词对应的排名列表</span></span><br><span class="line">    data.append(index)</span><br><span class="line">  count[<span class="number">0</span>][<span class="number">1</span>] = unk_count <span class="comment"># count是一个二维的元祖，一个元素是'UNK':times，count[0][1]代表的就是UNK对应的频率</span></span><br><span class="line">  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))<span class="comment">#键值对reverse</span></span><br><span class="line">  <span class="keyword">return</span> data, count, dictionary, reverse_dictionary</span><br><span class="line"></span><br><span class="line">data, count, dictionary, reverse_dictionary = build_dataset(words)</span><br><span class="line"><span class="keyword">del</span> words  <span class="comment"># Hint to reduce memory.</span></span><br><span class="line">print(<span class="string">'Most common words (+UNK)'</span>, count[:<span class="number">5</span>])<span class="comment"># 输出频率最高的5个词</span></span><br><span class="line">print(<span class="string">'Sample data'</span>, data[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">data_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Function to generate a training batch for the skip-gram model.</span></span><br><span class="line"><span class="comment"># num_skips 训练样本的源端要使用几次，出于n-skip算法的原因，一个中心词要对应多个周边词，也就是说一个中心词target要预测几次周边词，对应的词的数量</span></span><br><span class="line"><span class="comment"># skip_window 左右各考虑多少个词，skip_windows*2=num_skips</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch_size, num_skips, skip_window)</span>:</span></span><br><span class="line">  <span class="keyword">global</span> data_index</span><br><span class="line">  <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">  <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br><span class="line">  <span class="comment"># ndarray本质是数组，其不同于一般的数组，或者Python 的list的地方在于它可以有N 维（dimentions），也可简单理解为数组里面嵌套数组。</span></span><br><span class="line">  batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">  labels = np.ndarray(shape=(batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">  span = <span class="number">2</span> * skip_window + <span class="number">1</span> <span class="comment"># [ skip_window target skip_window ]</span></span><br><span class="line">  buffer = collections.deque(maxlen=span)</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</span><br><span class="line">    <span class="comment"># 最初的填充，填充进原来文本中word的频率</span></span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    <span class="comment"># 因为data_index是全局变量，训练要很多步，后面取余</span></span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range((int)(batch_size / num_skips)):<span class="comment">#batch中样batch_size个样本，衣蛾target有num_skips个样本</span></span><br><span class="line">    target = skip_window  <span class="comment"># target label at the center of the buffer</span></span><br><span class="line">    targets_to_avoid = [ skip_window ]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_skips):</span><br><span class="line">      <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">        <span class="comment"># 进行了num_skips轮，每次找到一个不在target_to_avoid中的元素，实际上就是每次找一个与target配对的word</span></span><br><span class="line">        target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">      targets_to_avoid.append(target)</span><br><span class="line">      batch[i * num_skips + j] = buffer[skip_window] <span class="comment">#batch中是连续的num_skips个target词</span></span><br><span class="line">      labels[i * num_skips + j, <span class="number">0</span>] = buffer[target]<span class="comment">#label中连续的num_skip个周边词</span></span><br><span class="line">    buffer.append(data[data_index])<span class="comment">#buffer是有容量限制的，之前的状态是满的，此时会将最早的元素挤出去</span></span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">  <span class="keyword">return</span> batch, labels</span><br><span class="line"></span><br><span class="line">batch, labels = generate_batch(batch_size=<span class="number">8</span>, num_skips=<span class="number">2</span>, skip_window=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">  print(batch[i], <span class="string">'-&gt;'</span>, labels[i, <span class="number">0</span>])</span><br><span class="line">  print(reverse_dictionary[batch[i]], <span class="string">'-&gt;'</span>, reverse_dictionary[labels[i, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Build and train a skip-gram model.</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span>  <span class="comment"># 嵌入矩阵的密度，或者说是矩阵长度，batch_size要和embedding_size一致</span></span><br><span class="line">skip_window = <span class="number">1</span>       <span class="comment"># How many words to consider left and right.</span></span><br><span class="line">num_skips = <span class="number">2</span>         <span class="comment"># How many times to re-use an input to generate a label.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造NEG集合相关参数，集合中的元素就作为分类结果的干扰</span></span><br><span class="line">valid_size = <span class="number">16</span>     <span class="comment"># Random set of words to evaluate similarity on.#随机的word集合估计相似度</span></span><br><span class="line">valid_window = <span class="number">100</span>  <span class="comment"># 选择在头部分布的开发样本</span></span><br><span class="line">valid_examples = np.array(random.sample(xrange(valid_window), valid_size))<span class="comment"># 从[0,valid_window]中随机的获取valid_size个数值返回</span></span><br><span class="line">num_sampled = <span class="number">64</span>    <span class="comment"># 负采样的个数</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Input data.</span></span><br><span class="line">  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">  train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br><span class="line">  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Construct the variables.</span></span><br><span class="line">  embeddings = tf.Variable(<span class="comment">#使用唯一的随机值来初始化大矩阵，shape=[vocabulary_size, embedding_size]</span></span><br><span class="line">      tf.random_uniform([vocabulary_size, embedding_size], -<span class="number">1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">  nce_weights = tf.Variable(<span class="comment">#每个word定义一个权重值与偏差</span></span><br><span class="line">      <span class="comment"># tf.truncated_normal初始函数将根据所得到的均值和标准差，生成一个随机分布。shape=[vocabulary_size, embedding_size]</span></span><br><span class="line">      tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                          stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">  nce_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Look up embeddings for inputs. 根据train_inputs中的id，寻找embeddings中的对应元素。比如，train_inputs=[1,3,5]，则找出embeddings中下标为1,3,5的向量组成一个矩阵返回。</span></span><br><span class="line">  embed = tf.nn.embedding_lookup(embeddings, train_inputs)<span class="comment">#这里是从train_inputs给的索引值找到embeddings大矩阵中的对应的嵌入值</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compute the average NCE loss for the batch.</span></span><br><span class="line">  <span class="comment"># tf.nce_loss automatically draws a new sample of the negative labels each</span></span><br><span class="line">  <span class="comment"># time we evaluate the loss.</span></span><br><span class="line">  loss = tf.reduce_mean(<span class="comment"># reduce_mean是平均值  vocabulary_size代表可能的数目  num_sampled代表per batch随机抽样的个数</span></span><br><span class="line">      tf.nn.nce_loss(nce_weights, nce_biases, train_labels,embed,<span class="comment">#这里的参数都是按batch计算的，而非具体的某个样本.同时要注意的是原文件中参数排序出错，这里修正</span></span><br><span class="line">                     num_sampled, vocabulary_size))</span><br><span class="line"></span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 计算在minibatch和所有的embedding的cosine相似度</span></span><br><span class="line">  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="keyword">True</span>))<span class="comment">#tf.reduce_sum就是求和</span></span><br><span class="line">  normalized_embeddings = embeddings / norm<span class="comment"># 正则化嵌入值</span></span><br><span class="line">  valid_embeddings = tf.nn.embedding_lookup( <span class="comment">#NEG集嵌入值</span></span><br><span class="line">      normalized_embeddings, valid_dataset)<span class="comment">#寻找NEG集合中对应的正则化后的嵌入值</span></span><br><span class="line">  similarity = tf.matmul(<span class="comment">#NEG集合正则化后的嵌入值与词典集合正则化后的嵌入值的矩阵乘</span></span><br><span class="line">      valid_embeddings, normalized_embeddings, transpose_b=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Begin training</span></span><br><span class="line">num_steps = <span class="number">100001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  <span class="comment"># We must initialize all variables before we use them.</span></span><br><span class="line">  tf.initialize_all_variables().run()</span><br><span class="line">  print(<span class="string">"Initialized"</span>)</span><br><span class="line"></span><br><span class="line">  average_loss = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> xrange(num_steps):</span><br><span class="line">    batch_inputs, batch_labels = generate_batch(</span><br><span class="line">        batch_size, num_skips, skip_window)</span><br><span class="line">    feed_dict = &#123;train_inputs : batch_inputs, train_labels : batch_labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We perform one update step by evaluating the optimizer op (including it</span></span><br><span class="line">    <span class="comment"># in the list of returned values for session.run()</span></span><br><span class="line">    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">    average_loss += loss_val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">        average_loss = average_loss / <span class="number">2000</span></span><br><span class="line">      <span class="comment"># The average loss is an estimate of the loss over the last 2000 batches.</span></span><br><span class="line">      print(<span class="string">"Average loss at step "</span>, step, <span class="string">": "</span>, average_loss)</span><br><span class="line">      average_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里是构建nosie词作</span></span><br><span class="line">    <span class="comment"># 注意这里的代价是很大的，没500步差不多就会减慢20%的计算</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">      sim = similarity.eval()</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> xrange(valid_size):</span><br><span class="line">        valid_word = reverse_dictionary[valid_examples[i]]</span><br><span class="line">        top_k = <span class="number">8</span> <span class="comment"># number of nearest neighbors#</span></span><br><span class="line">        nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k+<span class="number">1</span>]<span class="comment"># 返回的是按相似度排序后元素值的索引值</span></span><br><span class="line">        log_str = <span class="string">"Nearest to %s:"</span> % valid_word</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> xrange(top_k):</span><br><span class="line">          close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">          log_str = <span class="string">"%s %s,"</span> % (log_str, close_word)</span><br><span class="line">        print(log_str)</span><br><span class="line">  final_embeddings = normalized_embeddings.eval()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 7: Visualize the embeddings.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels, filename=<span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">  <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">"More labels than embeddings"</span></span><br><span class="line">  plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))  <span class="comment">#in inches</span></span><br><span class="line">  <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">    x, y = low_dim_embs[i,:]</span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    plt.annotate(label,</span><br><span class="line">                 xy=(x, y),</span><br><span class="line">                 xytext=(<span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">                 textcoords=<span class="string">'offset points'</span>,</span><br><span class="line">                 ha=<span class="string">'right'</span>,</span><br><span class="line">                 va=<span class="string">'bottom'</span>)</span><br><span class="line"></span><br><span class="line">  plt.savefig(filename)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">    tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>)</span><br><span class="line">    plot_only = <span class="number">500</span></span><br><span class="line">    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])</span><br><span class="line">    labels = list(dictionary.keys())[:plot_only]</span><br><span class="line">    plot_with_labels(low_dim_embs, labels)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">except</span> ImportError:</span><br><span class="line">    print(<span class="string">"Please install sklearn and matplotlib to visualize embeddings."</span>)<span class="string">'''</span></span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="http://peihao.space/img/article/ml/tsne.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;仅适用基于Hierarchical Softmax。&lt;/p&gt;
&lt;p&gt;edit. 适用Negative Smapling&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://download.csdn.net/detail/peihaozhu/9796105&quot;&gt;资源&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;word2vec通过训练将每个词映射成K维实数向量。通过词之间的距离，比如cosine相似度，欧式距离等来判断他们之间的语义相似度。根据词频使用哈弗慢编码，使得所有词频相似的词隐藏层的激活内容基本一致。出现频率越高的词语，他们激活的隐藏层数目越少，这样有效的降低了计算的复杂度。&lt;br&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow简单使用Cifar数据集</title>
    <link href="http://peihao.space/2017/03/26/tf-cifar/"/>
    <id>http://peihao.space/2017/03/26/tf-cifar/</id>
    <published>2017-03-26T14:18:42.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是对TensorFlow官方例子：CIFAR-10数据集分类的理解记录。</p><p>对CIFAR-10 数据集的分类是机器学习中一个公开的基准测试问题，其任务是对一组大小为32x32的RGB图像进行分类，这些图像涵盖了10个类别：</p><p>飞机， 汽车， 鸟， 猫， 鹿， 狗， 青蛙， 马， 船以及卡车。</p><p><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">数据集主页</a></p><p><a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="external">Python项目代码页面</a></p><p>这里主要介绍<code>cifar10_input.py</code>、<code>caifar10.py</code>、<code>caifar_train.py</code>和<code>cifar10_eval.py</code><br><a id="more"></a></p><h1 id="模型输入">模型输入</h1><p><code>cifar10_input.py</code>文件，从二进制文件<code>cifar-10-binary.tar.gz</code>中提取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distorted_inputs</span><span class="params">(data_dir, batch_size)</span>:</span></span><br><span class="line"></span><br><span class="line">  filenames = [os.path.join(data_dir, <span class="string">'data_batch_%d.bin'</span> % i)</span><br><span class="line">               <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">  <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</span><br><span class="line"></span><br><span class="line">  filename_queue = tf.train.string_input_producer(filenames)</span><br><span class="line"></span><br><span class="line">  read_input = read_cifar10(filename_queue)</span><br><span class="line">  reshaped_image = tf.cast(read_input.uint8image, tf.float32)</span><br><span class="line"></span><br><span class="line">  height = IMAGE_SIZE</span><br><span class="line">  width = IMAGE_SIZE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  distorted_image = tf.random_crop(reshaped_image, [height, width, <span class="number">3</span>])</span><br><span class="line">  distorted_image = tf.image.random_flip_left_right(distorted_image)</span><br><span class="line">  distorted_image = tf.image.random_brightness(distorted_image,</span><br><span class="line">                                               max_delta=<span class="number">63</span>)</span><br><span class="line">  distorted_image = tf.image.random_contrast(distorted_image,</span><br><span class="line">                                             lower=<span class="number">0.2</span>, upper=<span class="number">1.8</span>)</span><br><span class="line">  float_image = tf.image.per_image_standardization(distorted_image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  float_image.set_shape([height, width, <span class="number">3</span>])</span><br><span class="line">  read_input.label.set_shape([<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  min_fraction_of_examples_in_queue = <span class="number">0.4</span></span><br><span class="line">  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *</span><br><span class="line">                           min_fraction_of_examples_in_queue)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> _generate_image_and_label_batch(float_image,read_input.label,min_queue_examples, batch_size,shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>主要函数有两个，<code>inputs</code>和<code>distorted_inputs</code>，这里贴出来的是<code>distorted_inputs</code>。两个方法都是从训练/测试 数据集文件中读取数据，后者针对测试集，裁剪图像、提取变换成相应的格式，前者针对训练集，在变化成需要格式前，还要进行图像的处理，如翻转，亮度变换、随机替换等等来增加数据集。返回的是构建生成的数据样本和标签。</p><h1 id="模型构建">模型构建</h1><p>使用CNN模型，包括两级卷基层、两级全连接层和最后的softmax激励函数输出。</p><h2 id="模型">模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(images)</span>:</span></span><br><span class="line">  <span class="comment"># 卷基层1</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment"># 过滤器</span></span><br><span class="line">    kernel = _variable_with_weight_decay(<span class="string">'weights'</span>,</span><br><span class="line">                                         shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">64</span>],</span><br><span class="line">                                         stddev=<span class="number">5e-2</span>,</span><br><span class="line">                                         wd=<span class="number">0.0</span>)</span><br><span class="line">    conv = tf.nn.conv2d(images, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># 偏置</span></span><br><span class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">64</span>], tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    pre_activation = tf.nn.bias_add(conv, biases)</span><br><span class="line">    <span class="comment"># Relu非线性处理</span></span><br><span class="line">    conv1 = tf.nn.relu(pre_activation, name=scope.name)</span><br><span class="line">    _activation_summary(conv1)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 池化降维</span></span><br><span class="line">  pool1 = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                         padding=<span class="string">'SAME'</span>, name=<span class="string">'pool1'</span>)</span><br><span class="line">  <span class="comment"># 归一化处理</span></span><br><span class="line">  norm1 = tf.nn.lrn(pool1, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>,name=<span class="string">'norm1'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 卷积层2</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv2'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    kernel = _variable_with_weight_decay(<span class="string">'weights'</span>,</span><br><span class="line">                                         shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>],</span><br><span class="line">                                         stddev=<span class="number">5e-2</span>,</span><br><span class="line">                                         wd=<span class="number">0.0</span>)</span><br><span class="line">    conv = tf.nn.conv2d(norm1, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">64</span>], tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">    pre_activation = tf.nn.bias_add(conv, biases)</span><br><span class="line">    conv2 = tf.nn.relu(pre_activation, name=scope.name)</span><br><span class="line">    _activation_summary(conv2)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 归一化</span></span><br><span class="line">  norm2 = tf.nn.lrn(conv2, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>, name=<span class="string">'norm2'</span>)</span><br><span class="line">  <span class="comment"># 池化</span></span><br><span class="line">  pool2 = tf.nn.max_pool(norm2, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=<span class="string">'pool2'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 全连接层</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'local3'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment"># 尺寸对应全连接层变换处理</span></span><br><span class="line">    reshape = tf.reshape(pool2, [FLAGS.batch_size, -<span class="number">1</span>])</span><br><span class="line">    dim = reshape.get_shape()[<span class="number">1</span>].value</span><br><span class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, shape=[dim, <span class="number">384</span>],</span><br><span class="line">                                          stddev=<span class="number">0.04</span>, wd=<span class="number">0.004</span>)</span><br><span class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">384</span>], tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">    <span class="comment"># 不再使用卷积乘tf.nn.conv2d，直接矩阵乘</span></span><br><span class="line">    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)</span><br><span class="line">    _activation_summary(local3)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 全连接层2</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'local4'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, shape=[<span class="number">384</span>, <span class="number">192</span>],stddev=<span class="number">0.04</span>, wd=<span class="number">0.004</span>)</span><br><span class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">192</span>], tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)</span><br><span class="line">    _activation_summary(local4)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># (WX + b),线性logit，这里没使用softmax直接输出未归一化的logits是因为需要在loss中直接计算熵，使用的sparse_softmax_cross_entropy_with_logits函数接受的是未归一化的形式</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax_linear'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, [<span class="number">192</span>, NUM_CLASSES],stddev=<span class="number">1</span>/<span class="number">192.0</span>,wd=<span class="number">0.0</span>)</span><br><span class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [NUM_CLASSES],tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)</span><br><span class="line">    _activation_summary(softmax_linear)</span><br><span class="line">  <span class="keyword">return</span> softmax_linear</span><br></pre></td></tr></table></figure><h1 id="训练阶段">训练阶段</h1><h2 id="loss">loss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(logits, labels)</span>:</span></span><br><span class="line"><span class="comment"># 接受从模型构造函数inference返回的logits，以及从input或者distorted_inputs中的label部分，返回损失tensor</span></span><br><span class="line">  labels = tf.cast(labels, tf.int64)</span><br><span class="line">  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">      labels=labels, logits=logits, name=<span class="string">'cross_entropy_per_example'</span>)</span><br><span class="line">  cross_entropy_mean = tf.reduce_mean(cross_entropy, name=<span class="string">'cross_entropy'</span>)</span><br><span class="line">  tf.add_to_collection(<span class="string">'losses'</span>, cross_entropy_mean)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The total loss is defined as the cross entropy loss plus all of the weight</span></span><br><span class="line">  <span class="comment"># decay terms (L2 loss).</span></span><br><span class="line">  <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">'losses'</span>), name=<span class="string">'total_loss'</span>)</span><br></pre></td></tr></table></figure><h2 id="train">train</h2><p>训练阶段就是通过训练算法迭代优化，最小化损失函数的过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接受参数：总损失def train(total_loss, global_step):</span></span><br><span class="line"></span><br><span class="line">  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size</span><br><span class="line">  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 根据当前的训练步数、衰减速度、初始的学习速率确定更新新的学习速率.staircase=True，标明按照梯度下降衰减。即global_step,每隔decay_steps, learing_rate会按照LEARNING_RATE_DECAY_FACTOR（衰减系数）衰减一次。 如果straircase = false, 代表 learning rate 按照连续函数衰减， 即每训练一次，learning rate 都会衰减一次</span></span><br><span class="line">  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,</span><br><span class="line">                                  global_step,</span><br><span class="line">                                  decay_steps,</span><br><span class="line">                                  LEARNING_RATE_DECAY_FACTOR,</span><br><span class="line">                                  staircase=<span class="keyword">True</span>)</span><br><span class="line">  tf.summary.scalar(<span class="string">'learning_rate'</span>, lr)</span><br><span class="line"></span><br><span class="line">  loss_averages_op = _add_loss_summaries(total_loss)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 计算梯度。函数tf.control_dependencies，计算单元梯度计算要在统计之后</span></span><br><span class="line">  <span class="keyword">with</span> tf.control_dependencies([loss_averages_op]):</span><br><span class="line">    opt = tf.train.GradientDescentOptimizer(lr)</span><br><span class="line">    grads = opt.compute_gradients(total_loss)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 梯度更新参数：计算完了，就反向传播一次，更新被训练的参数</span></span><br><span class="line">  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</span><br><span class="line">    tf.summary.histogram(var.op.name, var)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> grad, var <span class="keyword">in</span> grads:</span><br><span class="line">    <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">      tf.summary.histogram(var.op.name + <span class="string">'/gradients'</span>, grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 指数移动平均，是指tensorflow会创建一个变量（一般称为shadow variable）来储存某个变量的指数移动平均值，在训练过程中，每训练一次，变量都会学习到一个新的值，则对应的shadow变量也会跟着更新一次（更新需要run update op）。在训练过程中，只会不断更新shadow变量的值，而不会在模型中使用这个shadow变量。这个shadow变量一般是提供给评估过程使用的。 我理解的是，直接使用学习到的变量值进行评估，会导致评估有一定的波动性，如果使用变量的移动平均值替换变量进行评估，则会使评估过程更稳定，而且获得的评估效果也更好。</span></span><br><span class="line">  variable_averages = tf.train.ExponentialMovingAverage(</span><br><span class="line">      MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">  variables_averages_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.control_dependencies([apply_gradient_op, variables_averages_op]):</span><br><span class="line">    train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 返回训练op</span></span><br><span class="line">  <span class="keyword">return</span> train_op</span><br></pre></td></tr></table></figure><h1 id="测试阶段">测试阶段</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试一次的函数，saver是操作模型参数文件checkpoints的对象;top_k_op计算准确率def eval_once(saver, summary_writer, top_k_op, summary_op):  </span></span><br><span class="line">  <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">      <span class="comment"># 从checkpoint取出参数</span></span><br><span class="line">      saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">      global_step = ckpt.model_checkpoint_path.split(<span class="string">'/'</span>)[-<span class="number">1</span>].split(<span class="string">'-'</span>)[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="string">'No checkpoint file found'</span>)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始测试样本队列，多线程</span></span><br><span class="line">    coord = tf.train.Coordinator()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      threads = []</span><br><span class="line">      <span class="comment"># tensorflow单独创建一个queue runner线程，它负责从文件中读取样本数据，并将其装载到一个队列中。我们只需要开启这个线程，在需要数据时从队列中获取想要的size的数据集就可以了，队列数据的装载由该线程自动实现的。</span></span><br><span class="line">      <span class="keyword">for</span> qr <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):</span><br><span class="line">        threads.extend(qr.create_threads(sess, coord=coord, daemon=<span class="keyword">True</span>,</span><br><span class="line">                                         start=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))</span><br><span class="line">      true_count = <span class="number">0</span>  <span class="comment"># Counts the number of correct predictions.</span></span><br><span class="line">      total_sample_count = num_iter * FLAGS.batch_size</span><br><span class="line">      step = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span> step &lt; num_iter <span class="keyword">and</span> <span class="keyword">not</span> coord.should_stop():</span><br><span class="line">        <span class="comment"># 计算准确率</span></span><br><span class="line">        predictions = sess.run([top_k_op])</span><br><span class="line">        true_count += np.sum(predictions)</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># 计算准确率</span></span><br><span class="line">      precision = true_count / total_sample_count</span><br><span class="line">      print(<span class="string">'%s: precision @ 1 = %.3f'</span> % (datetime.now(), precision))</span><br><span class="line"></span><br><span class="line">      summary = tf.Summary()</span><br><span class="line">      summary.ParseFromString(sess.run(summary_op))</span><br><span class="line">      summary.value.add(tag=<span class="string">'Precision @ 1'</span>, simple_value=precision)</span><br><span class="line">      summary_writer.add_summary(summary, global_step)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:  <span class="comment"># pylint: disable=broad-except</span></span><br><span class="line">      coord.request_stop(e)</span><br><span class="line"></span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads, stop_grace_period_secs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h1 id="其他">其他</h1><p>关于tf.train.shuffle_batch 中的参数 shuffle、min_after_dequeue</p><p>shuffle的作用在于指定是否需要随机打乱样本的顺序，一般作用于训练阶段，提高鲁棒性。</p><ol><li>当shuffle = false时，每次dequeue是从队列中按顺序取数据，遵从先入先出的原则</li><li>当shuffle = true时，每次从队列中dequeue取数据时，不再按顺序，而是随机的，所以打乱了样本的原有顺序。</li></ol><p>shuffle还要配合参数min_after_dequeue使用才能发挥作用。这个参数min_after_dequeue的意思是队列中，做dequeue（取数据）的操作后，queue runner线程要保证队列中至少剩下min_after_dequeue个数据。如果min_after_dequeue设置的过少，则即使shuffle为true，也达不到好的混合效果。</p><p>因为我们的目的肯定是想尽最大可能的混合数据，因此设置min_after_dequeue，可以保证每次dequeue后都有足够量的数据填充尽队列，保证下次dequeue时可以很充分的混合数据。</p><p>但是min_after_dequeue也不能设置的太大，这样会导致队列填充的时间变长，尤其是在最初的装载阶段，会花费比较长的时间。</p><hr><h2 id="关于训练与测试">关于训练与测试</h2><p>在以前的教程中，都是将训练和评估放在一个程序中运行，而在这个教程中，训练和评估是分开在两个独立的程序中进行的，之所以这样做，是因为评估过程不会直接使用训练学习到的模型参数（trainable variable的值），而是要使用的是变量的滑动平均（shadow variable）来代替原有变量进行评估。</p><p>具体的实现方法是，在训练过程中，为每个trainable variable 添加 指数滑动平均变量，然后每训练1000步就将模型训练到的变量值保存在checkpoint中，评估过程运行时，从最新存储的checkpoint中取出模型的shadow variable，赋值给对应的变量，然后进行评估</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是对TensorFlow官方例子：CIFAR-10数据集分类的理解记录。&lt;/p&gt;
&lt;p&gt;对CIFAR-10 数据集的分类是机器学习中一个公开的基准测试问题，其任务是对一组大小为32x32的RGB图像进行分类，这些图像涵盖了10个类别：&lt;/p&gt;
&lt;p&gt;飞机， 汽车， 鸟， 猫， 鹿， 狗， 青蛙， 马， 船以及卡车。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;数据集主页&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10&quot;&gt;Python项目代码页面&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里主要介绍&lt;code&gt;cifar10_input.py&lt;/code&gt;、&lt;code&gt;caifar10.py&lt;/code&gt;、&lt;code&gt;caifar_train.py&lt;/code&gt;和&lt;code&gt;cifar10_eval.py&lt;/code&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Tensorflow" scheme="http://peihao.space/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://peihao.space/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Dropout</title>
    <link href="http://peihao.space/2017/03/23/dropout/"/>
    <id>http://peihao.space/2017/03/23/dropout/</id>
    <published>2017-03-23T14:18:42.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>训练神经网络模型时，如果<strong>训练样本较少，为了防止模型过拟合</strong>，Dropout可以作为一种trikc供选择。Dropout是hintion最近2年提出的，源于其文章<a href="https://arxiv.org/abs/1207.0580" target="_blank" rel="external">Improving neural networks by preventing co-adaptation of feature detectors</a>.中文大意为：通过阻止特征检测器的共同作用来提高神经网络的性能。本篇博文就是按照这篇论文简单介绍下Dropout的思想。</p><p>大部分内容来源<a href="http://www.cnblogs.com/tornadomeet/p/3258122.html" target="_blank" rel="external">tornadomeet</a>，先记录，之后填充。<br><a id="more"></a></p><h1 id="基础知识">基础知识</h1><p>　Dropout是指在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了（有点抽象，具体实现看后面的实验部分）。</p><p>　　按照hinton的文章，他使用Dropout时训练阶段和测试阶段做了如下操作：</p><p>　　在样本的训练阶段，在没有采用pre-training的网络时（Dropout当然可以结合pre-training一起使用），hintion并不是像通常那样对权值采用L2范数惩罚，而是对每个隐含节点的权值L2范数设置一个上限bound，当训练过程中如果该节点不满足bound约束，则用该bound值对权值进行一个规范化操作（即同时除以该L2范数值），说是这样可以让权值更新初始的时候有个大的学习率供衰减，并且可以搜索更多的权值空间。</p><p>　　在模型的测试阶段，使用”mean network(均值网络)”来得到隐含层的输出，其实就是在网络前向传播到输出层前时隐含层节点的输出值都要减半（如果dropout的比例为50%），其理由文章说了一些，可以去查看（没理解）。</p><p>　　关于Dropout，文章中没有给出任何数学解释，Hintion的直观解释和理由如下：</p><ol><li><p>由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。</p></li><li><p>可以将dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。这样不同的样本就对应不同的模型，是bagging的一种极端情况。个人感觉这个解释稍微靠谱些，和bagging，boosting理论有点像，但又不完全相同。</p></li><li><p>native bayes是dropout的一个特例。Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错。而Droput每次不是训练一个特征，而是一部分隐含层特征。</p></li><li><p>还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。</p></li></ol><p>下面一个使用MNIST的CNN小例子使用dropout：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行20000次训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line"><span class="comment"># 随机取出50个样本数据，包括图片的灰度值x [28*28]，以及label数据y_</span></span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># 这里没有再学习权值和偏置，没有对他们更新，是使用训练集做了一个简单的测试，所以概率设置为1</span></span><br><span class="line">        train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">            x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy))</span><br><span class="line"><span class="comment"># 训练时使用dropout，概率为0.5</span></span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试时不使用dropout，将概率设置为1</span></span><br><span class="line">print(<span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;&lt;p&gt;训练神经网络模型时，如果&lt;strong&gt;训练样本较少，为了防止模型过拟合&lt;/strong&gt;，Dropout可以作为一种trikc供选择。Dropout是hintion最近2年提出的，源于其文章&lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt;.中文大意为：通过阻止特征检测器的共同作用来提高神经网络的性能。本篇博文就是按照这篇论文简单介绍下Dropout的思想。&lt;/p&gt;
&lt;p&gt;大部分内容来源&lt;a href=&quot;http://www.cnblogs.com/tornadomeet/p/3258122.html&quot;&gt;tornadomeet&lt;/a&gt;，先记录，之后填充。&lt;br&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>（转载）CNN卷积神经网络</title>
    <link href="http://peihao.space/2017/03/22/cnn/"/>
    <id>http://peihao.space/2017/03/22/cnn/</id>
    <published>2017-03-22T14:18:42.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="先述">先述</h1><p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">卷积神经网络-翻译</a><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">    英文原文</a>（ConvNets 或者 CNNs）属于神经网络的范畴，已经在诸如图像识别和分类的领域证明了其高效的能力。卷积神经网络可以成功识别人脸、物体和交通信号，从而为机器人和自动驾驶汽车提供视力。</p><p><img src="http://peihao.space/img/article/ml/x3.png" alt=""><br><a id="more"></a><br>上图中，ConvNet主要有四个操作：</p><ol><li>卷积</li><li>非线性处理（ReLU）</li><li>池化pooling</li><li>分类</li></ol><h2 id="图像是像素值的矩阵">图像是像素值的矩阵</h2><p>本质上来说，每张图像都可以表示为像素值的矩阵：</p><p><img src="http://peihao.space/img/article/ml/x4.gif" alt=""></p><p>通道 （chain）常用于表示图像的某种组成。一个标准数字相机拍摄的图像会有三通道 - 红、绿和蓝；你可以把它们看作是互相堆叠在一起的二维矩阵（每一个通道代表一个颜色），每个通道的像素值在 0 到 255 的范围内。</p><p>灰度图像，仅仅只有一个通道。在本篇文章中，我们仅考虑灰度图像，这样我们就只有一个二维的矩阵来表示图像。矩阵中各个像素的值在 0 到 255 的范围内——零表示黑色，255 表示白色。</p><h2 id="卷积">卷积</h2><p><strong>卷积的主要目的是为了从输入图像中提取特征。卷积可以通过从输入的一小块数据中学到图像的特征，并可以保留像素间的空间关系。</strong></p><p>每张图像都可以看作是像素值的矩阵。考虑一下一个 5 x 5 的图像，它的像素值仅为 0 或者 1（注意对于灰度图像而言，像素值的范围是 0 到 255，下面像素值为 0 和 1 的绿色矩阵仅为特例），同时，考虑下另一个 3 x 3 的矩阵。接下来，5 x 5 的图像和 3 x 3 的矩阵的卷积可以按下图所示的动画一样计算：</p><p><img src="http://peihao.space/img/article/ml/x2.gif" alt=""></p><p>现在停下来好好理解下上面的计算是怎么完成的。我们用橙色的矩阵在原始图像（绿色）上滑动，每次滑动一个像素（也叫做“步长”），在每个位置上，我们计算对应元素的乘积（两个矩阵间），并把乘积的和作为最后的结果，得到输出矩阵（粉色）中的每一个元素的值。注意，3 x 3 的矩阵每次步长中仅可以“看到”输入图像的一部分。</p><p>在 CNN 的术语中，3x3 的矩阵叫做“滤波器（filter）”或者“核（kernel）”或者“特征检测器（feature detector）”，通过在图像上滑动滤波器并计算点乘得到矩阵叫做“卷积特征（Convolved Feature）”或者“激活图（Activation Map）”或者“特征图（Feature Map）”。记住滤波器在原始输入图像上的作用是特征检测器。</p><p>不同滤波器对上图卷积的效果。通过在卷积操作前修改滤波矩阵的数值，我们可以进行诸如边缘检测、锐化和模糊等操作 —— 这表明不同的滤波器可以从图中检测到不同的特征，比如边缘、曲线等。</p><p>在实践中，CNN 会在训练过程中学习到这些滤波器的值（尽管我们依然需要在训练前指定诸如滤波器的个数、滤波器的大小、网络架构等参数）。我们使用的滤波器越多，提取到的图像特征就越多，网络所能在未知图像上识别的模式也就越好。</p><p>特征图的大小（卷积特征）由下面三个参数控制，我们需要在卷积前确定它们：</p><ul><li><p>深度（Depth）：深度对应的是卷积操作所需的滤波器个数。在下图的网络中，我们使用三个不同的滤波器对原始图像进行卷积操作，这样就可以生成三个不同的特征图。你可以把这三个特征图看作是堆叠的 2d 矩阵，那么，特征图的“深度”就是三。</p></li><li><p>步长（Stride）：步长是我们在输入矩阵上滑动滤波矩阵的像素数。当步长为 1 时，我们每次移动滤波器一个像素的位置。当步长为 2 时，我们每次移动滤波器会跳过 2 个像素。步长越大，将会得到更小的特征图。</p></li><li><p>零填充（Zero-padding）：有时，在输入矩阵的边缘使用零值进行填充，这样我们就可以对输入图像矩阵的边缘进行滤波。零填充的一大好处是可以让我们控制特征图的大小。使用零填充的也叫做泛卷积，不适用零填充的叫做严格卷积。这个概念在下面的参考文献 14 中介绍的非常详细。</p></li></ul><h2 id="ReLU">ReLU</h2><p>每次的卷积操作后都使用了一个叫做 ReLU 的操作。ReLU 表示修正线性单元（Rectified Linear Unit），是一个非线性操作。它的输入如下所示：</p><p><img src="http://peihao.space/img/article/ml/x5.png" alt=""></p><p>ReLU 是一个元素级别的操作（应用到各个像素），并将特征图中的所有小于 0 的像素值设置为零。ReLU 的目的是在 ConvNet 中引入非线性，因为在大部分的我们希望 ConvNet 学习的实际数据是非线性的（卷积是一个线性操作——元素级别的矩阵相乘和相加，所以我们需要通过使用非线性函数 ReLU 来引入非线性。</p><h2 id="池化">池化</h2><p>空间池化（Spatial Pooling）（也叫做亚采用或者下采样）降低了各个特征图的维度，但可以保持大部分重要的信息。空间池化有下面几种方式：最大化、平均化、加和等等。</p><p>对于最大池化（Max Pooling），我们定义一个空间邻域（比如，2x2 的窗口），并从窗口内的修正特征图中取出最大的元素。除了取最大元素，我们也可以取平均（Average Pooling）或者对窗口内的元素求和。在实际中，最大池化被证明效果更好一些。</p><p>我们以 2 个元素（也叫做“步长”）滑动我们 2x2 的窗口，并在每个区域内取最大值。如上图所示，这样操作可以降低我们特征图的维度。</p><p><img src="http://peihao.space/img/article/ml/x6.png" alt=""></p><p>池化函数可以逐渐降低输入表示的空间尺度。特别地，池化：</p><ul><li>使输入表示（特征维度）变得更小，并且网络中的参数和计算的数量更加可控的减小，因此，可以控制过拟合</li><li>使网络对于输入图像中更小的变化、冗余和变换变得不变性（输入的微小冗余将不会改变池化的输出——因为我们在局部邻域中使用了最大化/平均值的操作。</li><li>帮助我们获取图像最大程度上的尺度不变性（准确的词是“不变性”）。它非常的强大，因为我们可以检测图像中的物体，无论它们位置在哪里（参考 18 和 19 获取详细信息）。</li></ul><p>到目前为止我们了解了卷积、ReLU 和池化是如何操作的。理解这些层是构建任意 CNN 的基础是很重要的。我们有两组卷积、ReLU &amp; 池化层 —— 第二组卷积层使用六个滤波器对第一组的池化层的输出继续卷积，得到一共六个特征图。接下来对所有六个特征图应用 ReLU。接着我们对六个修正特征图分别进行最大池化操作。</p><p>这些层一起就可以从图像中提取有用的特征，并在网络中引入非线性，减少特征维度，同时保持这些特征具有某种程度上的尺度变化不变性。</p><p>第二组池化层的输出作为全连接层的输入，我们会在下一部分介绍全连接层。</p><h2 id="全连接层">全连接层</h2><p>全连接层是传统的多层感知器，在输出层使用的是 softmax 激活函数（也可以使用其他像 SVM 的分类器，但在本文中只使用 softmax）。“全连接（Fully Connected）”这个词表明前面层的所有神经元都与下一层的所有神经元连接。</p><p>卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类。</p><p>除了分类，添加一个全连接层也（一般）是学习这些特征的非线性组合的简单方法。从卷积和池化层得到的大多数特征可能对分类任务有效，但这些特征的组合可能会更好。</p><p>从全连接层得到的输出概率和为 1。这个可以在输出层使用 softmax 作为激活函数进行保证。softmax 函数输入一个任意大于 0 值的矢量，并把它们转换为零一之间的数值矢量，其和为一。</p><p><strong>卷积 + 池化层的作用是从输入图像中提取特征，而全连接层的作用是分类器。</strong></p><h1 id="小结">小结</h1><p>完整的卷积网络的训练过程可以总结如下：</p><ol><li><p>第一步：我们初始化所有的滤波器，使用随机值设置参数/权重</p></li><li><p>第二步：网络接收一张训练图像作为输入，通过前向传播过程（卷积、ReLU 和池化操作，以及全连接层的前向传播），找到各个类的输出概率</p><ul><li>我们假设船这张图像的输出概率是 [0.2, 0.4, 0.1, 0.3]</li><li>因为对于第一张训练样本的权重是随机分配的，输出的概率也是随机的</li></ul></li><li>第三步：在输出层计算总误差（计算 4 类的和）<ul><li>Total Error = ∑  ½ (target probability – output probability) ²</li></ul></li><li>第四步：使用反向传播算法，根据网络的权重计算误差的梯度，并使用梯度下降算法更新所有滤波器的值/权重以及参数的值，使输出误差最小化<ul><li>权重的更新与它们对总误差的占比有关</li><li>当同样的图像再次作为输入，这时的输出概率可能会是 [0.1, 0.1, 0.7, 0.1]，这就与目标矢量 [0, 0, 1, 0] 更接近了</li><li>这表明网络已经通过调节权重/滤波器，可以正确对这张特定图像的分类，这样输出的误差就减小了</li><li>像滤波器数量、滤波器大小、网络结构等这样的参数，在第一步前都是固定的，在训练过程中保持不变——仅仅是滤波器矩阵的值和连接权重在更新</li></ul></li><li>第五步：对训练数据中所有的图像重复步骤 1 ~ 4</li></ol><h1 id="补充">补充</h1><p>有时候，我们会在池化前后进行局部对比度归一化层（local contract normalization）。这个归一化包括两个部分：局部做减和局部做除</p><p>自然图像存在低阶和高阶的统计特征，低阶（例如二阶）的统计特征是满足高斯分布的，但高阶的统计特性是非高斯分布。图像中，空间上相邻的像素点有着很强的相关性。而对于PCA来说，因为它是对协方差矩阵操作，所以可以去掉输入图像的二阶相关性，但是却无法去掉高阶相关性。而有人证明了除以一个隐含的变量就可以去除高阶相关性。</p><p> 对输入图像的每一个像素，我们计算其邻域（例如3x3窗口）的均值，然后每个像素先减去这个均值，再除以这个邻域窗口（例如3x3窗口）拉成的9维向量的欧几里德范数（如果这个范数大于1的时候才除：这个约束是为了保证归一化只作用于减少响应（除以大于1的数值变小），而不会加强响应（除以小于1的数值变大））。也有论文在计算均值和范数的时候，都加入了距离的影响，也就是距离离该窗口中心越远，影响越小，例如加个高斯权重窗口（空间上相邻的像素点的相关性随着距离变大而变小）。</p><p> 数据归一化，就是将数据映射到[0,1]或[-1,1]区间或更小的区间，比如(0.1,0.9) 。</p><p> 为什么要归一化处理？</p><ol><li>输入数据的单位不一样，有些数据的范围可能特别大，导致的结果是神经网络收敛慢、训练时间长。</li><li>数据范围大的输入在模式分类中的作用可能会偏大，而数据范围小的输入作用就可能会偏小。</li><li>由于神经网络输出层的激活函数的值域是有限制的，因此需要将网络训练的目标数据映射到激活函数的值域。例如神经网络的输出层若采用S形激活函数，由于S形函数的值域限制在(0,1)，也就是说神经网络的输出只能限制在(0,1)，所以训练数据的输出就要归一化到[0,1]区间。</li><li>S形激活函数在(0,1)区间以外区域很平缓，区分度太小。例如S形函数f(X)在参数a=1时，f(100)与f(5)只相差0.0067。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;先述&quot;&gt;先述&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/&quot;&gt;卷积神经网络-翻译&lt;/a&gt;&lt;a href=&quot;https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/&quot;&gt;    英文原文&lt;/a&gt;（ConvNets 或者 CNNs）属于神经网络的范畴，已经在诸如图像识别和分类的领域证明了其高效的能力。卷积神经网络可以成功识别人脸、物体和交通信号，从而为机器人和自动驾驶汽车提供视力。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://peihao.space/img/article/ml/x3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>MAP、SRM、ERM与MLE</title>
    <link href="http://peihao.space/2017/03/20/MAP-MLE-SRM/"/>
    <id>http://peihao.space/2017/03/20/MAP-MLE-SRM/</id>
    <published>2017-03-20T03:22:09.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="最大似然与经验风险最小化">最大似然与经验风险最小化</h1><blockquote><p>当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计</p></blockquote><p>首先给出对数形式的ERM的公式：<br>$$\min \frac{1}{n}\sum\limits_{i=1}^n L(y_i,p(y_i\mid x_i))$$</p><p>其中$L(y_i,f(x_i))$是损失函数，输出预测值为$f(x_i)$，n是观察到的样本数。</p><hr><p>最大似然的前提是从模型总体随机抽取样本观测值，所有的采样都是独立同分布的。<br><a id="more"></a><br>假设$x_1,x_2,…,x_n$为独立同分布的采样，$\theta$为模型参数，f为我们使用的模型，我们使用条件概率分布，遵循独立同分布。假设我们需要根据观察数据$x$估计没有观察到的总体参数$\theta$：</p><p>$$f(x_1,x_2,…,x_n \mid \theta)=f(x_1 \mid \theta)\times f(x_2 \mid \theta)\times…\times f(x_n \mid \theta)$$</p><p>此时似然定义为：</p><p>$$L(\theta \mid x_1,x_2,…,x_n)=P(x_1,x_2,…,x_n\mid \theta)=\prod\limits_{i=1}^n f(x_i \mid \theta)$$</p><p>在实际应用中常用的是取两边取对数，并取似然值得平均值：</p><p>$$\frac{1}{n} \log L(\theta \mid x_1,x_2,…,x_n)=\frac{1}{n} \sum\limits_{i=1}^n \log f(x_i \mid \theta)$$</p><p>去取极大似然估计MLE：</p><p>$$\arg\max\limits_{\theta} \frac{1}{n} \sum\limits_{i=1}^n \log f(x_i\mid \theta)=\min \frac{1}{n}\sum\limits_{i=1}^n - \log f(x_i \mid \theta)$$</p><p>$-\log f(x_i\mid \theta)$可以看做是对数似然损失函数。可以明显看出此时的经验风险最小化就等价于极大似然估计。上式是要求参数$\theta$，在这个参数条件下，使得已知数据$x$出现的概率最大。</p><h1 id="后验概率与结构风险最小化">后验概率与结构风险最小化</h1><blockquote><p>当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。</p></blockquote><p>最大后验估计是根据经验数据获得对难以观察的量的点估计。与最大似然估计类似，但是最大的不同时，最大后验估计的融入了要估计量的先验分布在其中。故最大后验估计可以看做规则化的最大似然估计。</p><h2 id="MAP推导">MAP推导</h2><p>先来一段后验概率最大化MAP的推导，摘自<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87" target="_blank" rel="external">Wiki</a>：</p><p>假设我们需要根据观察数据$x$估计没有观察到的总体参数 $\theta$，让$f$作为$x$的采样分布，这样$f(x\mid \theta)$就是在那个题参数为$\theta$时$x$的概率。函数$\theta \to f(x \mid \theta)$，即为似然函数，其估计$\hat{\theta}_{ML}(x)=\arg\max\limits_{\theta}f(x\mid \theta)$，就是$\theta$的最大似然估计。</p><p>假设$\theta$存在一个先验分布$g$，这就允许我们将$\theta$作为贝叶斯分布中的随机变量，这样$\theta$的后验分布就是:</p><p>$$\theta \to \frac{f(x \mid \theta)g(\theta)}{\int_{\Theta}f(x \mid \theta_1)g(\theta_1)d\theta_1}$$</p><p>其中$\Theta$是$g$的域，上式分母的下部就相当于对已知数据$x$概率的估计，这里用的公式是贝叶斯公式，由先验概率去求后验概率$P(A\mid B)=(P(B\mid A)*P(A))/P(B)$。</p><p>最大后验估计方法估计$\theta$为这个随机变量的后验分布的众数：</p><p>$$\hat{\theta}_{MAP}(x)=\arg\max\limits_{\theta} \frac{f(x \mid \theta)g(\theta)}{\int_{\Theta}f(x \mid \theta_1)g(\theta_1)d\theta_1}=\arg\max\limits_{\theta}f(x\mid \theta)g(\theta)$$</p><p>后验分布的分母与$\theta$ 无关，在求解中分母不变，当成一个常数使用，所以在优化过程中不起作用。注意当前验$g$是常数函数时最大后验概率与最大似然估计的重合。</p><h2 id="先验概率">先验概率</h2><p>这里我先对我理解的先验概率含义做个叙述。<a href="http://blog.csdn.net/upon\_the\_yun/article/details/8915283" target="_blank" rel="external">先验分布</a>，我理解的就是在没有输入数据或者其他数据，根据经验主观或者频数客观的对整个模型的各个结果集占比的推测。</p><p>举例来说：假设有五个袋子，各袋中都有无限量的饼干(樱桃口味或柠檬口味)，已知五个袋子中两种口味的比例分别是</p><ol><li>樱桃 100%</li><li>樱桃 75% + 柠檬 25%</li><li>樱桃 50% + 柠檬 50%</li><li>樱桃 25% + 柠檬 75%</li><li>柠檬 100%</li></ol><p>如果只有如上所述条件，那问从同一个袋子中连续拿到2个柠檬饼干，那么这个袋子最有可能是上述五个的哪一个？</p><p>我们首先采用MLE来解这个问题。假设从袋子中能拿出柠檬饼干的概率为p(我们通过这个概率p来确定是从哪个袋子中拿出来的)，则似然函数可以写作：</p><p>$$p(两个柠檬饼干 \mid 袋子)=p^2$$</p><p>由于p的取值是一个离散值，即上面描述中的0,25%，50%，75%，1。我们只需要评估一下这五个值哪个值使得似然函数最大即可，得到为袋子5。这里便是最大似然估计的结果。</p><p>上述最大似然估计有一个问题，就是没有考虑到模型本身的概率分布，下面我们扩展这个饼干的问题。</p><p>假设拿到袋子1或5的机率都是0.1，拿到2或4的机率都是0.2，拿到3的机率是0.4，那同样上述问题的答案呢？这个时候就变MAP了。我们根据公式<br>$$\hat{\theta}_{MAP}(x)=\arg\max\limits_{\theta} \frac{f(x \mid \theta)g(\theta)}{\int_{\Theta}f(x \mid \theta_1)g(\theta_1)d\theta_1}=\arg\max\limits_{\theta}f(x\mid \theta)g(\theta)$$<br>写出我们的MAP函数：$MAP=p^2 \times g$</p><p>根据题意的描述可知，p的取值分别为0,25%，50%，75%，1，g的取值分别为0.1，0.2,0.4,0.2,0.1.分别计算出MAP函数的结果为：0,0.0125,0.125,0.28125,0.1.由上可知，通过MAP估计可得结果是从第四个袋子中取得的最高。</p><h2 id="SRM与MAP">SRM与MAP</h2><p>我们对MAP进行一些变换(先加上对数，再将对数展开)，则上式等价于：</p><p>$$\hat{\theta}_{MAP}(x)=\arg\max\limits_{\theta} [\ln f(x\mid \theta)+\ln g(\theta)]$$</p><p>进一步的，有：</p><p>$$\hat{\theta}_{MAP}(x)=\arg\max\limits_{\theta} \ln f(x \mid \theta)+\arg\max\limits_{\theta} \ln g(\theta)$$<br>可以发现，等式右边第一部分刚好为最大似然估计的公式，我们将最大似然估计的公式写出：</p><p>$$\max \frac{1}{n}\sum\limits_{i=1}^n \ln f(x_i \mid \theta)$$<br>将最大似然估计的公式代入，然后通过增加负号将最大后验概率分布公式的max改为min。这样，最大后验概率估计的公式可以写成下面这样：</p><p>$$\hat{\theta}_{MAP}(x)=\arg\min\limits_{\theta}{[\frac{1}{n}\sum\limits_{i=1}^n-\ln f(x_i \mid \theta)]- g(\theta)}$$</p><p>对比结构风险最小化公式：</p><p>$$\min\limits_{f \in F}\frac{1}{n}\sum\limits_{i=1}^n L(y_i,f(x_i))+\lambda J(f)$$</p><p>由于$f(\mid)$是模型，可以是条件概率分布模型，那么$-\ln f(x_i\mid \theta)$便可以看做是对数似然损失函数。</p><p>$g(\theta)$表示模型的先验概率，<strong>模型的复杂度与模型的先验概率没有必然的正比反比关系</strong>。这里我为了推导，暂且假定先验概率与模型复杂度成反比，$-g(\theta)$可以认为与复杂度成正比，$-g(\theta)$越大，复杂度越高。</p><p>此时，上式中的后半项就对应着结构风险最小化中的正则项。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;最大似然与经验风险最小化&quot;&gt;最大似然与经验风险最小化&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先给出对数形式的ERM的公式：&lt;br&gt;$$\min \frac{1}{n}\sum\limits_{i=1}^n L(y_i,p(y_i\mid x_i))$$&lt;/p&gt;
&lt;p&gt;其中$L(y_i,f(x_i))$是损失函数，输出预测值为$f(x_i)$，n是观察到的样本数。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;最大似然的前提是从模型总体随机抽取样本观测值，所有的采样都是独立同分布的。&lt;br&gt;
    
    </summary>
    
      <category term="数学" scheme="http://peihao.space/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="数学" scheme="http://peihao.space/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>梯度坐标下降</title>
    <link href="http://peihao.space/2017/03/17/lagrange-coordinate/"/>
    <id>http://peihao.space/2017/03/17/lagrange-coordinate/</id>
    <published>2017-03-17T03:22:09.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度下降">梯度下降</h1><p>梯度下降法是一种常用的一阶优化方法，是求解无约束优化问题方法之一。</p><p>原理：考虑无约束优化问题$$\min_x f(x)$$，其中$f(x)$为连续可微函数，若能构造一个序列$x^0,x^1,x^2,…$满足$$f(x^{t+1})  &lt; f(x^t),\ t=0,1,2…$$<br><a id="more"></a><br>不断迭代执行该过程即可收敛到局部极小点。根据泰勒展开式：$$f(x+\Delta x) \approx f(x)+\Delta x^T \nabla f(x)$$</p><p>欲满足：$$f(x+\Delta x) \ &lt; f(x)$$也就是近似满足<br>：$$f(x)+\Delta x^T \nabla f(x)\ &lt;\ f(x)\ \rightleftharpoons \Delta x f(x) \ &lt; 0$$</p><p>为了保证$\Delta x f(x) \ &lt; \ 0$成立，这里一般设置$\Delta x\ = \ -\gamma \nabla f(x)$这就是梯度下降法。当连续的两次迭代结果差小于阈值或者到达一定的迭代次数后，得到极小点</p><p>通过选取合适的步长，保证通过梯度下降收敛到局部极小点。当目标函数为凸函数时，局部极小点就对应着函数的全局最小点。</p><h1 id="坐标下降法">坐标下降法</h1><p>坐标下降法是一种非梯度优化方法，在每次迭代中沿一个坐标方向进行搜索，通过循环使用不同的坐标方向来达到目标函数的局部极小值。</p><p>他的原理就是在各个维度上搜索当前维度上函数的最小值，知道维度循环完毕。</p><p>不妨假设目标是求解函数$f(x)$的极小值，其中$x=(x_1,x_2,…,x_d)^T \ \in R^d$是一个d维向量。从初始点$x_0$开始，坐标下降法通过迭代构造序列求解问题，$x^{t+1}$的第i个分量$x_i^{t+1}$构造为$$x_i^{t+1}= \arg\min\limits_{y \in R} f(x_1^(t+1),…,x_{i-1}^{t+1},y,x_{i+1}^t,…,x_d^t)$$</p><p>通过迭代执行过程显然有$$f(x^0)\geq f(x^1) \geq f(x^2)$$</p><p>这里要注意函数必须要可微，否则不成立。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;梯度下降&quot;&gt;梯度下降&lt;/h1&gt;&lt;p&gt;梯度下降法是一种常用的一阶优化方法，是求解无约束优化问题方法之一。&lt;/p&gt;
&lt;p&gt;原理：考虑无约束优化问题$$\min_x f(x)$$，其中$f(x)$为连续可微函数，若能构造一个序列$x^0,x^1,x^2,…$满足$$f(x^{t+1})  &amp;lt; f(x^t),\ t=0,1,2…$$&lt;br&gt;
    
    </summary>
    
      <category term="数学" scheme="http://peihao.space/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="数学" scheme="http://peihao.space/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>对偶问题</title>
    <link href="http://peihao.space/2017/03/16/lagrange-dual/"/>
    <id>http://peihao.space/2017/03/16/lagrange-dual/</id>
    <published>2017-03-16T09:22:09.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对偶问题">对偶问题</h1><p>再来看不等式约束优化问题：</p><p>$$<br>\begin{aligned}<br>&amp;\min\limits_{x} \ f(x)\\<br>&amp;s.t.\ \ h_i(x)=0,\ i=1,2,…,m\\<br>&amp;g_j(x) \leq 0,\ j=1,2,…,n\\<br>\end{aligned}<br>$$</p><hr><p>定义Lagrange如下：<br>$$<br>L(x,\lambda,\mu)=f(x)+\sum\limits_{i=1}^m\lambda_ih_i(x)+\sum\limits_{j=1}^n\mu_jg_j(x)<br>$$</p><p>在优化理论中，目标函数 f(x) 会有多种形式：<a id="more"></a>如果目标函数和约束条件都为变量 x 的线性函数, 称该问题为线性规划； 如果目标函数为二次函数, 约束条件为线性函数, 称该最优化问题为二次规划; 如果目标函数或者约束条件均为非线性函数, 称该最优化问题为非线性规划。每个线性规划问题都有一个与之对应的对偶问题，对偶问题有非常良好的性质，以下列举几个：</p><ul><li>对偶问题的对偶是原问题；</li><li>无论原始问题是否是凸的，对偶问题都是凸优化问题；</li><li>对偶问题可以给出原始问题一个下界；</li><li>当满足一定条件时，原始问题与对偶问题的解是完全等价的；</li></ul><p>上式的拉格朗日对偶函数：</p><p>$$<br>\begin{aligned}<br>\Gamma(\lambda,\mu)=<br>&amp;\inf\limits_{x \in D}\ L(x,\lambda,\mu)\\<br>=&amp;\inf\limits_{x \in D}\lbrace f(x)+\sum\limits_{i=1}^m \lambda_i h_i(x)+\sum\limits_{j=1}^n \mu_jg_j(x) \rbrace<br>\end{aligned}<br>$$</p><p>若$\hat{x} \in D$是约束问题可行域的点，则对任意的$\mu \geq 0$和$\lambda$都有$\sum\limits_{i=1}^m \lambda_i h_i(x) + \sum\limits_{j=1}^n\mu_j g_j(x) \leq 0$</p><p>进而有</p><p>$$\Gamma(\lambda,\mu)=\inf\limits_{x \in D}L(x,\lambda,\mu) \leq L(\hat{x},\lambda,\mu) \leq f(\hat{x})$$</p><p>若对上面约束优化问题的最优值为$p^*$，则对任意$\mu \geq 0$和$\lambda$都有</p><p>$$\Gamma(\lambda,\mu)\leq p^*$$</p><p>对偶函数的最大值对应着原来约束条件优化问题的最大值，也就是说对偶问题能给主问题最优值的下界。</p><p>那么对偶函数能获得的最好的下界是什么呢？</p><p>$$\max\limits_{\lambda,\mu}\ \Gamma(\lambda,\mu)\ s.t. \ \mu \geq 0$$</p><p>这就是原来约束问题对应的对偶问题，$\mu,\lambda$叫对偶变量。为了方便，我们把它称为对偶式，原来约束问题称为原式。</p><h2 id="强弱对偶">强弱对偶</h2><p>假设对偶式的最优值为$d^<em>$，显然有$d^</em> \leq p^<em>$，这称为弱对偶性，如果$d^</em> = p^*$，称为强对偶性。</p><p>如果主问题为凸优化问题（例如原式中$f(x),g_j(x)$都是凸函数，$h_i(x)$是仿射函数），且可行域中至少有一点使不等式约束严格成立，此时强对偶成立。</p><p>强对偶下，将拉格朗日函数分别对原变量和对偶变量求导，再令导数为零，即可得到原变量与对偶变量的数值关系。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;对偶问题&quot;&gt;对偶问题&lt;/h1&gt;&lt;p&gt;再来看不等式约束优化问题：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;&amp;amp;\min\limits_{x} \ f(x)\\&lt;br&gt;&amp;amp;s.t.\ \ h_i(x)=0,\ i=1,2,…,m\\&lt;br&gt;&amp;amp;g_j(x) \leq 0,\ j=1,2,…,n\\&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;定义Lagrange如下：&lt;br&gt;$$&lt;br&gt;L(x,\lambda,\mu)=f(x)+\sum\limits_{i=1}^m\lambda_ih_i(x)+\sum\limits_{j=1}^n\mu_jg_j(x)&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;在优化理论中，目标函数 f(x) 会有多种形式：
    
    </summary>
    
      <category term="数学" scheme="http://peihao.space/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="数学" scheme="http://peihao.space/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>乘子与KKT</title>
    <link href="http://peihao.space/2017/03/16/lagrange-kkt/"/>
    <id>http://peihao.space/2017/03/16/lagrange-kkt/</id>
    <published>2017-03-16T06:22:09.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="拉格朗日乘子法">拉格朗日乘子法</h1><p>拉格朗日乘子法（Lagrange multipliers）是一种寻找多元函数在一组约束下的极值的方法。通过引入拉格朗日乘子，可将有$d$个变量与$k$个约束条件的最优化问题转化为具有$d+k$个变量的无约束优化问题求解</p><p>基本的拉格朗日乘子法就是求函数$f(x_1,x_2,…)$在$g(x_1,x_2,…)=0$的约束条件下的极值的方法。主要思想是将约束条件与原函数联系在一起，使能配成与变量数量相等的等式方程，从而求得原函数极值的各个变量的解。</p><hr><p>例子：假设需要求极值的目标函数为$f(x)$，约束条件为$\phi(x,y)=M$<br><a id="more"></a><br>设$g(x,y)=M-\phi(x,y)$，定义一个新函数$F(x,y,\lambda)=f(x,y)+\lambda g(x,y)$</p><p>求偏导：</p><p>$$<br>\begin{cases}<br>&amp;\frac{\partial F}{\partial x}=0 \\<br>&amp;\frac{\partial F}{\partial y}=0\\<br>&amp;\frac{\partial F}{\partial \lambda}=0<br>\end{cases}<br>$$</p><p>求出$x,y,\lambda$的值，代入即可得到目标函数的极值。</p><hr><p>机器学习中的拉格朗日乘子法，一般用于求解约束优化问题的方法，当目标函数是凸函数时，求解最小值，使用拉格朗日乘子法求得的局部最优解就是全局最优解。类似的，在凹函数中，求得的最大值，局部最大解就是全局最大解。</p><p>在没有约束条件下，直接使用求导取指即可，但是有了约束条件后，就不能这样任意的小了，需要首先满足约束条件之后再求解。</p><p>在二维空间中求解，假设约束条件是一个曲线：<img src="http://peihao.space/img/article/ml/ml-intro10-3.png" alt=""></p><p>环线是目标函数的取值的等高线，需要紧贴约束线来满足约束条件求得理想值。</p><p>图中可以很清晰的看出来，与约束条件相切的等高线正合适。至于其他的与约束条件曲线相切的都不能考虑，因为这种取值一部分是符合约束条件的，一部分不能满足约束条件。</p><p>曲线相切，实际上就是法线向量平行，同方向或者反方向。最优解处，f和g的斜率平行。也就是说，存在一个非零实数与其中一个斜率相乘，等于另外一个曲线的斜率。这个实数称之为$\lambda$，或者$-\lambda$随便啦</p><p>$\nabla[f(x,y)+\lambda(g(x,y)-c)]=0$</p><p>一旦求出λ的值，将其套入下式，易求在无约束极值和极值所对应的点。</p><p> $F \left( x , y \right)  =  f \left( x , y \right) + \lambda \left( g \left( x , y \right) - c \right)$</p><p>新方程$F(x,y)$在达到极值时与$f(x,y)$相等，因为$F(x,y)$达到极值时$g(x,y)-c$总等于零。</p><p>定义拉格朗日函数：$L(x,\lambda)=f(x)+\lambda g(x)$ 将其对$x$的偏导数$\nabla_xL(x,\lambda)$置零即得式子$\nabla f(x)+\lambda g(x)=0$；同时对$\lambda$的偏导数$\nabla_{\lambda}L(x,\lambda)$置零即得约束条件$g(x)=0$。所以原约束问题转换成了对拉格朗日函数$L(x,\lambda)$的无约束优化问题。</p><h1 id="KKT">KKT</h1><p><img src="http://peihao.space/img/article/ml/ml-intro10-4.png" alt=""></p><blockquote><p><a href="http://www.cnblogs.com/sddai/p/5730116.html" target="_blank" rel="external">KKT</a></p></blockquote><p>现在考虑不等式$g(x) \leq 0$，如上图，此时最优点$x$或者在$g(x)&lt;0$也就是环形区域内；或者在$g(x)=0$环形线上。</p><p>对于$g(x)&lt;0$的情况，约束$g(x) \leq 0$不起作用，可以直接通过条件$\nabla f(x)=0$来获得最优点，这里等价于将$\lambda=0$之后求解$\nabla _x L(x,\lambda)=0$</p><p>$g(x)=0$的情况类似与上图左侧，但是有一些区别。在拉格朗日乘子中，约束条件$g(x)$与$f(x)$保持梯度平行即可，可就是说参数$\lambda$无关正负；到了这里，我们好好分析一下，假设两者的梯度是同方向的，都是向外（就是环线区域外，相反方向当然也可以）。我们都知道，函数是按沿着梯度方向增大，所以$f(x)$在区域外的值是大于区域内的值，也就是说，区域内的值是小值。我们的目标就是在约束条件下求$\min f(x)$，这里区域内是满足约束条件的，所以最优值显然不在环线上取，而是在区域内取。如果我们非要在环线上取怎么办？两个函数的梯度方向相反。这样才符合我们的认知嘛，梯度相反，同一个方向一个变小一个变大，环线是临界值，很符合人们的罗辑思维。</p><p>接着说不等式约束条件，整合上面的两种情况：</p><ul><li>$g(x)&lt;0$，约束条件不起作用，使$\lambda=0$</li><li>$g(x)=0$，约束条件使得$\lambda &gt; 0$</li></ul><p>所以必有$\lambda g(x)=0$</p><p>KKT条件推出来了：</p><p>$$<br>\begin{cases}<br>&amp;g(x) \leq 0;\\<br>&amp;\lambda \geq 0;\\<br>&amp;\mu_jg_j(x)=0;\\<br>\end{cases}.<br>$$</p><h1 id="推广">推广</h1><p>推广到多个约束，考虑有m个等式约束和n个不等式约束，优化问题<br>$$<br>\begin{cases}<br>&amp;\min\limits_x f(x)\\<br>&amp;s.t. h_i(x)=0 \ \ (i=1,…,m),\\<br>&amp;g_j(x) \leq 0 \  \ (j=1,…,n).\\<br>\end{cases}<br>$$</p><p>引入拉格朗日乘子$\lambda=(\lambda_1,\lambda_2,…,\lambda_m)^T$和$\mu=(\mu_1,\mu_2,…,\mu_n)^T$,相应的拉格朗日函数为<br>$$<br>L(x,\lambda,\mu)=f(x)+\sum\limits_{i=1}^m\lambda_ih_i(x)+\sum\limits_{j=1}^n\mu_jg_j(x)<br>$$</p><p>引入的拉格朗日乘子条件与KKT条件为：</p><p>$$<br>\begin{cases}<br>&amp;h_i(x)=0;\\<br>&amp;\lambda_i \neq 0;\\<br>&amp;g_j(x) \leq 0;\\<br>&amp;\mu_j \geq 0;\\<br>&amp;\mu_jg_j(x)=0.\\<br>&amp;\end{cases}<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;拉格朗日乘子法&quot;&gt;拉格朗日乘子法&lt;/h1&gt;&lt;p&gt;拉格朗日乘子法（Lagrange multipliers）是一种寻找多元函数在一组约束下的极值的方法。通过引入拉格朗日乘子，可将有$d$个变量与$k$个约束条件的最优化问题转化为具有$d+k$个变量的无约束优化问题求解&lt;/p&gt;
&lt;p&gt;基本的拉格朗日乘子法就是求函数$f(x_1,x_2,…)$在$g(x_1,x_2,…)=0$的约束条件下的极值的方法。主要思想是将约束条件与原函数联系在一起，使能配成与变量数量相等的等式方程，从而求得原函数极值的各个变量的解。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;例子：假设需要求极值的目标函数为$f(x)$，约束条件为$\phi(x,y)=M$&lt;br&gt;
    
    </summary>
    
      <category term="数学" scheme="http://peihao.space/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="数学" scheme="http://peihao.space/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习</title>
    <link href="http://peihao.space/2017/03/13/ml-semi-supervised/"/>
    <id>http://peihao.space/2017/03/13/ml-semi-supervised/</id>
    <published>2017-03-13T11:17:14.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="主动学习与半监督">主动学习与半监督</h1><p>假设我们有训练样本集$D_l={ (x_1,y_1),(x_2,y_2),…,(x_l,y_l) }$，l个样本的类别标记已知，称为有标记（labeled）；此外还有$D_u={ x_1,x_2,…,x_u },l \ll u$，这u个样本样本的类别标记未知，称为未标记的unlabeled样本。</p><p>若直接使用之前一直介绍的监督学习，则仅有$D_l$能用于构建模型，剩余的未标记样本都浪费了；另一方面远小于u数量的标记样本往往由于训练样本不足，学得模型的泛化能力往往不佳。<br><a id="more"></a></p><h2 id="主动学习">主动学习</h2><p>一个做法是用$D_l$先训练一个模型，使用这个模型在$D_u$中拿一个模型出来，寻求专家知识，判定结果，然后把这个新标记的样本加入到$D_l$中重新训练一个模型，然后再去$D_u$中获取一个新的未标记样本。。。；如果每次都能挑出来对改善模型性能帮助大的瓜，则只需要询问专家较少的次数就能构建出较强的模型，大幅度的降低标记成本。称为主动学习（active learning），其目标是使用尽量少的查询获取尽量好的性能。</p><h2 id="半监督学习">半监督学习</h2><p>半监督（semi supervised）让学习器不依赖外界交互（针对主动学习的专家经验）、自动的利用未标记样本来提升学习性能。主要是考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类。</p><p>半监督学习进一步可分为纯半监督学习和直推学习。前者假定训练数据中的未标记样本并非待预测的数据，而后者假定学习过程中所考虑的未标记样本是待预测数据，学习目的是在这些未标记样本上获得最优泛化能力。也就是说，纯半监督学习是基于开放世界，希望学得的模型能适用于训练过程中未观测带的数据；直推学习是基于封闭世界假设，仅试图对学习过程中观察到的未标记数据进行预测。<br><img src="http://peihao.space/img/article/ml/ml-intro10-2.png" alt=""></p><h1 id="生成式方法">生成式方法</h1><p>生成模型是半监督学习的一种模型，生成式方法是直接基于生成式模型的方法。此类方法假设所有数据（labeled、unlabeled）都是由同一个潜在的模型生成的，这个假设使得我们能够通过潜在模型的参数将未标记数据与学习目标联系起来，而未标记数据的标记则可看作模型的缺失参数，基于EM算法进行极大似然估计求解。</p><p>给定样本x，其真实类别标记为$y \in Y$，其中$Y = { 1,2,…,N }$为所有可能的类别。假设样本由高斯混合模型生成，且每个类别对应一个高斯混合成分：$p(x)=\sum\limits_{i=1}^N \alpha_i \cdot p(x \mid \mu_i,\sum_i)$</p><p>其中，混合系数$\alpha \geq 0,\sum_{i=1}^N \alpha_i=1;p(x \mid \mu_i,\sum_i)$是样本x属于第i个高斯混合成分的概率；$\mu_i$和$\sum_i$为该高斯混合成分的参数。</p><p>令$f(x) \in Y$表示模型f对x的预测标记，$\Theta \in { 1,2,…,N }$表示样本x隶属的高斯混合成分。由最大化后验概率可知：</p><p>$$<br>f(x)=arg \max\limits_{j \in Y} p(y= j \mid x)<br>=arg \max\limits_{j \in Y} \sum\limits_{i=1}^Np(y=j,\Theta=i \mid x)<br>=arg \max\limits_{j \in Y} \sum\limits_{i=1}^Np(y=j,\Theta=i,x)\cdot p(\Theta =i \mid x)<br>$$</p><p>其中$p(\Theta=i \mid x)=\frac{\alpha_i \cdot p(x \mid \mu_i,\sum_i)}{\sum\limits_{i=1}^N\alpha_i \cdot p(x \mid \mu_i,\sum_i)}$（不涉及样本标记）是样本x由第i个高斯混合成分生成的后验概率，$p(y=j \mid \Theta =i ,x)$是x由第i个高斯混合成分且类别为j的概率。</p><p>给定标记样本集$D_l={ (x_1,y_1),(x_2,y_2),…,(x_l,y_l) }$和未标记样本集合$D_u={ x_{l+1},x_{l+2},…,x_{l+u} },l \ll u l+u=m$假定所有样本独立同分布，且都是由同一个高斯混合模型生成。用极大似然估计法来估计高斯混合模型的参数，使用EM算法。</p><h1 id="半监督SVM">半监督SVM</h1><p>半监督SVM（简称S3VM）是支持向量机在半监督学习上的推广，在不考虑未标记样本时，支持向量机试图找到最大间隔划分超平面，而在考虑未标记样本后，S3VM试图找<strong>能将两类有标记样本分开，且穿过数据低密度区域</strong>的划分超平面。</p><p>简单介绍一下其中最出名的TSVM，针对二分类问题，TSVM考虑对未标记样本进行各种可能的标记指派，然后再所有这些结果中寻求一个在所有样本（包括labeled and unlabeled）上间隔最大化的划分超平面。一旦划分超平面确定，未标记样本最终标记就是预测结果。</p><p>显然上面的思路是利用穷举方法，这样明显效率不高，TSVM在上面再进一步。给定$D_l={(x_1,y_1),(x_2,y_2),…,(x_l,y_l)}$和$D_u={x_{l+1},x_{l+2},…,x_{l+u}},y_i \in {-1,+1},l \ll u,l+u=m$。TSVM的学习目标就是为$D_u$中的样本给出预测标记$\hat{y}=(\hat{y}_{l+1},\hat{y}_{l+2},…,\hat{y}_{l+u}),\hat{y_i} \in {-1,+1}$，使得：$\min\limits_{\omega,b,\hat{y},\xi}\frac{1}{2}\mid\mid \omega \mid\mid_2^2+C_l\sum\limits_{i=1}^l \xi_i +C_u \sum\limits_{i=l+1}^m \xi_i$</p><p>上式中，$(\omega,b)$确定了一个划分超平面；$\xi$为松弛向量，$\xi_i(i=1,2,…,l)$对应于有标记样本，$\xi_i(i=l+1,l+2,…,m)$对应与未标记样本；$C_l,C_u$是由用户指定的用于平衡模型复杂度、有标记样本与未标记样本重要程度的这种参数。</p><p>它使用有标记样本学得一个SVM，然后使用这个SVM对未标记的数据进行标记指派，将SVM预测的结果作为伪标记赋予未标记样本。接下来TSVM找出两个标记指派为异类且很可能发生错误的未标记样本，交换标记，重新求得更新后SVM的划分超平面和松弛向量（在这一步中，因为SVM求得的伪标记往往是不准确的，所以需要设置好$C_l,C_u$，将$C_l$值大一点，标明有标记样本的作用更大)；然后再找两个标记指派为异类且很可能发生错误的未标记样本，交换。。标记指派完成后逐渐提高未标记样本对优化目标的影响，进行下一轮标记指派调整。。直到$C_u=C_l$</p><h1 id="图半监督">图半监督</h1><p>思想：相似或者相关联的顶点尽可能的赋予相同标记连接，以保证图的标记尽可能的平滑。相似性或者关系度越高，连接的权值越大。</p><p>定义相似矩阵$W=(w_{ij})_{(l+u)\times(l+u)},w_{ij}=exp(-\frac{\mid\mid x_i-x_j \mid\mid^2}{2\sigma^2})\ if\  e=(x_i,e_j) \in E\ else\ 0$其中$\sigma$是带宽系数，用于控制权值的减缓程度。$w_{ij}$随着欧式距离的增加会减少。</p><p>标记传递算法：已标记数据$Rightarrow$近邻未标记数据$Rightarrow$次级近邻未标记数据</p><h1 id="协同训练">协同训练</h1><p>协同训练（co-training）使用多学习器，学习器之间的分歧对未标记数据的利用很重要。</p><p>一个数据对象往往同时拥有多个属性集，每个属性集构成一个视图。假设不同的试图有相容性，即其包含的关于输出空间的信息是一只的，当两个一起考虑就会有大概率使得与真实标记接近。不同视图信息的互补性会给学习器的构建带来很多便利。</p><p>协同训练正是使用了多视图的相容互补性，假设数据有两个充分且条件独立视图。充分是指每个视图都包含足以产生最优学习器的信息，条件独立则是指在给定类别的标记下两个视图独立。协同训练使用下面的策略使用未标记数据：首先在每个视图上基于有标记样本分别训练出一个分类器，然后让每个分类器分别去挑选自己最有把握的未标记样本赋予伪标记，并将伪标记样本提供给另一个分类器作为新增的有标记样本用于训练更新。。。之后就是不断的过程迭代，直到分类器不再更新。</p><p>协同学习也可以在单视图上使用，例如使用不同的学习方法、不同的数据采样、甚至不同的参数设置。</p><h1 id="半监督聚类">半监督聚类</h1><p>聚类是一种典型的无监督学习任务，不过我们通常能够获取一些额外的信息：必连与勿连信息，即两个样本一定属于一个label、一定不属于一个label；第二种就是获得少量的有标记样本。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;主动学习与半监督&quot;&gt;主动学习与半监督&lt;/h1&gt;&lt;p&gt;假设我们有训练样本集$D_l={ (x_1,y_1),(x_2,y_2),…,(x_l,y_l) }$，l个样本的类别标记已知，称为有标记（labeled）；此外还有$D_u={ x_1,x_2,…,x_u },l \ll u$，这u个样本样本的类别标记未知，称为未标记的unlabeled样本。&lt;/p&gt;
&lt;p&gt;若直接使用之前一直介绍的监督学习，则仅有$D_l$能用于构建模型，剩余的未标记样本都浪费了；另一方面远小于u数量的标记样本往往由于训练样本不足，学得模型的泛化能力往往不佳。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>特征选择/稀疏学习</title>
    <link href="http://peihao.space/2017/03/12/ml-feature-chosen/"/>
    <id>http://peihao.space/2017/03/12/ml-feature-chosen/</id>
    <published>2017-03-12T08:10:23.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="范数">范数</h1><ul><li>$L_0$范数表示向量中非零元素的个数：$\mid\mid x \mid\mid_0 = count(x_i) while x_i \neq 0$</li></ul><p>使用这个范数希望参数的大部分元素是0（稀疏数据），通过最优化范数，可以寻找最优稀疏特征，不过这个范数的最优问题是NP难度。</p><ul><li>$L_1$范数表示向量中每个元素绝对值的和：$\mid\mid x \mid\mid_1=\sum_{i=1}^n \mid x_i \mid$</li></ul><a id="more"></a><p>$L_1$范数是$L_0$范数的最优凸优化，常用$L_1$代替$L_0$范数。</p><ul><li><p>$L_2$范数即欧式距离：$\mid\mid x \mid\mid_2=\sqrt{\sum_{i=1}^n x_i^2}$</p></li><li><p>Forbenius范数：$\mid\mid A \mid\mid_F = \sqrt{\sum\limits_{i=1}^m\sum\limits_{j=1}^n \mid a_{ij} \mid^2}$</p></li></ul><h1 id="子图搜索与特征选择">子图搜索与特征选择</h1><p>给定特征集合${a_1,a_2,…,a_d }$，我们可将每个特征看作一个候选子集，对这d个候选单特征子集进行评价，假定$a_2$最优，于是将$a_2$作为第一轮的选定集；然后，在上一轮的选定集中加入一个特征，构成包含两个特征的候选子集，假定在这d-1个候选两特征子集中$(a_2,a_4)$最优，且优于$(a_2)$，于是将$(a_2,a_4)$作为本轮的选定集；</p><p>……假定在第it+1轮时，最优的候选(k+1)特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的k特征集合作为特征选择结果.这样逐渐增加相关特征的策略称为“前向”（forward)搜索。</p><p>类似的，若我们从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略称为“后向”（backward)搜索.还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征(这些特征在后续轮中将确定不会被去除同时减少无关特征，这样的策略称为“双向”（bidirectional)搜索.</p><p>常见的特征选择方法大致分为三类，过滤式、包裹式和嵌入式。</p><h1 id="过滤式">过滤式</h1><p>过滤式先对数据集进行特征选择，然后再训练学习器，特侦选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。</p><h2 id="Relif">Relif</h2><p>过滤式特征选择方法，方法设计了一个相关统计量度量特征的重要性。统计量是一个向量，每个向量分别对应与一个初始特征，而特征子集的重要性则是由子集中每个特征对应的相关统计量分量之和决定。</p><p>过滤的方法就是设定一个阈值$\tau$，选择比$\tau$大的相关统计量分量对应的特征即可。</p><p>给定训练集${(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$，对每个示例$x_i$，算法先在$x_i$的同类样本中寻找其最近邻$x_{i,nh}$，称为猜中近邻，再从$x_i$的异类样本中寻找其最近临$x_{i,nm}$，称为猜错近邻，然后相关统计量对应于属性j的分量为：$\delta^j=\sum\limits_i -diff(x_i^j,x_{i,nh}^j)^2+diff(x_i^j,x_{i,nm}^j)^2$</p><p>其中$x_a^j$表示样本$x_a$在属性j上的取指，若属性j为离散型，则$diff(x_a^j,x_b^j)=0 if x_a^j==x_b^j else 1$，若是连续型，$diff(x_a^j,x_b^j)=\mid x_a^j-x_b^j \mid$</p><p>若$x_i$与其猜中近邻在属性j上的距离小于$x_i$与其猜错近邻的距离，则说明属性j对区分同类与异类样本是有益的，则增大属性j对应的统计分量。</p><h1 id="包裹式选择">包裹式选择</h1><p>包裹式特征选择直接把最终将要使用的学习器性能作为特征子集的评价准则。</p><p>从最终学习性能来看，包裹式选择比过滤式更好，然而需要的开销要大得多。</p><h2 id="LVM">LVM</h2><p>使用随机策略进行子图搜集，并以最终分类器的误差作为特征子集的评价准则。随机搜索，每次都要进行学习，交叉验证的结果作为误差标准比较，所以开销比较大。</p><h1 id="嵌入式选择">嵌入式选择</h1><p>嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动的进行了特征选择。</p><p>给定数据集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$，我们考虑最简单的线性回归模型，以平方误差为损失函数：</p><p>$\min\limits_{\omega} \sum\limits_{i=1}^m (y_i-\omega^Tx_i)^2$</p><p>当样本特征很多，样本数目较少时，容易陷入过拟合，此时需要加入正则化项：</p><p>$\min\limits_{\omega} \sum\limits_{i=1}^m (y_i-\omega^Tx_i)^2+\lambda\mid\mid \omega \mid\mid_2^2$</p><p>$\min\limits_{\omega} \sum\limits_{i=1}^m (y_i-\omega^Tx_i)^2+\lambda\mid\mid \omega \mid\mid_1^2$</p><p>分别使用了$L_2,L_1$范数，前者能比后者更易于获得稀疏解：求得的$\omega$有更少的非零向量。所以$\omega$取得的稀疏解意味着初始的d个特征中仅有对应着$\omega$的非零分量特征才会出现在最终模型中，于是求解$L_1$范数正则化的结果是得到了仅采用一部分初始特征的模型，基于$L_1$正则化的学习方法就是一种嵌入式特征选择方法。</p><h1 id="稀疏表示与字典学习">稀疏表示与字典学习</h1><p>稀疏表达形式对学习任务来说有不少的好处，例如线性的SVM之所以在文本数据上有很好的性能，就是因为文本数据使用上述的字频表示后有高度的稀疏性，使大多数问题变得线性可分。同时稀疏表示可以减少存储空间。</p><p>若给定数据集D是稠密的，普通非稀疏数据，我们需要一个“字典”，将样本转化成为合适的稀疏表示形式，使得模型复杂度降低，称为字典学习或者稀疏编码。</p><p>字典学习侧重于学得字典的过程；稀疏编码偏重对样本进行稀疏表达的过程。</p><p>给定数据集（训练集）${x_1,x_2,…,x_m}$，字典学习最简单的形式为$\min\limits_{B,\alpha_i}\sum\limits_{i=1}^m \mid\mid x_i-B\alpha_i \mid\mid_2^2 +\lambda\sum\limits_{i=1}^m \mid\mid \alpha_i \mid\mid_1$</p><p>前面的部分是最小化训练样本与学习的字典学习模型之间的误差，后面则是字典学习模型的$L_1$范数，即寻找稀疏数据。</p><p>其中B为字典矩阵，k为字典词汇量，由用户设定，$\alpha_i \in R^k$是样本$x_i \in R^d$的稀疏表示。</p><p>因为这里我们需要对B和%\alpha%进行学习，我们使用变量交替优化的策略求解。</p><ul><li>固定字典B，我ie每个样本$x_i$找到相应的$\alpha_i$</li></ul><p>$\min\limits_{\alpha_i} \mid\mid x_i-B\alpha_i \mid\mid_2^2 + \alpha\mid \alpha_i \mid_1$</p><ul><li>固定$\alpha_i$来更新字典B，此时原式改为了$\min\limits_{B}\mid\mid X-BA \mid\mid_F^2$</li></ul><p>其中$A=(\alpha_1,\alpha_2,…,\alpha_m)$，</p><p>初始化字典B后，迭代这两步，通过设置k的数值控制稀疏程度。</p><h1 id="压缩感知">压缩感知</h1><p>压缩感知关注的是如何利用信号本身所具有的稀疏性，从部分观测样本中恢复原信号。通常认为压缩感知分为：感知测量和重构恢复两个阶段。前者关注如何对原始信号进行处理以获得稀疏样本表示，这方面技术包括傅里叶变换、小波变换以及字典学习和稀疏编码；重构恢复则主要针对基于稀疏性从少量观测中恢复原信号，这部分是压缩感知的核心。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;范数&quot;&gt;范数&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;$L_0$范数表示向量中非零元素的个数：$\mid\mid x \mid\mid_0 = count(x_i) while x_i \neq 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用这个范数希望参数的大部分元素是0（稀疏数据），通过最优化范数，可以寻找最优稀疏特征，不过这个范数的最优问题是NP难度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$L_1$范数表示向量中每个元素绝对值的和：$\mid\mid x \mid\mid_1=\sum_{i=1}^n \mid x_i \mid$&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>周报(1)</title>
    <link href="http://peihao.space/2017/03/10/week-report310/"/>
    <id>http://peihao.space/2017/03/10/week-report310/</id>
    <published>2017-03-10T04:00:13.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="周报3-10">周报3.10</h1><p>这次周报，主要回顾从周一下午在公司开完会回来后的工作情况以及状态。</p><h2 id="状态">状态</h2><ul><li>时间：由于每天坚持健身锻炼，以及在临睡前习惯的观看某栏目的逻辑思维导论，拖拖拉拉的晚上会睡的比较迟。早上也是醒的蛮晚。这些习惯会在月底前的一周里改正。</li></ul><a id="more"></a><ul><li>地点：现在每天宿舍里空调打孔，噪音比较大。除了早上在寝室学习之外，之前没在实验室的会议室学习前，都会到教三的三楼自习。周二开始在保密会议室内学习，晚上九点半离开。</li></ul><ul><li>效率：工作效率方面，白天的学习效率很低，经常会被外界的环境影响；在封闭的实验室内效率提高不少，特别是晚上六点之后。deadline是第一生产力，那个时候开始有时间危机，想着不能又白白浪费一天时间，逐渐不去理会外界干扰。同时在会议室的办公桌上可以使用外接的大屏显示器辅助工作，感觉很棒。</li></ul><h2 id="工作情况">工作情况</h2><p>目前是三月份，导师要我们9月份之前确定论文的题目，留给我们几乎半年的时间。所以这一阶段的前几十天我主要目标是在导师的指引下，在自己感兴趣的、或者认为有前景的领域粗粗的学一学，领略一番，期望遇到自己有深入进去欲望的方向、点。</p><p>最近一直在看南大周教授的《机器学习》和李航老师的《统计学习方法》。第一遍读不求甚解，没有公示的推倒，只是在顺着作者的思路学习，目的就是了解这一领域的基础知识、原理与技术。</p><p>两本书都对数学有一定的要求。上学期的《概率论与随机过程》课程中的部分章节在这里派上了用场，但是还有一些需要用到本科时的内容，包括《概率论与数理统计》、《线性代数》甚至《高级数学》。由于我没有参加统考，没有对这几门课程有系统的复习，几年过去，说实话遗忘了很多。类似协方差$Cov(\vec{x},\vec{y})$，特征值$\lambda$，矩阵的秩、迹、线性空间、欧氏空间、超平面等等很多定义、用法都要用到。这些有的是我学过还记得，有的学过忘掉了，还有部分没有接触过的，往往一个书上式子的理解都需要很长时间。不过嘛，这是我必经的路。前天听了同学的介绍，准备在网络上听一些课程，期望有所好转。</p><p>这期间开始使用latex。</p><p>周四，也就是昨天参加了在《中欧在5G及物联网领域的合作》会议。主要内容就是5G的最新进展以及物联网领域的突飞猛进。会议基本以英文演说为主，很多由于英文水平有限没有很好的理解，不过几个突出表达的新技术名词记录下来，回来查看资料还是大开眼见。几个关键词：光通信LIFI、AIOTI物联网组织、ICT合作。</p><p>昨天拿到了上届学长学姐的开题报告，到写周报前，只是把题目、时间安排简单的浏览了一下。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;周报3-10&quot;&gt;周报3.10&lt;/h1&gt;&lt;p&gt;这次周报，主要回顾从周一下午在公司开完会回来后的工作情况以及状态。&lt;/p&gt;
&lt;h2 id=&quot;状态&quot;&gt;状态&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;时间：由于每天坚持健身锻炼，以及在临睡前习惯的观看某栏目的逻辑思维导论，拖拖拉拉的晚上会睡的比较迟。早上也是醒的蛮晚。这些习惯会在月底前的一周里改正。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="周报" scheme="http://peihao.space/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
      <category term="周报" scheme="http://peihao.space/tags/%E5%91%A8%E6%8A%A5/"/>
    
  </entry>
  
  <entry>
    <title>降维</title>
    <link href="http://peihao.space/2017/03/09/ml-dimension-reduction/"/>
    <id>http://peihao.space/2017/03/09/ml-dimension-reduction/</id>
    <published>2017-03-09T02:36:28.000Z</published>
    <updated>2017-09-14T15:37:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="k近邻学习">k近邻学习</h1><p>k近邻（k-Nearest Neighboor，KNN）是一种常用的监督学习方法：给定测试样本，基于某种距离距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个训练样本的信息对本样本进行预测。分类学习中通常使用投票法，少数服从多数；回归学习中则使用平均法。通常为了体现出靠近的距离指标，在分类学习与回归学习中使用加权法，越靠近样本的权值越大。</p><p>给定测试样本x，若其最近邻样本为z，则最近邻分类器出错的概率就是x与z类别标记不同的概率：$P(err)=1-\sum\limits_{c \in y}P(c \mid x)P(c \mid z)$</p><a id="more"></a><p>假设样本独立同分布，且对任意x和任意小正数$\delta$，在x附近$\delta$距离范围内找到一个训练样本z；令$c^T=arg \max\limits_{c \in y}P(c \mid x)$表示贝叶斯最优分类器的结果，有：</p><p>$$<br>P(err)=1-\sum\limits_{c \in y}P(c \mid x)P(c \mid z)<br>\approx1-\sum\limits_{c \in y}P^2(c \mid x)<br>\leq 1- P^2(c^T \mid x)<br>=(1+P(c^T \mid x))(1-P(c^T \mid x))<br>\leq 2 \times (1-p(c^T \mid x))<br>$$</p><p>可以看出，在这种理想条件下KNN分类器的泛化错误率不超过贝叶斯错误率的两倍。</p><h1 id="降维">降维</h1><p>上面的讨论的基础是在足够小的距离内都有相邻的样本作为参考，考虑到做个属性（也就是多个维度）的情况，需要的合适的样本数量回事天文数字。高维度空间会给距离计算带来很大麻烦。</p><p><img src="http://peihao.space/img/article/ml/ml-intro9-2.png" alt=""></p><p>一般采用的方式是通过某种数学变换将原始高维属性空间转变为一个低维子空间。子空间中样本密度大幅提升。</p><p><img src="http://peihao.space/img/article/ml/ml-intro9-1.png" alt=""></p><h2 id="MDS">MDS</h2><p>MDS即多维缩放(Multiple Dimensional Scaling)，是要就将原始空间中样本间的距离在低维空间中得以保持。</p><p>假定m个样本在原始空间的距离矩阵为$D \in R^{m \times m}$，在第i行j列的元素$dist_{ij}$为样本$x_i$到$s_j$的距离。目标是获得样本在$d^c$维空间的表示$Z \in R^{d^c \times m},d^c \leq d$，且任意两个样本在$d^c$维空间的欧式距离等于原始空间$\mid\mid z_i-z_j \mid\mid = dist_{ij}$</p><p>令$B=Z^TZ \in R^{m \times m}$，其中B为降维后样本的内积矩阵，$b_{ij}=b_{ji}=z_i^Tz_j$</p><p>$$<br>dist_{i.}^2=\frac{1}{m}\sum\limits_{j=1}^m dist_{ij}^2<br>dist_{.j}^2=\frac{1}{m}\sum\limits_{i=1}^m dist_{ij}^2<br>dist_{..}^2=\frac{1}{m^2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m dist_{ij}^2<br>$$</p><p>由以上的式子推出：</p><p>$b_ij=-\frac{1}{2}(dist_{ij}^2-dist_{i.}^2-dist_{.j}^2+dist_{..}^2)$</p><p>依次求出矩阵B中所有的元素，然后对矩阵做特征值分解：$B=V\Lambda V^T$其中V为特征向量矩阵，$\Lambda$为特征值构成的对角矩阵。假定其中有$d^c$个非零特征值，构成对角矩阵$\Lambda_x=diag(\lambda_1,\lambda_2,…,\lambda_{dx})$，令$Vx$表示相应的特征向量矩阵，则：</p><p>$Z=\Lambda_c^{1/2}V_c^T \in R^(d^c \times m)$</p><h1 id="主成分分析">主成分分析</h1><p>通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。</p><p>主成份（Principal Component Analysis）分析是降维（Dimension Reduction）的重要手段。每一个主成分都是数据在某一个方向上的投影，在不同的方向上这些数据方差Variance的大小由其特征值（eigenvalue）决定。一般我们会选取最大的几个特征值所在的特征向量（eigenvector），这些方向上的信息丰富，一般认为包含了更多我们所感兴趣的信息。</p><blockquote><p>在很多情形，变量之间是有一定的相关关系的，当两个变量之间有一定相关关系时，可以解释为这两个变量反映此课题的信息有一定的重叠。主成分分析是对于原先提出的所有变量，将重复的变量（关系紧密的变量）删去多余，建立尽可能少的新变量，使得这些新变量是两两不相关的，而且这些新变量在反映课题的信息方面尽可能保持原有的信息。</p></blockquote><p>设法将原来变量重新组合成一组新的互相无关的几个综合变量，同时根据实际需要从中可以取出几个较少的综合变量尽可能多地反映原来变量的信息的统计方法叫做主成分分析或称主分量分析，是数学上用来降维的一种方法。</p><p>先定义一些变量：</p><p>样本$(\vec{x_1},\vec{x_2},\vec{x_3},…,\vec{x_m})$</p><p>投影变换得到的 <strong>新坐标系</strong>${ \vec{\omega_1},\vec{\omega_2},…,\vec{\omega_d}, }$</p><p>样本点在低维坐标系中的投影是$z_i=(z_{i1};z_{i2};z_{i3};…;z_{id_‘};)$</p><p>其中$z_{ij}=\vec{\omega}_j^T\vec{x_i}$是x在低维坐标系下的第j维坐标</p><p>投影点方差是$\sum_i W^Tx_ix_i^TW$ 优化目标是最大化这个方差，使得在投影之后尽可能的分散</p><p>PCA一般有以下几个步骤：</p><ol><li><p>数据减去均值：样本中心化$\sum_i \vec{x_i}=\vec{0}$</p></li><li><p>计算协方差矩阵：$C=\frac{XX^T}{N}$</p></li><li><p>计算协方差矩阵的特征矢量和特征值：对协方差矩阵$\frac{XX^T}{N}$特征分解$CU=U\Lambda$，其中C为方差矩阵，U为计算的特征矢量，$\Lambda$是对角线矩阵$\Lambda=diag(\lambda_1,\lambda_2,…,\lambda_d)$</p></li><li><p><strong>选择成分组成模式矢量</strong>对应最大特征值的特征矢量就是数据的主成分：取最大的d个特征值所对应的特征向量$u_1,u_2,…,u_d$</p></li></ol><p>一般地，从协方差矩阵找到特征矢量以后，下一步就是按照特征值由大到小进行排列，这将给出成分的重要性级别。现在，如果你喜欢，可以忽略那些重要性很小的成分，当然这会丢失一些信息，但是如果对应的特征值很小，你不会丢失很多信息。</p><ol><li>获得投影的新数据$y_i=U_k^T x_i$</li></ol><h1 id="核化线性降维">核化线性降维</h1><p>线性降维假设从高维空间到低维度空间的函数映射是线性的，然而在不少的现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。</p><p>非线性降维的一种常用方法是基于核技巧对线性降维方法进行”核化”。</p><blockquote><p><a href="http://zhanxw.com/blog/2011/02/kernel-pca-%E5%8E%9F%E7%90%86%E5%92%8C%E6%BC%94%E7%A4%BA/" target="_blank" rel="external">在KPCA中</a>，除了在PCA中的一些先决条件外，我们认为原有的数据有更高的维数，我们可以在更高的维度空间中做PCA分析（即在更高维里，把原始数据向不同方向进行投影）</p></blockquote><p>我们拿到样本点，需要将它映射到高维空间中，然后使用PCA算法进行降维。</p><p>假定我们将在高维特征空间中把数据投影到由W确定的超平面上，即PCA欲求解$(\sum\limits_{i=1}^m)\vec(W)=\lambda \vec{W}$  （拉格朗日乘子法）</p><p>其中$z_i$是样本点在高维特征空间的像，有$\vec{W}=\sum\limits_{i=1}^m z_i \alpha _i$</p><p>$\alpha_i=\frac{1}{\lambda}z_i^T \vec{W}$</p><p>z是由原始属性空间中的样本点x通过映射$\phi$产生。但是在映射到高维空间这一步，一般情况下，我们并不知道映射函数$phi$的具体形，不知道要映射到哪重维度上，于是引入核函数$\kappa(x_i,x_j)=\phi(x_i)^T\phi(x_j)$</p><p>化简式子：$KA=\lambda A$，K为$\kappa$对应核矩阵，$A=(\alpha_1;\alpha_2;…;\alpha_m)$</p><p>上式是特征值分解问题，取K对应的最大的d个特征值对应的特征向量。</p><p>对于新样本x，投影的第j维坐标为$z_j=\omega_j^T \phi(x)=\sum\limits_{i=1}^m \alpha_i^j \kappa(x_i,x)$</p><h1 id="流形学习">流形学习</h1><p>流形是一类借鉴了拓扑流形概念的降维方法，在局部有欧式空间的性质，能用欧式距离来进行距离计算。</p><p>若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在局部上仍具有欧式空间的性质。因此可以容易的在局部建立降维映射关系，然后再设法将局部映射关系推广到全局。</p><h2 id="等度量映射">等度量映射</h2><p>等度量映射（Isometric Mapping）认为低维流形嵌入到高维空间后，直接在高维空间中计算直线距离具有误导性，因为高维空间中的直线距离往往在低维嵌入流形上是不可达的。</p><p><img src="http://peihao.space/img/article/ml/ml-intro9-3.png" alt=""></p><p>对每个点基于欧式距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻之间不存在连接，于是计算两点间测地线距离的问题，转变为了计算近邻点之间最短路径问题。</p><p>步骤：</p><ol><li><p>确定$x_i$的k近邻，并将$x_i$与k近邻之间的距离设置为欧氏距离，与其他点的距离设置为无穷大</p></li><li><p>调用最短路径算法计算任意来那个样本之间的距离$dist(x_i,dist_j)$</p></li><li><p>将$dist(x_i,dist_j)$作为MDS算法的输入求解</p></li></ol><h2 id="局部线性嵌入">局部线性嵌入</h2><p>局部线性嵌入（Locally Linear Embedding，LEE）试图保持邻域内样本之间的线性关系。假定样本点$x_i$的坐标能通过他的邻域样本$x_j,x_k,x_l$的坐标通过线性组合组成，$x_i=w_{ij}x_j+w_{ik}x_k+w_{il}x_l$</p><p>LLE希望关系在低维空间中得以保持。</p><p><img src="http://peihao.space/img/article/ml/ml-intro9-4.png" alt=""></p><p>LLE算法的主要原理就是先在高维空间，计算出相应的重构系数序列$W={w_1,w_2,…,_m}$，随后在低维空间中通过相同的重构系数获取投影点。</p><p>令$Z=(z_1,z_2,…,z_m) \in R^{d^. \times m}$</p><p>$M = (I-W)^T(I-W)$则寻求最小化：$\min\limits_z tr(ZMZ^T)$，约束条件$ZZ^T=I$</p><p>上式通过特征值分解，M最小的$d^.$个特征值对应的特征向量组成的矩阵就是$Z^T$，即在低维的投影</p><h1 id="度量学习">度量学习</h1><p>对两个d维样本$x_i  x_j$，他们之间的平方欧式距离可写为：$dist^2(x_i,x_j)=dist_{ij,1}^2+dist_{ij,2}^2+…+dist_{ij,d}^2$</p><p>其中$dist_{ij,k}$表示$x_i,x_j$在第k维上的距离，若嘉定不同属性的重要程度不一样，可以引入属性权重$\omega$</p><p>$dist^2(w_i,x_j)=\omega_1\cdot dist_{ij,1}^2+\omega_2\cdot dist_{ij,2}^2+…+\omega_d\cdot dist_{ij,d}^2=(x_i-x_j)^T W(x_i-x_j)$</p><p>其中$\omega_i \geq 0,W=diag(\omega)$是一个对角矩阵</p><p>W是对角矩阵，坐标轴正交，属性之间无关；然而现实中并不是这样将W图换位一个普通的半正定对称矩阵M，得到马氏距离。</p><p>其中M称作“度量矩阵”，而度量学习是对M进行学习。M必须是正定对称的，即必有正交基P使得M能写成$M=PP^T$</p><p>对M的学习，我们需要把M直接嵌入到需要提高效率的评价指标中，通过优化指标求得M</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;k近邻学习&quot;&gt;k近邻学习&lt;/h1&gt;&lt;p&gt;k近邻（k-Nearest Neighboor，KNN）是一种常用的监督学习方法：给定测试样本，基于某种距离距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个训练样本的信息对本样本进行预测。分类学习中通常使用投票法，少数服从多数；回归学习中则使用平均法。通常为了体现出靠近的距离指标，在分类学习与回归学习中使用加权法，越靠近样本的权值越大。&lt;/p&gt;
&lt;p&gt;给定测试样本x，若其最近邻样本为z，则最近邻分类器出错的概率就是x与z类别标记不同的概率：$P(err)=1-\sum\limits_{c \in y}P(c \mid x)P(c \mid z)$&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="http://peihao.space/categories/ML/"/>
    
    
      <category term="ML" scheme="http://peihao.space/tags/ML/"/>
    
  </entry>
  
</feed>
